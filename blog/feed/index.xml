<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0"
     xmlns:content="http://purl.org/rss/1.0/modules/content/"
     xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
     xmlns:atom="http://www.w3.org/2005/Atom"
     xmlns:dc="http://purl.org/dc/elements/1.1/"
     xmlns:wfw="http://wellformedweb.org/CommentAPI/"
     >
  <channel>
    <title>Gregory Szorc's Digital Home</title>
    <link>http://gregoryszorc.com/blog</link>
    <description>Rambling on</description>
    <pubDate>Tue, 28 Feb 2017 21:12:06 GMT</pubDate>
    <generator>Blogofile</generator>
    <sy:updatePeriod>hourly</sy:updatePeriod>
    <sy:updateFrequency>1</sy:updateFrequency>
    <item>
      <title>Mercurial, SHA-1, and Trusting Version Control</title>
      <link>http://gregoryszorc.com/blog/2017/02/28/mercurial,-sha-1,-and-trusting-version-control</link>
      <pubDate>Tue, 28 Feb 2017 12:40:00 PST</pubDate>
      <category><![CDATA[Mercurial]]></category>
      <category><![CDATA[Mozilla]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2017/02/28/mercurial,-sha-1,-and-trusting-version-control</guid>
      <description>Mercurial, SHA-1, and Trusting Version Control</description>
      <content:encoded><![CDATA[<p>The Internet went crazy on Thursday when Google
<a href="https://security.googleblog.com/2017/02/announcing-first-sha1-collision.html">announced a SHA-1 collision</a>.
This has spawned a lot of talk about the impact of SHA-1 in version
control. Linus Torvalds (the creator of Git) weighed in on the
<a href="http://marc.info/?l=git&amp;m=148787047422954">Git mailing list</a> and on
<a href="https://plus.google.com/+LinusTorvalds/posts/7tp2gYWQugL">Google+</a>.
There are also posts like
<a href="http://www.metzdowd.com/pipermail/cryptography/2017-February/031623.html">SHA1 collisions make Git vulnerable to attakcs by third-parties, not just repo maintainers</a>
outlining the history of Git and SHA-1. On the Mercurial side,
Matt Mackall (the creator of Mercurial) authored a
<a href="https://www.mercurial-scm.org/wiki/mpm/SHA1">SHA-1 and Mercurial security article</a>.
(If you haven't read Matt's article, please do so now before
continuing.)</p>
<p>I'd like to contribute my own take on the problem with a slant towards
Mercurial and while also comparing Mercurial's exposure to SHA-1
collisions to Git's. Since this is a security topic, I'd like to
explicitly state that I'm not a cryptographer. However, I've worked on
a number of software components that do security/cryptography (like
Firefox Sync) and I'm pretty confident saying that my grasp on
cryptographic primitives and security techniques is better than the
average developer's.</p>
<p>Let's talk about Mercurial's exposure to SHA-1 collisions on a technical
level.</p>
<p>Mercurial, like Git, is <em>vulnerable</em> to SHA-1 collisions. Mercurial is
vulnerable because its logical storage mechanism (like Git's) indexes
tracked content by SHA-1. If two objects with differing content have the
same SHA-1, content under version control could be changed and detecting
that would be difficult or impossible. That's obviously bad.</p>
<p>But, Mercurial's exposure to SHA-1 collisions isn't as severe as Git's.
To understand why, we have to understand how each stores data.</p>
<p>Git's logical storage model is a content-addressable key-value store.
Values (<em>objects</em> in Git parlance) consist of a header identifying the
object type (commit, tree, blob, or tag), the size of the data (as a
string), and the raw content of the thing being stored. Common content
types are file content (blob), a list of files (tree), and a description
of a commit (commit). Keys in this blob store are SHA-1 hashes of objects.
All Git objects go into a single <em>namespace</em> in the Git repository's
<em>store</em>. A beneficial side-effect of this is data de-duplication: if the
same file is added to a Git repository, it's <em>blob</em> object will be
identical and it will only be stored once by Git. A detrimental
side-effect is that hash collisions are possible between any two objects,
irregardless of their type or location in the repository.</p>
<p>Mercurial's logical storage model is also content-addressable. However,
it is significantly different from Git's approach. Mercurial's logical
storage model allocates a separate sub-store for each tracked <em>path</em>.
If you run <code>find .hg/store -name '*.i'</code> inside a Mercurial repository,
you'll see these files. There is a separate file for each path
that has committed data. If you <code>hg add foo.txt</code> and <code>hg commit</code>, there
will be a <code>data/foo.txt.i</code> file holding data for <code>foo.txt</code>.
There are also special files <code>00changelog.i</code> and <code>00manifest.i</code> holding
data for commits/changesets and file lists, respectively. Each of these
<code>.i</code> files - a <a href="https://www.mercurial-scm.org/repo/hg/file/a185b903bda3/mercurial/help/internals/revlogs.txt">revlog</a> -
is roughly equivalent to an ordered collection of Git objects for a
specific tracked path. This means that Mercurial's store consists of
N discrete and independent namespaces for data. Contrast with Git's
single namespace.</p>
<p>The benefits and drawbacks are the opposite of those pointed out for
Git above: Mercurial doesn't have automatic content-based de-duplication
but it does provide some defense against hash collisions. Because each
logical path is independent of all others, a Mercurial repository will
happily commit two files with different content but same hashes. This
is more robust than Git because a hash collision is isolated to a single
logical path / revlog. In other words, a random file added to the
repository in directory <code>X</code> that has a hash collision with a file in
directory <code>Y</code> won't cause problems.</p>
<p>Mercurial also differs significantly from Git in terms of how the hash
is obtained. Git's hash is computed from raw content preceded by a header
derived directly from the object's role and size. (Takeaway: the header is
static and can be derived trivially.) Mercurial's hash is computed from
raw content preceded by a header. But that header consists of the 20 byte
SHA-1 hash(es) of the parent revisions in the revlog to which the content
is being added. This <em>chaining</em> of hashes means that the header is not
always static nor always trivially derived. This means that the same
content can be stored in the revlog under multiple hashes. It also means
that it is possible to store differing content having a hash collision
within the same revlog! But only under some conditions - Mercurial will
still barf in some scenarios if there is a hash collision within content
tracked by the revlog. This is different from Git's behavior, where the
same content <em>always</em> results in the same Git object hash. (It's worth
noting that a SHA-1 collision on data with a Git object header has not
yet been encountered in the wild.)</p>
<p>The takeaway from the above paragraphs is Mercurial's storage model
is slightly more robust against hash collisions than Git's because there
are multiple, isolated namespaces for storing content and because all
hashes are chained to previous content. So, when SHA-1 collisions
are more achievable and someone manages to create a collision for a
hash used by version control, Mercurial's storage layer will be able
to cope with that better than Git's.</p>
<p>But the concern about SHA-1 weakness is more about security than storage
robustness. The disaster scenario for version control is that an
attacker could replace content under version control, possibly
undetected. If one can generate a hash collision, then this is
possible. Mercurial's chaining of content provides some defense, but
it isn't sufficient.</p>
<p>I agree with <a href="https://www.mercurial-scm.org/wiki/mpm/SHA1">Matt Mackall</a>
that at the present time there are bigger concerns with content
safety than SHA-1 collisions. Namely, if you are an attacker, it is
much easier to introduce a subtle bug that contains a security
vulnerability than to introduce a SHA-1 collision. It is also
much easier to hack the canonical version control server (or any
user or automated agent that has permissions to push to the server)
and add a <em>bad</em> commit. Many projects don't have adequate defenses
to detect such <em>bad</em> commits. Ask yourself: if a bad actor pushed
a <em>bad</em> commit to my repository, would it be detected? Keep in mind
that spoofing author and committer metadata in commits is trivial.
<strong>The current state of Mercurial and Git rely primarily on trust -
not SHA-1 hashes - as their primary defense against malicious
actors.</strong></p>
<p>The desire to move away from SHA-1 has been on the radar of the
Mercurial project for years. For 10+ years, the <em>revlog</em> data
structure has allocated 32 bytes for hashes while only using 20 bytes
for SHA-1. And, the topic of SHA-1 weakness and desire to move to
something stronger has come up at the developer <em>sprints</em> for the
past several years. However, it has never been pressing enough to act
on because <em>there are bigger problems</em>. If it were easy to change, then
Mercurial likely would have done it already. But changing is not easy.
As soon as you introduce a new hash format in a repository, you've
potentially locked out all legacy versions of the Mercurial software
from accessing the repository (unless the repository stores multiple
hashes and allows legacy clients to access the legacy SHA-1 hashes).
There are a number of concerns from legacy compatibility (something
Mercurial cares deeply about) to user experience to even performance
(SHA-1 hashing even at 1000+MB/s floats to the top of performance
profiling for some Mercurial operations). I'm sure the topic will be
discussed heavily at the upcoming developers sprint in a few weeks.</p>
<p>While Mercurial should (and will eventually) replace SHA-1, I think
the biggest improvement Mercurial (or Git for that matter) can make
to repository <em>security</em> is providing a better mechanism for tracking
and auditing trust. Existing mechanisms for GPG signing every commit
aren't practical or are a non-starer for many workflows. And, they
rely on GPG, which has notorious end-user usability problems. (I would
prefer my version control tool not subject me to toiling with GPG.)
I've thought about this topic considerably, authoring a
<a href="https://www.mercurial-scm.org/wiki/CommitSigningPlan">proposal for easier and more flexible commit signing</a>.
There is also a related proposal to establish a
<a href="https://www.mercurial-scm.org/wiki/CommitCustodyConcept">cryptographically meaningful chain-of-custody for a patch</a>.
There are some good ideas there. But, like all user-facing
cryptography, the devil is in the details. There are some hard
problems to solve, like how to manage/store public keys that were
used for signatures. While there is some prior art in version control
tools (see Monotone), it is far from a solved problem. And at the
end of the day, you are still left having to trust a set of keys
used to produce signatures.</p>
<p>While version control can keep using cryptographically strong hashes
to mitigate collisions within its storage layer to prevent content
swapping and can employ cryptographic signatures of tracked data,
there is still the issue of trust. Version control can give you the
tools for establishing and auditing trust. Version control can also
provide tools for managing trust relationships. But at the end of the
day, the actual act of trusting trust boils down to people making
decisions (possibly through corporate or project policies). This
will always be a weak link. Therefore, it's what malicious actors
will attack. The best your version control tool can do is give its
users the capability and tools to run a secure and verifiable
repository so that when bad content is inevitably added you can't
blame the version control tool for having poor security.</p>]]></content:encoded>
    </item>
    <item>
      <title>MozReview Git Support and Improved Commit Mapping</title>
      <link>http://gregoryszorc.com/blog/2016/02/08/mozreview-git-support-and-improved-commit-mapping</link>
      <pubDate>Mon, 08 Feb 2016 11:05:00 PST</pubDate>
      <category><![CDATA[MozReview]]></category>
      <category><![CDATA[Mozilla]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2016/02/08/mozreview-git-support-and-improved-commit-mapping</guid>
      <description>MozReview Git Support and Improved Commit Mapping</description>
      <content:encoded><![CDATA[<p>MozReview - Mozilla's Review Board based code review tool - now
supports ingestion from Git. Previously, it only supported Mercurial.</p>
<p><a href="https://mozilla-version-control-tools.readthedocs.org/en/latest/mozreview/install-git.html">Instructions</a>
for configuring Git with MozReview are available. Because blog posts
are not an appropriate medium for documenting systems and processes, I
will not say anything more here on how to use Git with MozReview.</p>
<p>Somewhat related to the introduction of Git support is an improved
mechanism for mapping commits to existing review requests.</p>
<p>When you submit commits to MozReview, MozReview has to decide how
to <a href="https://mozilla-version-control-tools.readthedocs.org/en/latest/mozreview/commits.html#understanding-how-commits-are-mapped-to-review-requests">map</a>
those commits to review requests in Review Board. It has to choose
whether to recycle an existing review request or create a new one.
When recycling, is has to pick an appropriate one. If it chooses
incorrectly, wonky things can happen. For example, a review request
could switch to tracking a new and completely unrelated commit. That's
bad.</p>
<p>Up until today, our commit mapping algorithm was extremely simple. Yet
it seemed to work 90% of the time. However, a number of people
found the cracks and complained. With Git support coming online,
I had a feeling that Git users would find these cracks with higher
frequency than Mercurial users due to what I perceive to be
variations in the commit workflows of Git versus Mercurial. So,
I decided to proactively improve the commit mapping before the Git
users had time to complain.</p>
<p><strong>Both the Git and Mercurial MozReview client-side extensions now insert
a <em>MozReview-Commit-ID</em> metadata line in commit messages.</strong> This line
effectively defines a (likely) unique ID that identifies the commit
across rewrites. When MozReview maps commits to review requests,
it uses this identifier to find matches. What this means is that
history rewriting (such as reordering commits) should be handled
well by MozReview and should not confuse the commit mapping
mechanism.</p>
<p>I'm not claiming the commit mapping mechanism is perfect. In fact,
I know of areas where it can still fall apart. But it is much
better than it was before. If you think you found a bug in the
commit mapping, don't hesitate to
<a href="https://bugzilla.mozilla.org/enter_bug.cgi?product=Developer%20Services&amp;component=MozReview">file a bug</a>.
Please have it block <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1243483">bug 1243483</a>.</p>
<p>A side-effect of introducing this improved commit mapping is that
commit messages will have a <em>MozReview-Commit-ID</em> line in them. This
may startle some. Some may complain about the <em>spam</em>. Unfortunately,
there's no better alternative. Both Mercurial and Git do support a
hidden key-value dictionary for each commit object. In fact, the
MozReview Mercurial extension has been storing the very commit IDs
that now appear in the commit message in this dictionary for months!
Unfortunately, actually using this hidden dictionary for metadata
storage is riddled with problems. For example, some Mercurial
commands don't preserve all the metadata. And accessing or setting
this data from Git is painful. While I wish this metadata (which
provides little value to humans) were not located in the commit
message where humans could be bothered by it, it's really the only
practical place to put it. If people find it super annoying, we
could modify Autoland to strip it before landing. Although, I think
I like having it preserved because it will enable some useful
scenarios down the road, such as better workflows for uplift
requests. It's also worth noting that there is precedent for storing
unique IDs in commit messages for purposes of commit mapping in the
code review tool: <a href="https://gerrit-documentation.storage.googleapis.com/Documentation/2.11.5/user-changeid.html">Gerrit uses Change-ID lines</a>.</p>
<p>I hope you enjoy the Git support and the more robust commit to review
request mapping mechanism!</p>]]></content:encoded>
    </item>
    <item>
      <title>Making MozReview Faster by Disregarding RESTful Design</title>
      <link>http://gregoryszorc.com/blog/2016/01/13/making-mozreview-faster-by-disregarding-restful-design</link>
      <pubDate>Wed, 13 Jan 2016 15:25:00 PST</pubDate>
      <category><![CDATA[MozReview]]></category>
      <category><![CDATA[Mozilla]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2016/01/13/making-mozreview-faster-by-disregarding-restful-design</guid>
      <description>Making MozReview Faster by Disregarding RESTful Design</description>
      <content:encoded><![CDATA[<p>When I first started writing web services, I was a huge RESTful fan
boy. The architectural properties - especially the parts related to
caching and scalability - really jived with me. But as I've grown
older and gained experienced, I've realized that RESTful design,
like many aspects of software engineering, is more of a guideline
or ideal than a panacea. This post is about one of those experiences.</p>
<p><a href="https://www.reviewboard.org/docs/manual/2.5/webapi/">Review Board's Web API</a>
is RESTful. It's actually one of the better examples of a RESTful API
I've seen. There is a very clear separation between <em>resources</em>. And
everything - and I mean everything - is a resource. <em>Hyperlinks</em> are
used for the purposes described in Roy T. Fielding's dissertation.
I can tell the people who authored this web API understood RESTful
design and they succeeded in transferring that knowledge to a web API.</p>
<p>Mozilla's <a href="https://reviewboard.mozilla.org">MozReview code review tool</a>
is built on top of Review Board. We've made a number of customizations.
The most significant is the ability to submit a series of commits
as one logical review series. This occurs as a side-effect of a
<em>hg push</em> to the code review repository. Once your changesets
are pushed to the remote repository, that server issues a number of
Review Board Web API HTTP requests to reviewboard.mozilla.org to
create the review requests, assign reviewers, etc. This is mostly all
built on the built-in web API endpoints offered by Review Board.</p>
<p>Because Review Board's Web API adheres to RESTful design principles
so well, turning a series of commits into a series of review requests
takes a lot of HTTP requests. For each commit, we have to perform
something like 5 HTTP requests to define the server state. For
series of say 10 commits (which aren't uncommon since we try to
encourage the use of microcommits), this can add up to dozens of
HTTP requests! And that's just counting the HTTP requests to
Review Board: because we've integrated Review Board with Bugzilla,
events like publishing result in additional RESTful HTTP requests from
Review Board to bugzilla.mozilla.org.</p>
<p>At the end of the day, submitting and publishing a series of 10
commits consumes somewhere between 75 and 100 HTTP requests! While
the servers are all in close physical proximity (read: low network
latencies), we are reusing TCP connections, and each HTTP request
completes fairly quickly, the overhead adds up. <strong>It's not uncommon
for publishing commit series to take over 30s.</strong> This is unacceptable
to developers. We want them to publish commits for review as quickly
as possible so they can get on with their next task. Humans should not
have to wait on machines.</p>
<p>Over in <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1229468">bug 1220468</a>,
I implemented a new batch submit web API for Review Board and converted
the Mercurial server to call it instead of the classic, RESTful Review
Board web APIs. <strong>In other words, I threw away the RESTful properties
of the web API and implemented a monolith API doing exactly what we
need.</strong> The result is a drastic reduction in net HTTP requests. In our
tests, submitting a series of 20 commits for review reduced the HTTP
request count by 104! Furthermore, the new API endpoint performs
all modifications in a single database transaction. Before, each HTTP
request was independent and we had bugs where failures in the middle of
a HTTP request series left the server in inconsistent and unexpected
state. The new API is significantly faster and more atomic as a bonus.
The main reason the new implementation isn't yet nearly instantaneous
is because we're still performing several RESTful HTTP requests to
Bugzilla from Review Board. But there are plans for Bugzilla to
implement the batch APIs we need as well, so stay tuned.</p>
<p>(I don't want to blame the Review Board or Bugzilla maintainers for
their RESTful web APIs that are giving MozReview a bit of scaling pain.
MozReview is definitely abusing them almost certainly in ways that
weren't imagined when they were conceived. To their credit, the
maintainers of both products have recognized the limitations in their
APIs and are working to address them.)</p>
<p>As much as I still love the properties of RESTful design, there are
practical limitations and consequences such as what I described
above. The older and more experienced I get, the less patience I have
for tolerating architecturally pure implementations that sacrifice
important properties, such as ease of use and performance.</p>
<p>It's worth noting that many of the properties of RESTful design are
applicable to <em>microservices</em> as well. When you create a new service
in a microservices architecture, you are creating more overhead for
clients that need to speak to multiple services, making changes less
transactional and atomic, and making it difficult to consolidate
multiple related requests into a higher-level, simpler, and performant
API. I recommend
<a href="http://martinfowler.com/articles/microservice-trade-offs.html">Microservice Trade-Offs</a>
for more on this subject.</p>
<p>There is a place in the world for RESTful and microservice
architectures. And as someone who does a lot of server-side engineering,
I sympathize with wanting scalable, fault-tolerant architectures. But
like most complex problems, you need to be cognizant of trade-offs. It is
also important to not get too caught up with architectural purity if
it is getting in the way of delivering a simple, intuitive, and fast
product for your users. So, please, follow me down from the ivory tower.
The air was cleaner up there - but that was only because it was so
distant from the swamp at the base of the tower that surrounds every
software project.</p>]]></content:encoded>
    </item>
    <item>
      <title>Investing in the Firefox Build System in 2016</title>
      <link>http://gregoryszorc.com/blog/2016/01/11/investing-in-the-firefox-build-system-in-2016</link>
      <pubDate>Mon, 11 Jan 2016 14:20:00 PST</pubDate>
      <category><![CDATA[Mozilla]]></category>
      <category><![CDATA[build system]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2016/01/11/investing-in-the-firefox-build-system-in-2016</guid>
      <description>Investing in the Firefox Build System in 2016</description>
      <content:encoded><![CDATA[<p>Most of Mozilla gathered in Orlando in December for an all hands meeting.
If you attended any of the plenary sessions, you probably heard people
like David Bryant and Lawrence Mandel make references to improving the
Firefox build system and related tools. Well, the cat is out of the bag:
<strong>Mozilla will be investing heavily in the Firefox build system and related
tooling in 2016!</strong></p>
<p>In the past 4+ years, the Firefox build system has mostly been held
together and incrementally improved by a loose coalition of people who
cared. We had a period in 2013 where a number of people were making
significant updates (this is when moz.build files happened). But for the
past 1.5+ years, there hasn't really been a coordinated effort to
improve the build system - just a lot of one-off tasks and
(much-appreciated) individual heroics. This is changing.</p>
<p>Improving the build system is a high priority for Mozilla in 2016.
And investment has already begun. In Orlando, we had a marathon 3 hour
meeting planning work for Q1. At least 8 people have committed to
projects in Q1.</p>
<p>The focus of work is split between immediate short-term wins and
longer-term investments. <strong>We also decided to prioritize the Firefox and
Fennec developer workflows (over Gecko/Platform) as well as the
development experience on Windows.</strong> This is because these areas have
been under-loved and therefore have more potential for impact.</p>
<p>Here are the projects we're focusing on in Q1:</p>
<ul>
<li>Turnkey artifact based builds for Firefox and Fennec (download
  pre-built binaries so you don't have to spend 20 minutes compiling
  C++)</li>
<li>Running tests from the source directory (so you don't have to copy
  tens of thousands of files to the object directory)</li>
<li>Speed up configure / prototype a replacement</li>
<li>Telemetry for mach and the build system</li>
<li>NSPR, NSS, and (maybe) ICU build system rewrites</li>
<li><em>mach build faster</em> improvements</li>
<li>Improvements to build rules used for building binaries (enables
  non-make build backends)</li>
<li>mach command for analyzing C++ dependencies</li>
<li>Deploy automated testing for <em>mach bootstrap</em> on TaskCluster</li>
</ul>
<p>Work has already started on a number of these projects. I'm optimistic
2016 will be a watershed year for the Firefox build system and the
improvements will have a drastic positive impact on developer
productivity.</p>]]></content:encoded>
    </item>
    <item>
      <title>hg.mozilla.org replication updates</title>
      <link>http://gregoryszorc.com/blog/2016/01/05/hg.mozilla.org-replication-updates</link>
      <pubDate>Tue, 05 Jan 2016 15:00:00 PST</pubDate>
      <category><![CDATA[Mercurial]]></category>
      <category><![CDATA[Mozilla]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2016/01/05/hg.mozilla.org-replication-updates</guid>
      <description>hg.mozilla.org replication updates</description>
      <content:encoded><![CDATA[<p>A few minutes ago, I formally enabled a new replication system
for <a href="https://hg.mozilla.org/">hg.mozilla.org</a>. For the curious,
<a href="https://mozilla-version-control-tools.readthedocs.org/en/latest/hgmo/replication.html">technical details</a>
are available.</p>
<p>This impacts you because pushes to hg.mozilla.org should now be
significantly faster. For example, pushes to mozilla-inbound
that used to take 15s now take 2s. Pushes to Try that used to
take 45s now take 10s. (Yes, the old replication system really
added a lot of overhead.) Pushes to hg.mozilla.org are still not
as fast as they could be due to us running the service on 5 year
old hardware (we plan to buy new servers this year) and due to the
use of NFS on the server. However, I believe push latency is now
reasonable for every repo except Try.</p>
<p>The new replication system opens the door to a number of future
improvements. We'd like to stand up mirrors in multiple data
centers - perhaps even offices - so clients have the fastest
connectivity and so we have a better disaster recovery story.
The new replication system facilitates this.</p>
<p>The new replication log is effectively a unified pushlog -
something people have wanted for years. While there is not yet
a public API for it, one could potentially be exposed, perhaps
indirectly via Pulse.</p>
<p>It is now trivial for us to stand up new consumers of the
replication log that react to repository events merely milliseconds
after they occur. This should eventually result in downstream
systems like build automation and conversion to Git repos starting
and thus completing faster.</p>
<p>Finally, the new replication system has been running unofficially for
a few weeks, so you likely won't notice anything different today
(other than removal of some printed messages when you push). What
changed today is the new system is enabled by default and we have
no plans to continue supporting or operating the legacy system.
Good riddance.</p>]]></content:encoded>
    </item>
    <item>
      <title>Mozilla Mercurial Extension Updates</title>
      <link>http://gregoryszorc.com/blog/2015/12/16/mozilla-mercurial-extension-updates</link>
      <pubDate>Wed, 16 Dec 2015 17:40:00 PST</pubDate>
      <category><![CDATA[Mercurial]]></category>
      <category><![CDATA[Mozilla]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2015/12/16/mozilla-mercurial-extension-updates</guid>
      <description>Mozilla Mercurial Extension Updates</description>
      <content:encoded><![CDATA[<p>There have been a handful of updates to Mozilla's client-side
Mercurial tools in the past week and all users are encouraged
to pull down the latest commit of mozilla-central and run
<em>mach mercurial-setup</em> to ensure their configuration is up to
date.</p>
<p>Improvements to <em>mach mercurial-setup</em> include:</p>
<ul>
<li><em>~</em> are now used in paths when appropriate</li>
<li>Mercurial 3.5.2 is now the oldest version you can run without
  the wizard complaining</li>
<li>The clone bundles feature is enabled when running Mercurial 3.6</li>
<li><em>hg wip</em> is available for configuration</li>
<li><em>hgwatchman</em> (make <em>hg status</em> significantly faster via background
  filesystem watching) is now available on OS X</li>
<li>x509 host key fingerprints are no longer pinned if your Python
  and Mercurial version is modern (this pinning exists in Mercurial
  because old versions of Python don't verify x509 certificates
  properly)</li>
<li>3rd party Mercurial extensions are pulled with extensions disabled
  (to prevent issues with old, incompatible extensions crashing the
  <em>hg pull</em> invocation)</li>
</ul>
<p>The firefoxtree extension has also been updated. It now uses the
new <em>namespaces</em> feature of Mercurial so all Firefox labels/names/refs
are exposed in a <em>firefoxtree</em> namespace instead of as tags. As part
of this change, <strong>you will lose old tags created by this extension
and will need to re-pull repos to recreate them as namespaced
labels</strong>. <em>hg log</em> output will now look like the following:</p>
<pre><code>changeset:   332223:0babaa3edcf9
fxtree:      central
parent:      332188:40038a66525f
parent:      332222:c6fc9d77e86f
user:        Carsten "Tomcat" Book &lt;cbook@mozilla.com&gt;
date:        Wed Dec 16 12:01:46 2015 +0100
summary:     merge mozilla-inbound to mozilla-central a=merge
</code></pre>
<p>(Note the <em>fxtree</em> line.)</p>
<p>The firefoxtree extension also now works with <em>hg share</em>. i.e. if
you <em>hg share</em> from a Firefox repository and <em>hg pull</em> from
the source repo or any shared repo, the labels will be updated in
every repo. This only works on newly-created shares. If you want
to enable this on an existing share, see
<a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1115188#c31">this comment</a>.</p>]]></content:encoded>
    </item>
    <item>
      <title>hg.mozilla.org Updates</title>
      <link>http://gregoryszorc.com/blog/2015/11/05/hg.mozilla.org-updates</link>
      <pubDate>Thu, 05 Nov 2015 09:20:00 PST</pubDate>
      <category><![CDATA[Mozilla]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2015/11/05/hg.mozilla.org-updates</guid>
      <description>hg.mozilla.org Updates</description>
      <content:encoded><![CDATA[<p>A number of changes have been made to
<a href="https://hg.mozilla.org">hg.mozilla.org</a> over the past few days:</p>
<ul>
<li>
<p>Bookmarks are now replicated from master to mirrors properly (before,
  you could have seen <em>foo@default</em> bookmarks appearing). This means
  bookmarks now properly work on hg.mozilla.org!
  (<a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1139056">bug 1139056</a>).</p>
</li>
<li>
<p>Universally upgraded to Mercurial 3.5.2. Previously we were running
  3.5.1 on the SSH master server and 3.4.1 on the HTTP endpoints. Some
  HTML templates received minor changes.
  (<a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1200769">bug 1200769</a>).</p>
</li>
<li>
<p>Pushes from clients running Mercurial older than 3.5 will now see
  an advisory message encouraging them to upgrade.
  (<a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1221827">bug 1221827</a>).</p>
</li>
<li>
<p>Author/user fields are now validated to be a RFC 822-like value
  (e.g. "Joe Smith &lt;someone@example.com&gt;").
  (<a href="https://bugzilla.mozilla.org/show_bug.cgi?id=787620">bug 787620</a>).</p>
</li>
<li>
<p>We can now mark individual repositories as read-only.
  (<a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1183282">bug 1183282</a>).</p>
</li>
<li>
<p>We can now mark all repositories read-only (useful during maintenance
  events).
  (<a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1183282">bug 1183282</a>).</p>
</li>
<li>
<p>Pushlog commit list view only shows first line of commit message.
  (<a href="https://bugzilla.mozilla.org/show_bug.cgi?id=666750">bug 666750</a>).</p>
</li>
</ul>
<p>Please report any issues in their respective bugs. Or if it is an
emergency, #vcs on irc.mozilla.org.</p>]]></content:encoded>
    </item>
    <item>
      <title>Cloning Improvements in Mercurial 3.6</title>
      <link>http://gregoryszorc.com/blog/2015/10/22/cloning-improvements-in-mercurial-3.6</link>
      <pubDate>Thu, 22 Oct 2015 05:00:00 PDT</pubDate>
      <category><![CDATA[Mercurial]]></category>
      <category><![CDATA[Mozilla]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2015/10/22/cloning-improvements-in-mercurial-3.6</guid>
      <description>Cloning Improvements in Mercurial 3.6</description>
      <content:encoded><![CDATA[<p>Mercurial 3.6 (scheduled for release on or shortly after November 1)
contains a number of improvements to cloning. In this post, I will
describe a new feature to help server operators reduce load (while
enabling clients to clone faster) and some performance work to make
clone operations faster on the client.</p>
<p>Cloning repositories can incur a lot of load on servers. For
<a href="https://hg.mozilla.org/mozilla-central">mozilla-central</a> (the main
Firefox repository), clones require the server to spend 4+ minutes CPU
time and send ~1,230 MB over the network. Multiply by thousands of
clients from build and test automation and developers, and you could
quickly finding yourself running out of CPU cores or network bandwidth.
Scaling Mercurial servers (like many services) can therefore be
challenging. (It's worth noting that Git is in the same boat for
reasons technically similar to Mercurial's.)</p>
<p>Mozilla previously implemented a Mercurial extension to
<a href="/blog/2015/05/29/faster-cloning-from-hg.mozilla.org-with-server-provided-bundles/">seed clones from pre-generated bundle files</a>
so the Mercurial servers themselves don't have to work very hard for
an individual clone. (That linked post goes into the technical
reasons why cloning is expensive). We now offload cloning of frequently
cloned repositories on <a href="https://hg.mozilla.org/">hg.mozilla.org</a> to
<a href="/blog/2015/07/08/cloning-from-s3/">Amazon S3</a> and
<a href="/blog/2015/09/01/serving-mercurial-clones-from-a-cdn/">a CDN</a> and
are diverting 1+ TB/day and countless hours of CPU work away from the
hg.mozilla.org servers themselves.</p>
<p>The positive impact from seeding clones from pre-generated,
externally-hosted bundles has been immense. Load on hg.mozilla.org
dropped off a cliff. Clone times on clients became a lot faster (mainly
because they aren't waiting for a server to dynamically generate/stream
bits). But there was a problem with this approach: it required the
cooperation of clients to install an extension in order for clone load
to be offloaded. It didn't <em>just work</em>.</p>
<p><strong>I'm pleased to announce that the ability to seed clones from
server-advertised pre-generated bundles is now a core feature in
Mercurial 3.6!</strong> Server operators can install the
<a href="https://selenic.com/repo/hg/file/default/hgext/clonebundles.py">clonebundles extension</a> (it is distributed with Mercurial)
to advertise the location of pre-generated, externally-hosted bundle
files. Compatible clients will automatically clone from the
server-advertised URLs instead of creating potentially excessive load
on the Mercurial server. The implementation is almost identical to what
Mozilla has deployed with great success. <strong>If you operate a Mercurial
server that needs to serve larger repositories (100+ MB) and/or is under
high load, you should be jumping with joy at the existence of this
feature, as it should make scaling problems attached to cloning mostly
go away.</strong></p>
<p>Documentation for server operators is currently in the extension and
can be accessed at the aforementioned URL or with
<em>hg help -e clonebundles</em>. It does require a bit of setup work. But
if you are at the scale where you could benefit from the feature, the
results will almost certainly be worth it.</p>
<p>One caveat is that the feature is currently behind an <em>experimental</em>
flag on the client. This means that it doesn't <em>just work</em> yet. This
is because we want to reserve the right to change some behaviors without
worrying about backwards compatibility. However, I'm pretty confident
the server parts won't change significantly. Or if they do, I'm pretty
committed to providing an easy transition path since I'll need one for
hg.mozilla.org. So, I'm giving server operators a tentative
green light to deploy this extension. I can't guarantee there won't be a
few bumps transitioning to a future release. But it shouldn't be a
break-the-world type of problem. It is my intent to remove the
experimental flag and have the feature enabled by default in
Mercurial 3.7. At that point, server operators just need clients to run
a modern Mercurial release and they can count on drastically reduced
load from cloning.</p>
<p>To help with adoption and testing of the <em>clone bundles</em> feature,
servers advertising bundles will inform compatible clients of the
existence of the feature when they clone:</p>
<div class="pygments_murphy"><pre><span></span>$ hg clone https://hg.mozilla.org/mozilla-central
requesting all changes
remote: this server supports the experimental &quot;clone bundles&quot; feature that should enable faster and more reliable cloning
remote: help test it by setting the &quot;experimental.clonebundles&quot; config flag to &quot;true&quot;
adding changesets
adding manifests
adding file changes
...
</pre></div>

<p>And if you have the feature enabled, you'll see something like:</p>
<div class="pygments_murphy"><pre><span></span>$ hg clone https://hg.mozilla.org/mozilla-central
applying clone bundle from https://hg.cdn.mozilla.net/mozilla-central/daa7d98525e859d32a3b3e97101e129a897192a1.gzip.hg
adding changesets
adding manifests
adding file changes
added 265986 changesets with 1501210 changes to 223996 files
finished applying clone bundle
searching for changes
adding changesets
adding manifests
adding file changes
added 1 changesets with 1 changes to 1 files
</pre></div>

<p>This new <em>clone bundles</em> feature is deployed on hg.mozilla.org. Users of
Mercurial 3.6 can start using it today by cloning from one of the
<a href="https://hg.cdn.mozilla.net/">repositories with bundles enabled</a>. (If
you have previously installed the <em>bundleclone</em> extension, please be
sure your <em>version-control-tools</em> repository is up to date, as the
extension was recently changed to better interact with the official
feature.)</p>
<p>And that's the <em>clone bundles</em> feature. I hope you are as excited about
it as I am!</p>
<p>Mercurial 3.6 also contains numerous performance improvements that make
cloning faster, regardless of whether you are using <em>clone bundles</em>! For
starters:</p>
<ul>
<li><a href="https://selenic.com/repo/hg/rev/836291420d53">Caching just-added entries</a>
  made changelog writing 25% faster.</li>
<li><a href="https://selenic.com/repo/hg/rev/39d643252b9f">Reusing file handles</a>
  when adding revlog entries drastically reduced the number of file
  opens, closes, and writes.</li>
<li><a href="https://selenic.com/repo/hg/rev/56a640b0f656">Avoiding excessive file flushing</a>
  when adding revlog entries drastically reduced system call count.</li>
</ul>
<p>These performance enhancements will make all operations that write
new repository data faster. But it will be felt most on clone and pull
operations on the client and push operations on the server.</p>
<p>One of the most impressive performance optimizations was to a Python
class that converts a generator of raw data chunks to something that
resembles a file object so it can be read() from.
<a href="https://selenic.com/repo/hg/rev/6ae14d1ca3aa">Refactoring read()</a>
to avoid <a href="https://docs.python.org/2/library/collections.html#collections.deque">collections.deque</a>
operations and an extra string slice and allocation made <em>unbundle</em>
operations 15-20% faster. Since this function can handle hundreds of
megabytes or even gigabytes of data across hundreds of thousands of
calls, small improvements like this can make a huge difference! This
patch was a stark reminder that function calls, collection mutations,
string slicing, and object allocation all can have a significant cost
in a higher-level, garbage collected language like Python.</p>
<p>The end result of all this performance optimization on applying a
mozilla-central gzip bundle on Linux on an i7-6700K:</p>
<ul>
<li>35-40s wall time faster (~245s to ~205s) (~84% of original)</li>
<li>write(2) calls reduced from 1,372,411 to 679,045 (~49% of
  original)</li>
<li>close(2) calls reduced from 405,147 to 235,039 (~58% of
  original)</li>
<li>total system calls reduced from 5,120,893 to 2,938,479 (~57% of
  original)</li>
</ul>
<p>And the same operation on Windows 10 on the same machine:</p>
<ul>
<li>~300s wall time faster (933s to 633s) (~68% of original)</li>
</ul>
<p>You may have noticed the discrepancy between Linux and Windows wall
times, where Windows is 2-4x slower than Linux. What gives? The reason
is closing file handles that have been appended to is slow on Windows.
For more, read my <a href="/blog/2015/10/22/append-i/o-performance-on-windows/">recent blog post</a>.</p>
<p>Mercurial writes ~226,000 files during a clone of mozilla-central
(excluding the working copy). Assuming 2ms per file close operation,
that comes out to <strong>~450s just for file close operations</strong>! (All
operations are on the same thread.) The current wall time difference
between clone times on Windows and Linux is ~428s. So it's fair to say
that waiting on file closes accounts for most of this.</p>
<p>Along the same vein, the aforementioned performance work reduced total
number of file close operations during a mozilla-central clone by
~165,000. Again assuming 2ms per file close, that comes to ~330s, which
is in the same ballpark as the ~300s wall time decrease we see on
Windows in Mercurial 3.6. <strong>Writing - and therefore closing - hundreds
of thousands of files handles is slower on Windows and accounts for most
of the performance difference on that platform.</strong></p>
<p>Empowered by this knowledge, I wrote some patches to move file closing
to a background thread on Windows. The results were
<a href="https://selenic.com/pipermail/mercurial-devel/2015-September/073788.html">promising</a>
(minutes saved when writing 100,000+ files). Unfortunately, I didn't
have time to finish these patches for Mercurial 3.6. Hopefully they'll
make it into 3.7. I also have some mad scientist ideas for alternate
storage mechanisms that don't rely on hundreds of thousands of files.
This should enable clones to run at 100+ MB/s on all platforms -
basically as fast as your network and system I/O can keep up (yes,
Python and Windows are capable of this throughput). Stay tuned.</p>
<p>And that's a summary of the cloning improvements in Mercurial 3.6!</p>
<p>Mercurial 3.6 is currently in release candidate. Please help test
it by downloading the RC at
<a href="https://www.mercurial-scm.org/">https://www.mercurial-scm.org/</a>.
Mercurial 3.6 final is due for release on or shortly after November 1.
There is a large gathering of Mercurial contributors in London this
weekend. So if a bug is reported, I can pretty much guarantee a lot of
eyeballs will see it and there's a good chance it will be acted upon.</p>]]></content:encoded>
    </item>
    <item>
      <title>Append I/O Performance on Windows</title>
      <link>http://gregoryszorc.com/blog/2015/10/22/append-i/o-performance-on-windows</link>
      <pubDate>Thu, 22 Oct 2015 02:15:00 PDT</pubDate>
      <category><![CDATA[Mozilla]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2015/10/22/append-i/o-performance-on-windows</guid>
      <description>Append I/O Performance on Windows</description>
      <content:encoded><![CDATA[<p>A few weeks ago, some coworkers were complaining about the relative
performance of Mercurial cloning on Windows. I investigated on my
brand new i7-6700K Windows 10 desktop machine and sure enough they were
correct: cloning times on Windows were several minutes slower than
Linux on the same hardware. What gives?</p>
<p>I performed a few clones with Python under a profiler. It pointed to a
potential slowdown in file I/O. I wanted more details so I fired up
<a href="https://technet.microsoft.com/en-us/library/bb896645.aspx">Sysinternals Process Monitor</a>
(strace for Windows) and captured data for a clone.</p>
<p>As I was looking at the raw system calls related to I/O, something
immediately popped out: <em>CloseFile()</em> operations were frequently
taking 1-5 <em>milliseconds</em> whereas other operations like opening, reading,
and writing files only took 1-5 <em>microseconds</em>. <strong>That's a 1000x
difference!</strong></p>
<p>I wrote a custom Python script to analyze an export of Process Monitor's
data. Sure enough, it said we were spending hundreds of seconds in
<em>CloseFile()</em> operations (it was being called a few hundred thousand
times). I posted the findings to some mailing lists.
<a href="https://groups.google.com/d/msg/mozilla.dev.platform/yupx2ToQ5T4/WAMC_Q-DCAAJ">Follow-ups</a>
in Mozilla's dev-platform list pointed me to
<a href="http://blogs.msdn.com/b/oldnewthing/archive/2011/09/23/10215586.aspx">an old MSDN blog post</a>
where it documents behavior similar to what I was seeing.</p>
<p>Long story short, <strong>closing file handles that have been appended to is
slow on Windows.</strong> This is apparently due to an implementation detail of
NTFS. Writing to a file in place is fine and only takes microseconds for
the open, write, and close. But if you append a file, closing the
associated file handle is going to take a few milliseconds. Even if you
are using Overlapped I/O (async I/O on Windows), the <em>CloseHandle()</em>
call to close the file handle blocks the calling thread! Seriously.</p>
<p>This behavior is in stark contrast to Linux and OS X, where system I/O
functions take microseconds (assuming your I/O subsystem can keep up).</p>
<p>There are two ways to work around this issue:</p>
<ol>
<li>Reduce the amount of file closing operations on appended files.</li>
<li>Use multiple threads for I/O on Windows.</li>
</ol>
<p>Armed with this knowledge, I dug into the guts of Mercurial and
proceeded to write a
<a href="https://selenic.com/repo/hg/rev/836291420d53">number</a>
<a href="https://selenic.com/repo/hg/rev/39d643252b9f">of</a>
<a href="https://selenic.com/repo/hg/rev/56a640b0f656">patches</a>
that drastically reduced the amount of file I/O system calls during
clone and pull operations. While I intend to write a blog post with the
full details, <strong>cloning the Firefox repository with Mercurial 3.6 on
Windows is now several minutes faster.</strong> Pretty much all of this is due
to reducing the number of file close operations by aggressively reusing
file handles.</p>
<p>I also
<a href="https://selenic.com/pipermail/mercurial-devel/2015-September/073788.html">experimented</a>
with moving file close operations to a separate thread on Windows. While
this change didn't make it into Mercurial 3.6, the results were very
promising. Even on Python (which doesn't have real asynchronous threads
due to the GIL), moving file closing to a background thread freed up the
main thread to do the CPU heavy work of processing data. This made clones
several minutes faster. (Python does release the GIL when performing an
I/O system call.) Furthermore, <strong>simply creating a dedicated thread for
closing file handles made Mercurial faster than 7-zip at writing tens of
thousands of files from an uncompressed tar archive</strong>. (I'm not going to
post the time for <em>tar</em> on Windows because it is embarassing.) That's a
Python process on Windows faster than a native executable that is lauded
for its speed (7-zip). Just by offloading file closing to a single
separate thread. Crazy.</p>
<p>I can optimize file closing in Mercurial all I want. However,
Mercurial's storage model relies on several files. For the Firefox
repository, we have to write ~225,000 files during clone. Assuming
1ms per file close (which is generous), that's 225s (or 3:45) wall
time performing file closes. That's not going to scale. I've already
started experimenting with alternative storage modes that initially use
1-6 files. This should enable Mercurial clones to run at over 100 MB/s
(yes, Python and Windows can do I/O that quickly if you are smart about
things).</p>
<p>My primary takeaway is that creating/appending to thousands of files
is slow on Windows and should be addressed at the architecture level
by not requiring thousands of files and at the implementation level
by minimizing the number of file close operations after write.
If you absolutely must create/append to thousands of files, use multiple
threads for at least closing file handles.</p>
<p>My secondary takeaway is that Sysinternals Process Monitor is amazing.
I used it against Firefox and immediately found
<a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1211090">performance concerns</a>.
It can be extremely eye opening to see how your higher-level code
is translated into function calls into your operating system and where
the performance hot spots are or aren't at the OS level.</p>]]></content:encoded>
    </item>
    <item>
      <title>Lowering the Barrier to Pushing to MozReview</title>
      <link>http://gregoryszorc.com/blog/2015/10/14/lowering-the-barrier-to-pushing-to-mozreview</link>
      <pubDate>Wed, 14 Oct 2015 12:30:00 PDT</pubDate>
      <category><![CDATA[MozReview]]></category>
      <category><![CDATA[Mozilla]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2015/10/14/lowering-the-barrier-to-pushing-to-mozreview</guid>
      <description>Lowering the Barrier to Pushing to MozReview</description>
      <content:encoded><![CDATA[<p>Starting today, a Mozilla LDAP account with Mercurial SSH access is no
longer required to <em>hg push</em> into
<a href="https://reviewboard.mozilla.org">MozReview</a> to initiate code review
with Mozilla projects.</p>
<p>The <a href="https://mozilla-version-control-tools.readthedocs.org/en/latest/mozreview/install.html">instructions for configuring your client to use MozReview</a>
have been updated to reflect how you can now push to MozReview over HTTP
using a Bugzilla API Key for authentication.</p>
<p>This change effectively enables first-time contributors to use MozReview
for code review. Before, you had to obtain an LDAP account and configure
your SSH client, both of which could be time consuming processes and
therefore discourage people from contributing. (Or you could just use
Bugzilla/Splinter and not get the benefits of MozReview, which many
did.)</p>
<p><strong>I encourage others to update contribution docs to start nudging people
towards MozReview over Bugzilla/patch-based workflows</strong> (such as
bzexport).</p>
<p><a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1195856">Bug 1195856</a>
tracked this feature.</p>]]></content:encoded>
    </item>
  </channel>
</rss>

<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0"
     xmlns:content="http://purl.org/rss/1.0/modules/content/"
     xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
     xmlns:atom="http://www.w3.org/2005/Atom"
     xmlns:dc="http://purl.org/dc/elements/1.1/"
     xmlns:wfw="http://wellformedweb.org/CommentAPI/"
     >
  <channel>
    <title>Gregory Szorc's Digital Home</title>
    <link>http://gregoryszorc.com/blog</link>
    <description>Rambling on</description>
    <pubDate>Thu, 06 Feb 2014 00:24:51 GMT</pubDate>
    <generator>Blogofile</generator>
    <sy:updatePeriod>hourly</sy:updatePeriod>
    <sy:updateFrequency>1</sy:updateFrequency>
    <item>
      <title>The Mercurial Revlog</title>
      <link>http://gregoryszorc.com/blog/2014/02/05/the-mercurial-revlog</link>
      <pubDate>Wed, 05 Feb 2014 16:26:00 PST</pubDate>
      <category><![CDATA[Mercurial]]></category>
      <category><![CDATA[Mozilla]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2014/02/05/the-mercurial-revlog</guid>
      <description>The Mercurial Revlog</description>
      <content:encoded><![CDATA[<p>Mercurial stores lots of its data in append-only,
intended-to-be-immutable data structures called <em>revlogs</em>. Each revlog is
backed by a file on the filesystem. The main component of a revlog
is a <em>revision</em>. When a new revision arrives, its content is compared
against the previous revision in the file and a zlib-compressed
delta/diff is generated. If the delta is smaller than the entry
itself, the delta is appended to the revlog. If the compressed delta
itself or the stream of deltas that must be applied to recreate the
original text is larger than the source text, the compressed full content
is appended to the revlog. In other words, Mercurial tries to maintain
a balance between file size/growth rate and read times in terms of
both I/O and CPU.</p>
<p>The <em>changelog</em> is a <em>revlog</em> that holds information about every
changeset/commit in the repository. The <em>manifest</em> is a <em>revlog</em> that
holds details about which files are present in every changeset. There
exist a separate <em>revlog</em> for each file/path ever present in the
repository. These are called <em>filelogs</em>.</p>
<p>When you commit a change touching a single file, Mercurial will append
a revision to the changelog describing the commit, a revision to the
manifest describing the set of files active in that specific commit,
and a new revision to a filelog.</p>
<p>You can poke around your repository's revlogs by running some debug
commands. For example:</p>
<pre><code># Show high-level information about the manifest revlog
$ hg debugrevlog -m

# Show details about each entry in the manifest revlog (this
# actually reads data from an index to the data revlog - pretend
# there aren't index files for now)
$ hg debugindex -m

# Dump the content of a single entry in the changelog.
$ hg debugdata -c 2
</code></pre>
<p>I'm generally a big fan of append-only data structures because I love
the beneficial caching properties and linear I/O performance.
Mercurial and the revlog can take advantage of these properties for
some performance wins under key scenarios, especially server
operation (pushing commits doesn't necessarily invalidate cached revlog
entries, allowing the page cache to service many reads).</p>
<h2>Edge cases and deficiencies</h2>
<p>As with any system trying to solve a complex problem, the revlog
storage format doesn't always work as well as intended.</p>
<p>Let's start by taking by looking at how revlogs do delta storage. I said
earlier that revlogs try to store a compressed delta against the
previous entry. Well, that previous entry is the previous physical
revision in the file/revlog, not the parent revision. You see,
entries/revisions in revlogs have a numeric index (0, 1, 2, 3, ...),
a SHA-1 hash of their content, and a link to the logical <em>parent</em> of
the entry. e.g. say you have two commits in your repo, one made after
the other:</p>
<pre><code>0:8ba995b74e18
1:9b2a99adc05e
</code></pre>
<p>The revlog has entries at indexes 0 and 1. They are also accessible
by their hash/node values of 8ba995b74e18 and 9b2a99adc05e. The parent
of 1 is 0. The entry for 0 is the full, compressed content of 0. The
entry for 1 is likely a compressed delta from 0 to 1. As we commit,
we keep appending new entries. These exist as compressed deltas until
the sizes of 0..n is greater than n by itself. At that time, a
compressed version of n is stored.</p>
<p>This model works great for linear histories, as changes from n to n+1
are usually small. Mercurial can store a very small delta for each
entry. The world is good.</p>
<p>This model works great when entries in revlogs are small. By having
small entries (and small changes), the number of deltas required to
eclipse the size of a single entry remains small, in effect keeping the length
of a <em>delta chain</em> in check. Limiting the length of delta chains is good
because it keeps the cost of looking up a single revision's content low.
If you have a delta chain of 1000, for example, Mercurial will need to read
1000 revlog entries and apply 1000 deltas to obtain the original value.
That can get expensive computationally. Since reads are linear, I/O
should remain in check. But you do need to scan a lot of bytes to read
in all the deltas and you need to perform a lot of the same computations
(such as zlib decompression).</p>
<p>Let's talk a little about where the revlog starts to break down.</p>
<p>First, there's the issue of multiple, interleaved branches in the
revlog. Say we have a repository with many branches/heads. We alternate
committing between all the heads. This can play havoc with the default
revlog delta compression. Since revlogs compress against the previous
<em>physical</em> entry in the revlog, if there is lots of alternating between
branches and the contents of those branches diverges significantly, the
deltas can grow quite large and full revisions will be stored with
higher frequency. This means more storage space and more CPU tasked
with resolving deltas (since deltas are larger). Although, CPU is kept
in check since delta chains tend to be smaller since full revisions are
stored with higher frequency.</p>
<p>Second, we have an issue with delta chain explosion of large entries
with small turnover. If your base content is large and it isn't changing
that much, it will take hundreds or even thousands of revisions before
the sum of the delta sizes outgrows that of an entry. This means delta
chains can be very long and Mercurial will have to spend a lot of CPU
to resolve a single entry.</p>
<p>Third, revlogs are using zlib for compression. As many
<a href="http://pokecraft.first-world.info/wiki/Quick_Benchmark:_Gzip_vs_Bzip2_vs_LZMA_vs_XZ_vs_LZ4_vs_LZO">benchmarks</a>
have shown, zlib isn't the fastest or most efficient compression
algorithm in the world. It's likely justified as a reasonable default.
But alternatives exist. The choice of zlib has implications, especially
when other factors (such as excessively long delta chains) come into
play.</p>
<p>Let's talk about mitigation strategies.</p>
<h2>True parent deltas</h2>
<p>While I wasn't around for the original design decisions of revlogs, I'm
guessing they were strongly influenced by the fact that sequential I/O
on magnetic disks is much, much faster than random I/O. With SSDs and
flash storage growing in popularity - a medium that offers random I/O
commonly over 100 MB/s - this buys us the luxury of asking <em>do revlogs
need to be optimized for sequential I/O</em> and <em>how can revlogs change to
take advantage of fast random I/O</em>.</p>
<p>One of the ways revlogs can adapt to fast random I/O is to store the
delta against the logical parent, not the physical. The delta chain will
thus <em>skip</em> revisions in the revlog. e.g. the chain could be 1, 2, 5, 6,
10, not n, n+1, n+2, .... This would keep deltas small and would reduce
the overall size of the revlog. Although, it would likely increase the
length of delta chains, especially when dealing with non-linear
histories.</p>
<p>It turns out Mercurial has a setting that enables this -
<strong>format.generaldelta</strong>. To create a repository with this enabled, run:</p>
<pre><code>$ hg --config format.generaldelta=true init path/to/repo
</code></pre>
<p>The revlogs in that repository with now have deltas computed against the
logical parent!</p>
<p>To verify you are using general delta, look for <strong>generaldelta</strong> in the
<em>.hg/requires</em> file. If it isn't there, you probably don't have
generaldelta enabled. <strong>Please note that cloning a generaldelta repo
won't necessarily give your repo generaldelta. You need to have the
custom config option set on the client or the client will likely use the
defaults.</strong></p>
<p>On repositories with lots of interleaved heads, this can make a huge
difference. As a pathalogical example, Mozilla's
<a href="https://hg.mozilla.org/try/">Try</a> repository (where people push heads
to trigger test/automation runs) has over 22,000 heads. The on-disk
size of the repository is 3117 MB with the standard settings and 2139 MB
with generaldelta! The bulk of this difference comes from the manifest
revlog, which is 954 MB smaller with generaldelta.</p>
<h2>Alternate compression format</h2>
<p>Mercurial uses zlib for revlog compression by default. This is a safe
choice. It's relatively fast and yields relatively good compression.</p>
<p>Since Mercurial is highly extensible, it's possible to plug in a custom
compression format for revlogs. Facebook's
<a href="https://bitbucket.org/facebook/lz4revlog/">lz4revlog extension</a> will
use lz4 for compression. lz4 is faster than zlib, but compression isn't
as good. Repositories with lz4 are commonly ~1.5x larger. But CPU bound
tasks can be significantly faster.</p>
<p>In theory, any compression format can be used. However, it's not
trivially selectable in Mercurial (yet), so someone will need to provide
an extension that implements your desired compression format.</p>
<h2>Alternate storage backends</h2>
<p>In theory, Mercurial revlogs can be backed by anything. This is the
extensibility of Mercurial at work. There's just a Mercurial extension
sitting between using SQL, S3, Cassandra, or any other storage backend
for revlogs.</p>
<p>It's also possible to write custom revlog implementations that change
the file layout for interesting scaling possibilities. For example,
modern filesystems like ZFS and btrfs support block-level deduplication
and transparent compression. If you had block-aligned revlog entries
with deduplication enabled, servers could in theory only store each
revision at most once. Another idea is to let the filesystem handle
the compression. This would cause compression to occur in kernel space
(rather than inside Python userspace). This may have beneficial
performance properties. It may not. It may depend on the repository.
These would be interesting experiments to conduct!</p>
<h2>Caveat with alternate revlog implementations</h2>
<p>Before you go experimenting with alternative revlog implementations,
be forewarned that wall time performance for push and pull operations
may suffer!</p>
<p>Currently, Mercurial isn't as smart as it could be when it comes to
transferring <em>bundles</em> of changeset data between repositories. Let me
explain.</p>
<p>When Mercurial transfers changeset data between repositories, it often
uses an encoding format called a <a href="http://mercurial.selenic.com/wiki/BundleFormat">bundle</a>.
A bundle is effectively a custom archive format for revlog data. If
you've ever used <em>hg bundle</em> or <em>hg unbundle</em> to transfer repository
data, you've explicitly interacted with bundles. But what's lesser known
is that the <em>hg push</em> and <em>hg pull</em> operations also transfer bundles.
The only major difference is that they are created on the fly and their
existence is hidden from view.</p>
<p>In theory, bundles can be encoded a number of different ways. The most
common tunable is the compression format. Over the wire, bundles are
zlib (or even uncompressed). If you run <em>hg bundle</em>, you'll likely
produce a bzip2 bundle. (This is why <em>hg unbundle</em> can be slower than
<em>hg clone</em> - the CPU time spent for bzip2 is much greater than for
zlib.) But a problem with bundles as they exist today is that the format
is rather static when it comes to what's transferred over the wire.
Unless you've mucked about with settings, the client and server will
send a zlib-compressed bundle using the physical revlog order. In other
words, the bundle format is the Mercurial defaults.</p>
<p>If Mercurial were completely dumb, transferring bundles would involve
1) determining the full text of a revlog entry 2) compressing that entry
into a bundle 3) sending that data to a peer 4) decompressing that entry
5) appending that entry to the appropriate revlog (which involves
recompression). Fortunately, Mercurial is a bit smarter than that:
Mercurial can detect when compressed bits in a bundle will match what
is on disk and will avoid the unncessary compression operations.</p>
<p>Anyway, the current bundle transfer mechanism falls apart when there is
a mismatch between client and server configurations or even when the
server or client is using non-defaults. You can think of this as a
revlog impedance mismatch. Essentially, when the client and server are
operating with different or uncommon types of revlogs, Mercurial tends
to default to the lowest common denominator, or zlib deltas against the
physical parent. If, for example, a client wants to use generaldelta
with a server employing defaults, the client will have to convert the
server's deltas into generaldelta deltas. This requires a non-trivial
amount of CPU and pull operations become slower. I believe the opposite
is also true: generaldelta clients will emit generaldelta bundles,
causing the server to recompute the deltas.</p>
<p><strong>When it comes to custom revlog formats today, essentially nobody
wins.</strong></p>
<p>The good news is this will be fixed. There is an
<a href="http://mercurial.selenic.com/wiki/BundleFormat2">effort</a> to improve the
bundle format and wire protocols to make matters better. A little
protocol negotiation can go a long way to make the situation a lot
better. That said, there is still the underlying problem that some
clients may want settings that differ from the server's. e.g. clients
with SSDs likely want generaldelta because they don't need sequential
I/O and SSD space is more expensive, so the smaller repository sizes
achieved with generaldelta are appreciated. But a server operator may
not want to force generaldelta on all clients because it would make
clients on mechanical hard drives slower! The point is revlog impedance
mismatch will occur and someone needs to spend the CPU cycles to rectify
the matter. I suspect this will be pushed to clients since distributed
CPU load is easier to deal with than centralized on the server. But, I
wouldn't be surprised if Mercurial allowed server operators to configure
the behavior. It's a hard problem. Time will tell.</p>
<p>In the mean time, just know that if you experiment with custom revlog
settings, push and pull operations will likely be slower. You may not
notice this on day-to-day operations. But on things like initial clone,
you could experience a massive slow-down.</p>
<p>Here's some timings with
<a href="https://hg.mozilla.org/mozilla-central">mozilla-central</a> with a
Mercurial 1.9 server and client on the same SSD-backed multi-core machine:</p>
<ul>
<li>clone default settings - 4:25</li>
<li>clone --uncompressed - 0:49</li>
<li>clone from generaldelta - 14:11</li>
<li>clone from generaldelta to generaldelta - 16:49</li>
<li>clone from generaldelta --uncompressed - 0:50</li>
<li>pull 2250 changesets, default from default - 6.7s</li>
<li>pull 2250 changesets, default from generaldelta - 17.9s</li>
<li>pull 2250 changesets, generaldelta from default - 28.5s</li>
<li>pull 2250 changesets, generaldelta from generaldelta - 20.0s</li>
</ul>
<p>As you can see, anything involving compression and generaldelta is
much slower. Once <em>bundle format 2</em> is fully implemented, I expect the
situation to improve.  Until then, know that you'll get a ~2.5-4x
slowdown from using generaldelta. You'll have to measure other
revlog formats for their impact.</p>
<p>In conclusion, Mercurial's revlog file format is an interesting and
tunable data structure. It works pretty well for most repositories. But
if you are the 1%, you might want to spend some time to investigate
changing its default configuration.</p>]]></content:encoded>
    </item>
    <item>
      <title>Review Board at Mozilla</title>
      <link>http://gregoryszorc.com/blog/2014/01/27/review-board-at-mozilla</link>
      <pubDate>Mon, 27 Jan 2014 16:30:00 PST</pubDate>
      <category><![CDATA[Mozilla]]></category>
      <category><![CDATA[mach]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2014/01/27/review-board-at-mozilla</guid>
      <description>Review Board at Mozilla</description>
      <content:encoded><![CDATA[<p>Some Mozillians recently stood up an
<a href="https://reviewboard.allizom.org/">instance</a> of
<a href="http://www.reviewboard.org/">Review Board</a> - a web-based code review
tool for evaluation purposes. Having used Review Board at a previous
company, I can say with high confidence that when properly configured,
it beats the pants off Splinter (the code review interface baked into
Bugzilla). Here are some advantages:</p>
<ul>
<li>The HTML interface is more pleasant on the eyes (subjective).</li>
<li>Interdiffs actually work.</li>
<li>Intra-line diffs are rendered.</li>
<li>You can open <em>issues</em> for individual review comments and these
  issues can be tracked during subsequent reviews (Bugzilla doesn't
  really have anything similar and review comments tend to get lost
  unless the reviewer is sharp).</li>
<li>It integrates with the VCS, so you can expand code context from the
  review interface.</li>
<li>There are buttons to toggle whitespace differences.</li>
<li>Syntax hightlighting! It even recognizes things like <em>TODO</em> in
  comments.</li>
</ul>
<p>You can read more from the <a href="http://www.reviewboard.org/docs/manual/1.7/users/">official user guide</a>.</p>
<p>If you have any interest in evaluating Review Board, the easiest way
to upload patches to Mozilla's instance is to run <strong>mach rbt</strong>.</p>
<p><strong>mach rbt</strong> will launch the Review Board tools command-line client
(called RBTools). From there, you can do a number of things. Run
<strong>mach rbt help</strong> to see the full list.</p>
<p>Here are some examples:</p>
<pre><code># See a diff that would be uploaded to Review Board:
$ mach rbt diff

# Create a review request based on the current Mercurial changeset:
$ mach rbt post

# That should print out a URL to the not-yet-published review
# request. If you go to that URL, you'll notice that the fields
# in that request are all empty.

# Next time, you can have some fields auto-populate by running:
$ mach rbt post --guess-summary --guess-description

# This grabs info from the commit message.

# To update an existing review request (e.g. to submit a new patch):
$ mach rbt post -r &lt;review id&gt;

# (where &lt;review ID&gt; is the ID of the review).

# You can also have it generate a "squashed" patch from multiple
# commits:
$ mach rbt post 164123::164125
</code></pre>
<p>Run <strong>mach rbt help post</strong> for more options. Also see the
<a href="http://www.reviewboard.org/docs/rbtools/dev/">RBTools documentation</a>
for more.</p>
<p>It's worth noting that <strong>mach rbt</strong> will download an unreleased version
of RBTools. This is because the released version doesn't work well with
Mercurial. I contributed a handful of patches to RBTools to make
Mercurial work better.</p>
<p>Before you dive in and start using Review Board for actual code review,
there are some things you need to know:</p>
<ul>
<li>Mozilla's Review Board instance does not yet send emails on changes.
  <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=958236">Bug 958236</a>
  tracks this. When it works, you'll see nice emails, just like you do
  for Bugzilla reviews.</li>
<li>Review Board doesn't currently interact with Bugzilla that well. In
  theory, we could have Review Board update corresponding Bugzilla bugs
  when actions are performed. Someone just needs to write this code
  and get it deployed.</li>
<li>If you create a Bugzilla attachment that contains the URL of a Review
  Board review (e.g. https://reviewboard.allizom.org/r/23/), Bugzilla
  will automatically set the MIME type as a Review Board review and set
  up an HTML redirect when the attachment is <em>downloaded</em> via the
  browser. You can even set <strong>r?</strong> on this attachment to have Bugzilla
  nag about reviews. See <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=875562">bug 875562</a>
  for an example.</li>
<li>There is currently no way to upload a patch to Review Board and update
  Bugzilla is one go. I have proof-of-concept code for this. Although,
  there is <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=947596">pushback</a>
  on getting that checked in.</li>
<li>Review Board 2 is in development. It has a number of new and exciting
  features. And it looks better.</li>
</ul>
<p>Finally and most importantly, <strong>Review Board at Mozilla is still in
evaluation mode.</strong> It's usage has not been officially blessed as far
as I know. I don't believe the SLA is as high as other services (like
Bugzilla). Nobody is really using it yet. It still needs a lot of
polish and integration for it to realize its potential. And, there is
some <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=947596">talk</a>
about the future of code review at Mozilla that may or may not involve
Review Board. In short, the future of Review Board at Mozilla is
uncertain. I wouldn't rely on it to archive review comments from super
important reviews/decisions.</p>
<p>Despite the shortcomings, I encourage people to play around with Review
Board. If nothing else, at least <a href="https://reviewboard.allizom.org/r/24/diff/">gaze upon it's patch rendering
beauty</a> and witness
what the future could hold.</p>]]></content:encoded>
    </item>
    <item>
      <title>Aggregating Version Control Info at Mozilla</title>
      <link>http://gregoryszorc.com/blog/2014/01/21/aggregating-version-control-info-at-mozilla</link>
      <pubDate>Tue, 21 Jan 2014 10:50:00 PST</pubDate>
      <category><![CDATA[Git]]></category>
      <category><![CDATA[Mercurial]]></category>
      <category><![CDATA[Mozilla]]></category>
      <category><![CDATA[Python]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2014/01/21/aggregating-version-control-info-at-mozilla</guid>
      <description>Aggregating Version Control Info at Mozilla</description>
      <content:encoded><![CDATA[<p>Over the winter break, I set out on an ambitious project to create
a service to help developers and others manage the flury
of patches going into Firefox. While the project is far from complete,
I'm ready to unleash the first part of the project upon the world.</p>
<p>If you point your browsers to
<a href="http://moztree.gregoryszorc.com/">moztree.gregoryszorc.com</a>, you'll
hopefully see some documentation about what I've built.
<a href="https://bitbucket.org/indygreg/moz-tree-utopia">Source code</a> is
available and free, of course. Patches very welcome.</p>
<p>Essentially, I built a centralized indexing service for version
control repositories with Mozilla's extra metadata thrown in.
I tell it what repositories to mirror, and it clones everything,
fetches data such as the pushlog and Git SHA-1 mappings, and
stores everything in a central database. It then exposes this
aggregated data through world-readable web services.</p>
<p>Currently, I have the service indexing the popular project branches
for Firefox (central, aurora, beta, release, esr, b2g, inbound, fx-team,
try, etc). You can view the
<a href="http://moztree.gregoryszorc.com/api/repos">full list</a> via the web
service. As a bonus, I'm also serving these repositories via
<a href="http://hg.gregoryszorc.com/">hg.gregoryszorc.com</a>. My server appears
to be significantly faster than
<a href="https://hg.mozilla.org">hg.mozilla.org</a>. If you want to use it for
your daily needs, go for it. I make no SLA guarantees, however.</p>
<p>I'm also using this service as an opportunity to experiment with
alternate forms of Mercurial hosting. I have mirrors of mozilla-central
and the try repository with generaldelta and lz4 compression enabled.
I may blog about what those are eventually. The teaser is that they can
make Mercurial perform a lot faster under some conditions. I'm also
using ZFS under the hood to manage repositories. Each repository is a
ZFS filesystem. This means I can create repository copies on the server
(user repositories anyone?) at a nearly free cost. Contrast this to the
traditional method of full clones, which take lots of time, memory, CPU,
and storage.</p>
<p>Anyway, some things you can do with the existing web service:</p>
<ul>
<li>Obtain metadata about Mercurial changesets.
  <a href="http://moztree.gregoryszorc.com/api/changeset/940b2974f35b">Example</a>.</li>
<li>Look up metadata about Git commits.
  <a href="http://moztree.gregoryszorc.com/api/git-sha1/40438af67c321">Example</a>.</li>
<li>Obtain a <a href="http://moztree.gregoryszorc.com/api/spore">SPORE descriptor</a>
  describing the web service endpoints. This allows you to auto-generate
  clients from descriptors. Yay!</li>
</ul>
<p>Obviously, that's not a lot. But adding new endpoints is relatively
straightforward. See the <a href="https://bitbucket.org/indygreg/moz-tree-utopia/src/tip/repodata/web/app.py">source</a>.
It's literally as easy as defining a URL mapping and writing a
database query.</p>
<p>The performance is also not the best. I just haven't put in the effort
to tune things yet. All of the querying hits the database, not
Mercurial. So, making things faster should merely be a matter of
database and hosting optimization. Patches welcome!</p>
<p>Some ideas that I haven't had time to implement yet:</p>
<ul>
<li>Return changests in a specific repository</li>
<li>Return recently pushed changesets</li>
<li>Return pushes for a given user</li>
<li>Return commits for a given author</li>
<li>Return commits referencing a given bug</li>
<li>Obtain TBPL URLs for pushes with changeset</li>
<li>Integrate bugzilla metadata</li>
</ul>
<p>Once those are in place, I foresee this service powering a number of
dashboards. Patches welcome.</p>
<p>Again, this service is only the tip of what's possible. There's a lot
that could be built on this service. I have ideas. Others have ideas.</p>
<p>The project includes a Vagrant file and Puppet
manifests for provisioning the server. It's a one-liner to get a
development environment up and running. It should be really easy to
contribute to this project. Patches welcome.</p>]]></content:encoded>
    </item>
    <item>
      <title>Things Mozilla Could Do with Mercurial</title>
      <link>http://gregoryszorc.com/blog/2014/01/17/things-mozilla-could-do-with-mercurial</link>
      <pubDate>Fri, 17 Jan 2014 15:00:00 PST</pubDate>
      <category><![CDATA[Mercurial]]></category>
      <category><![CDATA[Mozilla]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2014/01/17/things-mozilla-could-do-with-mercurial</guid>
      <description>Things Mozilla Could Do with Mercurial</description>
      <content:encoded><![CDATA[<p>As I've <a href="/blog/category/mercurial/">written before</a>, Mercurial is a
highly extensible version control system. You can do things with
Mercurial you can't do in other version control systems.</p>
<p>In this post, I'll outline some of the cool things Mozilla could do with
Mercurial. But first, I want to outline some features of Mercurial that
many don't know exist.</p>
<h2>pushkey and listkeys command</h2>
<p>The Mercurial wire protocol (how two Mercurial peer repositories
talk to each other over a network) contains two very useful commands:
<em>pushkey</em> and <em>listkeys</em>. These commands allow the storage and listing
of arbitrary key-value pair metadata in the repository.</p>
<p>This generic storage mechanism is how Mercurial stores and synchronizes
bookmarks and phases information, for example.</p>
<p>By implementing a Mercurial extension, you can have Mercurial store
key-value data for any arbitrary data namespace. You can then write
a simple extension that synchronizes this data as part of the push
and pull operations.</p>
<h2>Extending the wire protocol</h2>
<p>For cases where you want to transmit arbitrary data to/from Mercurial
servers and where the <em>pushkey</em> framework isn't robust enough, it's
possible to implement custom commands in the Mercurial wire protocol.</p>
<p>A server installs an extension making the commands available. A client
installs an extension knowing how to use the commands. Arbitrary data
is transferred or custom actions are performed.</p>
<p>When it comes to custom commands, the sky is really the limit. You
could do pretty much anything from transfer extra data types (this
is how the <a href="http://mercurial.selenic.com/wiki/LargefilesExtension">largefiles extension</a>
works) to writing commands that interact with remote agents.</p>
<h2>Custom revision set queries and templating</h2>
<p>Mercurial offers a rich framework for querying repository data and
for formatting data. The querying is called <em>revision sets</em> and the
later <em>templates</em>. If you are unfamiliar with the feature, I
encourage you to run <em>hg help revset</em> and <em>hg help templates</em> right
now to discover the awesomeness.</p>
<p>As I've <a href="/blog/2013/11/08/using-mercurial-to-query-mozilla-metadata/">demonstrated</a>,
you can do some very nifty things with custom revision sets and
templating!</p>
<h2>The possibilities</h2>
<p>Now that you know some ways Mercurial can be extended, let's talk about
some cool use cases at Mozilla. I want to be clear that I'm not
advocating we do these things, just that they are possible and maybe
they are a little cool.</p>
<h3>Storing pushlog data</h3>
<p>Mozilla records information about who pushed what changesets where and
when in what's called the <em>pushlog</em>. The pushlog data is currently
stored in a SQLite database inside the repository on the server. The
data is made available via a HTTP+JSON API.</p>
<p>We could go a step further and make the pushlog data available via
<em>listkeys</em> so Mercurial clients could download pushlog data with the
same channel used to pull core repository data. (Currently, we have
to open a new TCP connection and talk to the HTTP+JSON API.) This
would make fetching of pushlog data faster, especially for clients
on slow connections.</p>
<p>I concede this is an iterative improvement and adds little value beyond
what we currently have. But if I were designing pushlog storage from
scratch, this is how I'd do it.</p>
<h3>Storing a changeset's automation results</h3>
<p>The <em>pushkey</em> framework could be used to mark specific changesets
as passing automation. When release automation or a sheriff determines
that a changeset/push is green, they could issue an authenticated
<em>pushkey</em> command to the Mercurial server stating such. Clients
could then easily obtain a list of all changesets that are green.</p>
<p>Why stop there? We could also record automation failures in Mercurial as
well. Depending on how complex this gets, we may outgrow <em>pushkey</em>
and require a separate command. But that's all doable.</p>
<p>Anyway, clients could download automation results for a specific
changeset as part of the repository data. The same extension that
pulls down that data could also monkeypatch the bisection algorithm used
by <em>hg bisect</em> to automatically skip over changesets that didn't pass
automation. You'll never bisect a backed out changeset again!</p>
<p>If this automation data were stored on the Try repository, the autoland
tool would just need to query the Mercurial repo to see which changesets
are candidates for merging into mainline - there would be no need for
a separate database and web service!</p>
<h3>Marking a changeset as reviewed</h3>
<p>Currently, Mozilla's review procedure is very patch and Bugzilla
centric. But it doesn't have to be that way. (I argue it shouldn't be
that way.)</p>
<p>Imagine a world where code review is initiated by pushing changesets
to a special server, kind of like how Try magically turns pushes into
automation jobs.</p>
<p>In this world, reviews could be initiated by issuing a <em>pushkey</em>
or custom command to the server. This could even initiate
server-side static analysis that would hold off publishing the review
unless static analysis checks passed!</p>
<p>Granted review could be recorded by having someone issue a
<em>pushkey</em> command to mark a changeset as reviewed. The channel to the
Mercurial server is authenticated via SSH, so the user behind the
current SSH key is the reviewer. The Mercurial server could store this
username as part of the repository data. The autoland tool could then
pull down the reviewer data and only consider changesets that have an
appropriate reviewer.</p>
<p>It <em>might</em> also be possible to integrate crypto magic into this
workflow so reviewers could digitally sign a changeset as reviewed.
This could help with the verification of the Firefox source code
that Brendan Eich <a href="https://brendaneich.com/2014/01/trust-but-verify/">recently outlined</a>.</p>
<p>Like the automation data above, no separate
database would be required: all data would be part of the repository.
All you need to build is a Mercurial extension.</p>
<h3>Encouraging best practices</h3>
<p>Mozillians have written a handful of useful Mercurial extensions to
help people become more productive. We have also noticed that many
developers are still (unknowingly?) running old, slow, and buggy
Mercurial releases. We want people to have the best experience possible.
How do we do that?</p>
<p>One idea is to install an extension on the server that strongly
recommands or even requires users follow best practices (minimal HG
version, installed extensions, etc).</p>
<p>I have developed a <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=941856">proof-of-concept</a>
that does just this.</p>
<h3>Rich querying of metadata</h3>
<p>When you start putting more metadata into Mercurial (or at least write
Mercurial extensions to aggregate this metadata), all kinds of
interesting query opportunities open up. Using revsets and templates,
you can do an awful lot to use Mercurial as a database of sorts
to extract useful reports.</p>
<p>I dare say reports like
<a href="http://oduinn.com/blog/2013/12/03/infrastructure-load-for-november-2013/">John O'duinn's Monthly Infrastructure Load</a>
posts could be completely derived from Mercurial. I've
<a href="/blog/2013/11/08/using-mercurial-to-query-mozilla-metadata/">demonstrated</a>
this ability previously. That's only the tip of the iceburg.</p>
<h2>Summary</h2>
<p>We could enable a lot of new and useful scenarios by extending
Mercurial. We could accomplish this without introducing new
services and tools into our already complicated infrastructure
and workflows.</p>
<p>The possibilities I've suggested are by no means exhaustive. I encourage
others to dream up new and interesting ideas. Who knows, maybe some of
them may actually happen.</p>]]></content:encoded>
    </item>
    <item>
      <title>mach now lives in mozilla-central</title>
      <link>http://gregoryszorc.com/blog/2014/01/09/mach-now-lives-in-mozilla-central</link>
      <pubDate>Thu, 09 Jan 2014 10:55:00 PST</pubDate>
      <category><![CDATA[Mozilla]]></category>
      <category><![CDATA[mach]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2014/01/09/mach-now-lives-in-mozilla-central</guid>
      <description>mach now lives in mozilla-central</description>
      <content:encoded><![CDATA[<p><a href="https://pypi.python.org/pypi/mach/">mach</a> -- the generic command
line interface framework that is behind the <em>mach</em> tool used to
build Firefox -- now has its canonical home in
<a href="https://hg.mozilla.org/mozilla-central/file/default/python/mach/">mozilla-central</a>,
the canonical repository for Firefox. The
<a href="https://github.com/indygreg/mach">previous home</a> has been updated to
reflect the change.</p>
<p>mach will continue to be released on
<a href="https://pypi.python.org/pypi/mach/">PyPI</a> and installable via <strong>pip
install mach</strong>.</p>
<p>I made the change because keeping multiple repositories in sync
wasn't something I wanted to spend time doing. Furthermore,
Mozillians have been contributing a steady stream of improvements to
the mach core recently and it makes sense to leverage Mozilla's
familiar infrastructure for patch contribution.</p>
<p>This decision may be revisited in the future. Time will tell.</p>]]></content:encoded>
    </item>
    <item>
      <title>Why do Projects Support old Python Releases</title>
      <link>http://gregoryszorc.com/blog/2014/01/08/why-do-projects-support-old-python-releases</link>
      <pubDate>Wed, 08 Jan 2014 17:00:00 PST</pubDate>
      <category><![CDATA[Python]]></category>
      <category><![CDATA[Mozilla]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2014/01/08/why-do-projects-support-old-python-releases</guid>
      <description>Why do Projects Support old Python Releases</description>
      <content:encoded><![CDATA[<p>I see a number of open source projects supporting old versions
of Python. Mercurial supports 2.4, for example. I have to ask: why do
projects continue to support old Python releases?</p>
<p>Consider:</p>
<ul>
<li><a href="http://python.org/download/releases/2.4.6/">Python 2.4</a> was last
  released on December 19, 2008 and <strong>there will be no more releases of
  Python 2.4</strong>.</li>
<li><a href="http://python.org/download/releases/2.5.6/">Python 2.5</a> was last
  released on May 26, 2011 and <strong>there will be no more releases of
  Python 2.5</strong>.</li>
<li><a href="http://python.org/download/releases/2.6.9/">Python 2.6</a> was last
  released on October 29, 2013 and <strong>there will be no more releases of
  Python 2.6</strong>.</li>
<li><strong>Everything before Python 2.7 is end-of-lifed</strong></li>
<li>Python 2.7 continues to see periodic releases, but mostly for bug fixes.</li>
<li>Practically all of the work on CPython is happening in the 3.3 and 3.4
  branches. Other implementations continue to support 2.7.</li>
<li>Python 2.7 has been available since July 2010</li>
<li>Python 2.7 provides some very compelling language features over
  earlier releases that developers want to use</li>
<li>It's much easier to write dual compatible 2/3 Python when 2.7 is the
  only 2.x release considered.</li>
<li>Python 2.7 can be installed in userland relatively easily (see
  projects like <a href="https://github.com/yyuu/pyenv">pyenv</a>).</li>
</ul>
<p>Given these facts, I'm not sure why projects insist on supporting old
and end-of-lifed Python releases.</p>
<p><strong>I think maintainers of Python projects should seriously consider
dropping support for Python 2.6 and below.</strong> Are there really that many
people on systems that don't have Python 2.7 easily available? Why are
we Python developers inflicting so much pain on ourselves to support
antiquated Python releases?</p>
<p>As a data point, I successfully transitioned Firefox's build system
from requiring Python 2.5+ to 2.7.3+ and it was relatively
<a href="https://groups.google.com/d/msg/mozilla.dev.platform/djN02O03APc/OS8A9LuHX0sJ">pain</a>
<a href="https://bugzilla.mozilla.org/show_bug.cgi?id=870420">free</a>.
Sure, a few people complained. But as far as I know, not very many new
developers are coming in and complaining about the requirement.
If we can do it with a few thousand developers, I'm guessing your
project can as well.</p>
<p><strong>Update 2014-01-09 16:05:00 PST</strong>: This post is being discussed on
<a href="http://developers.slashdot.org/story/14/01/09/1940232/why-do-projects-continue-to-support-old-python-releases">Slashdot</a>. A lot of the comments
talk about Python 3. Python 3 is its own set of
considerations. The intended focus of this post is strictly about
dropping support for Python 2.6 and below. Python 3 is related
in that porting Python 2.x to Python 3 is much easier the higher
the Python 2.x version. This especially holds true when you want
to write Python that works simultaneously in both 2.x and 3.x.</p>]]></content:encoded>
    </item>
    <item>
      <title>On Multiple Patches in Bugs</title>
      <link>http://gregoryszorc.com/blog/2014/01/07/on-multiple-patches-in-bugs</link>
      <pubDate>Tue, 07 Jan 2014 16:40:00 PST</pubDate>
      <category><![CDATA[Mozilla]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2014/01/07/on-multiple-patches-in-bugs</guid>
      <description>On Multiple Patches in Bugs</description>
      <content:encoded><![CDATA[<p>There is a common practice at Mozilla for developing patches with
multiple parts. Nothing wrong with that. In fact, I think it's a best
practice:</p>
<ul>
<li>Smaller, self-contained patches are much easier to grok and
  review than larger patches.</li>
<li>Smaller patches can land as soon as they are reviewed. Larger patches
  tend to linger and get bit rotted.</li>
<li>Smaller patches contribute to a culture of being fast and nimble, not
  slow and lethargic. This helps with developer confidence, community
  contributions, etc.</li>
</ul>
<p>There are some downsides to multiple, smaller patches:</p>
<ul>
<li>The bigger picture is harder to understand until all parts of a
  logical patch series are shared. (This can be alleviated through
  commit messages or reviewer notes documenting future intentions.
  And of course reviewers can delay review until they are comfortable.)</li>
<li>There is more overhead to maintain the patches (rebasing, etc).
  IMO the solutions provided by Mercurial and Git are sufficient.</li>
<li>The process overhead for dealing with multiple patches and/or bugs
  can be non-trivial. (I would like to think good tooling coupled with
  continuous revisiting of policy decisions is sufficient to counteract
  this.)</li>
</ul>
<p>Anyway, the prevailing practice at Mozilla seems to be that multiple
patches related to the same logical change are attached to the same
bug. I would like to challenge the effectiveness of this practice.</p>
<p>Given:</p>
<ul>
<li>An individual commit to Firefox should be standalone and should not
  rely on future commits to unbust it (i.e. bisects to any commit should
  be safe).</li>
<li>Bugzilla has no good mechanism to isolate review comments from
  multiple attachments on the same bug, making deciphering simultaneous
  reviews on multiple attachments difficult and frustrating. This leads
  to review comments inevitably falling through the cracks and the
  quality of code suffering.</li>
<li>Reiterating the last point because it's important.</li>
</ul>
<p>I therefore argue that attaching multiple reviews to a single Bugzilla
bug is not a best practice and it should be avoided if possible. If that
means filing separate bugs for each patch, so be it. That process can
be automated. Tools like
<a href="https://hg.mozilla.org/users/tmielczarek_mozilla.com/bzexport">bzexport</a>
already do it. Alternatively (and even better IMO), we ditch
Bugzilla's code review interface (Splinter) and integrate something like
<a href="https://reviewboard.allizom.org/">ReviewBoard</a> instead. We limit
Bugzilla to tracking, high-level discussion, and metadata aggregation.
Code review happens elsewhere, without all the clutter and chaos that
Bugzilla brings to the table.</p>
<p>Thoughts?</p>]]></content:encoded>
    </item>
    <item>
      <title>Python Package Providing Clients for Mozilla Services</title>
      <link>http://gregoryszorc.com/blog/2014/01/06/python-package-providing-clients-for-mozilla-services</link>
      <pubDate>Mon, 06 Jan 2014 10:45:00 PST</pubDate>
      <category><![CDATA[Python]]></category>
      <category><![CDATA[Mozilla]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2014/01/06/python-package-providing-clients-for-mozilla-services</guid>
      <description>Python Package Providing Clients for Mozilla Services</description>
      <content:encoded><![CDATA[<p>I have a number of Python projects and tools that interact with
various Mozilla services. I had authored clients for all these services
as standalone Python modules so they could be reused across projects.</p>
<p>I have consolidated all these Python modules into a unified source
control
<a href="https://bitbucket.org/indygreg/python-mozautomation">repository</a>
and have made the project available on
<a href="https://pypi.python.org/pypi/mozautomation">PyPI</a>. You can install it
by running:</p>
<p>$ pip install mozautomation</p>
<p>Currently included in the Python package are:</p>
<ul>
<li>A client for <a href="https://treestatus.mozilla.org/">treestatus.mozilla.org</a></li>
<li>Module for extracting cookies from Firefox profiles (useful for
  programmatically getting Bugzilla auth credentials).</li>
<li>A client for reading and interpretting the
  <a href="http://builddata.pub.build.mozilla.org/buildjson/">JSON dumps of automation jobs</a></li>
<li>An interface to a SQLite database to manage associations between
  Mercurial changesets, bugs, and pushes.</li>
<li>Rudimentary parsing of commit messages to extract bugs and reviewers.</li>
<li>A client to obtain information about Firefox releases via the
  <a href="http://releases-api.mozilla.org/">releases API</a></li>
<li>A module defining common Firefox source repositories, aliases, logical
  groups (e.g. twigs and integration trees), and APIs for fetching
  pushlog data.</li>
<li>A client for the <a href="https://secure.pub.build.mozilla.org/buildapi/self-serve/">self serve API</a></li>
</ul>
<p>Documentation and testing is currently sparse. Things aren't up to my
regular high quality standard. But something is better than nothing.</p>
<p>If you are interested in contributing, drop me a line or send pull
requests my way!</p>]]></content:encoded>
    </item>
    <item>
      <title>Importance of Hosting Your Version Control Server</title>
      <link>http://gregoryszorc.com/blog/2013/11/13/importance-of-hosting-your-version-control-server</link>
      <pubDate>Wed, 13 Nov 2013 09:25:00 PST</pubDate>
      <category><![CDATA[Git]]></category>
      <category><![CDATA[Mercurial]]></category>
      <category><![CDATA[Mozilla]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2013/11/13/importance-of-hosting-your-version-control-server</guid>
      <description>Importance of Hosting Your Version Control Server</description>
      <content:encoded><![CDATA[<p>The subject of where to host version control repositories comes up a lot
at Mozilla. It takes many forms:</p>
<ul>
<li>We should move the Firefox repository to GitHub</li>
<li>I should be allowed to commit to GitHub</li>
<li>I want the canonical repository to be hosted by Bitbucket</li>
</ul>
<p>When Firefox development is concerned, Release Engineerings puts down
their foot and insists the canonical repository be hosted by Mozilla,
under a Mozilla hostname. When that's not possible, they set up a mirror
on Mozilla infrastructure.</p>
<p>I think a
<a href="https://groups.google.com/d/topic/jenkinsci-dev/-myjRIPcVwU/discussion">recent issue with the Jenkins project</a>
demonstrates why hosting your own version control server is important.
The gist is someone force pushed to a bunch of repos hosted on GitHub.
They needed to involve GitHub support to recover from the issue. While
it appears they largely recovered (and GitHub support deserves kudos - I
don't want to take away from their excellence), this problem would have
been avoided or the response time significantly decreased if the Jenkins
people had direct control over the Git server: they either could have
installed a custom hook that would have prevented the pushes or had
access to the reflog so they could have easily seen the last pushed
revision and easily forced pushed back to it. GitHub doesn't have a
mechanism for defining pre-* hooks, doesn't allow defining custom
hooks (a security and performance issue for them), and doesn't
expose the reflog data.</p>
<p>Until repository hosting services expose full repository data (such as
reflogs) and allow you to define custom hooks, accidents like these will
happen and the recovery time will be longer than if you hosted the repo
yourself.</p>
<p>It's possible repository hosting services like GitHub and Bitbucket will
expose these features or provide a means to quickly recover. If so,
kudos to them. But larger, more advanced projects will likely employ
custom hooks and considering custom hooks are a massive security and
performance issue for any hosted service provider, I'm not going to
hold my breath this particular feature is rolled out any time soon.
This is unfortunate, as it makes projects seemingly choose between
low risk/low convenience and GitHub's vibrant developer community.</p>]]></content:encoded>
    </item>
    <item>
      <title>Mercurial 2.8 released</title>
      <link>http://gregoryszorc.com/blog/2013/11/08/mercurial-2.8-released</link>
      <pubDate>Fri, 08 Nov 2013 14:30:00 PST</pubDate>
      <category><![CDATA[Mercurial]]></category>
      <category><![CDATA[Mozilla]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2013/11/08/mercurial-2.8-released</guid>
      <description>Mercurial 2.8 released</description>
      <content:encoded><![CDATA[<p><a href="http://mercurial.selenic.com/">Mercurial</a> 2.8 has been released.</p>
<p>The <a href="http://mercurial.selenic.com/wiki/WhatsNew#Mercurial_2.8_.282013-11-1.29">changes</a>
aren't as sexy as previous releases. But there are a handful of bug
fixes that seem useful to pull in. People may also find the new <em>shelve</em>
extension useful.</p>
<p>I encourage Mozillians to keep their Mercurial up to date. I once went
around the San Francisco office and stood behind people as they
upgraded to a modern Mercurial. For the next few weeks I was hearing a
lot of "OMG Mercurial is so much better now." Don't handicap yourself by
running an older, buggy Mercurial.</p>
<p>If you don't yet feel comfortable running 2.8, 2.7 should be safe.</p>]]></content:encoded>
    </item>
  </channel>
</rss>

<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0"
     xmlns:content="http://purl.org/rss/1.0/modules/content/"
     xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
     xmlns:atom="http://www.w3.org/2005/Atom"
     xmlns:dc="http://purl.org/dc/elements/1.1/"
     xmlns:wfw="http://wellformedweb.org/CommentAPI/"
     >
  <channel>
    <title>Gregory Szorc's Digital Home</title>
    <link>http://gregoryszorc.com/blog</link>
    <description>Rambling on</description>
    <pubDate>Thu, 07 May 2015 23:08:14 GMT</pubDate>
    <generator>Blogofile</generator>
    <sy:updatePeriod>hourly</sy:updatePeriod>
    <sy:updateFrequency>1</sy:updateFrequency>
    <item>
      <title>Dropping Explicit Support for Mercurial 3.0</title>
      <link>http://gregoryszorc.com/blog/2015/05/07/dropping-explicit-support-for-mercurial-3.0</link>
      <pubDate>Thu, 07 May 2015 16:05:00 PDT</pubDate>
      <category><![CDATA[Mercurial]]></category>
      <category><![CDATA[Mozilla]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2015/05/07/dropping-explicit-support-for-mercurial-3.0</guid>
      <description>Dropping Explicit Support for Mercurial 3.0</description>
      <content:encoded><![CDATA[<p>As of a few minutes ago, we explicitly dropped support for Mercurial 3.0
for all the Mercurial code in the
<a href="https://hg.mozilla.org/hgcustom/version-control-tools">version-control-tools</a>
repository. File issues in
<a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1162304">bug 1162304</a>.</p>
<p>Code may still work against Mercurial 3.0. But it isn't supported and
could break hard at any time.</p>
<p>Supporting multiple versions of <em>any</em> software carries
with it some cost. The people writing tooling around Mercurial are busy.
It is a waste of our time to bend over backwards to support old versions
of software that <strong>all</strong> users should have upgraded from months ago.
Still using older Mercurial versions means you aren't getting the best
performance and may encounter bugs that have since been fixed.</p>
<p>See the <a href="https://mozilla-version-control-tools.readthedocs.org/en/latest/hgmozilla/installing.html">Mozilla tailored Mercurial installation instructions</a>
for info on how to upgrade to the latest/greatest Mercurial version.</p>]]></content:encoded>
    </item>
    <item>
      <title>Reporting Mercurial Issues</title>
      <link>http://gregoryszorc.com/blog/2015/05/04/reporting-mercurial-issues</link>
      <pubDate>Mon, 04 May 2015 13:45:00 PDT</pubDate>
      <category><![CDATA[Mercurial]]></category>
      <category><![CDATA[Mozilla]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2015/05/04/reporting-mercurial-issues</guid>
      <description>Reporting Mercurial Issues</description>
      <content:encoded><![CDATA[<p>I semi-frequently stumble upon conversations in hallways and
on irc.mozilla.org about issues people are having with Mercurial.
These conversations periodically involve a legitimate bug with
Mercurial. Unfortunately, these conversations frequently end
without an actionable result. Unless someone files a bug, pings me,
etc, the complaints disappear into ether. That's not good for
anyone and only results in bugs living longer than they should.</p>
<p>There are posters around Mozilla offices that say
<em>if you see something, file something.</em> This advice does not just
apply to Mozilla projects!</p>
<p><strong>If you encounter an issue in Mercurial, please take the time
to report it somewhere meaningful.</strong> The
<a href="https://mozilla-version-control-tools.readthedocs.org/en/latest/hgmozilla/issues.html">Reporting Issues with Mercurial</a>
page from the
<a href="https://mozilla-version-control-tools.readthedocs.org/en/latest/hgmozilla/index.html">Mercurial for Mozillians</a>
guide tells you how to do this.</p>
<p>It is OK to complain about something. But if you don't inform
someone empowered to do something about it, you are part of the
problem without being part of the solution. Please make the
incremental effort to be part of the solution.</p>]]></content:encoded>
    </item>
    <item>
      <title>Mercurial 3.4 Released</title>
      <link>http://gregoryszorc.com/blog/2015/05/04/mercurial-3.4-released</link>
      <pubDate>Mon, 04 May 2015 12:40:00 PDT</pubDate>
      <category><![CDATA[Mercurial]]></category>
      <category><![CDATA[Mozilla]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2015/05/04/mercurial-3.4-released</guid>
      <description>Mercurial 3.4 Released</description>
      <content:encoded><![CDATA[<p>Mercurial 3.4 was released on May 1 (following Mercurial's time-based
schedule of releasing a new version every 3 months).</p>
<p>3.4 is a significant release for a few reasons.</p>
<p>First, the next version of the wire protocol (<em>bundle2</em>) has been
marked as non-experimental on servers. This version of the protocol
paves over a number of deficiencies in the classic protocol. I won't
go into low-level details. But I will say that the protocol enables
some rich end-user experiences, such as having the server hand out URLs
for pre-generated bundles (e.g. offload clones to S3), atomic push
operations, and advanced workflows, such as having the server rebase
automatically on push. Of course, you'll need a server running 3.4
to realize the benefits of the new protocol. hg.mozilla.org won't be
updated until at least June 1.</p>
<p>Second, Mercurial 3.4 contains improvements to the tags cache to make
performance concerns a thing of the past. Due to the structure of the
Firefox repositories, the previous implementation of the tags cache
could result in pauses of dozens of seconds during certain workflows.
The problem should go away with Mercurial 3.4. <strong>Please note that on
first use of Mercurial 3.4, your repository may perform a one-time
upgrade of the tags cache. This will spin a full CPU core and will
take up to a few minutes to complete on Firefox repos. Let it run to
completion and performance should not be an issue again.</strong> I wrote the
patches to change the tags cache (with lots of help from Pierre-Yves
David, a Mercurial core contributor). So if you find anything wrong,
I'm the one to complain to.</p>
<p>Third, the HTTP interface to Mercurial (hgweb) now has JSON output
for nearly every endpoint. The implementation isn't yet complete,
but it is better than nothing. But, it should be good enough for
services to start consuming it. Again, this won't be available on
hg.mozilla.org until the server is upgraded on June 1 at the earliest.
This is a feature I added to core Mercurial. If you have feature
requests, send them my way.</p>
<p>Fourth, a number of performance regressions introduced in Mercurial
3.3 were addressed. These performance issues frequently manifested
during <em>hg blame</em> operations. Many Mozillians noticed them on
hg.mozilla.org when looking at blame through the web interface.</p>
<p>For a more comprehensive list of changes, see
<a href="https://groups.google.com/d/msg/mozilla.dev.version-control/z4aWvBoAGYw/d0hUGKJU_psJ">my post about the 3.4 RC</a>
and the
<a href="http://mercurial.selenic.com/wiki/WhatsNew#Mercurial_3.4_.282015-05-01.29">official release notes</a>.</p>
<p>3.4 was a significant release. There are compelling reasons to upgrade.
That being said, there were a lot of changes in 3.4. If you want to wait
until 3.4.1 is released (scheduled for June 1) so you don't run into
any regressions, nobody can fault you for that.</p>
<p>If you want to upgrade, I recommend reading the
<a href="https://mozilla-version-control-tools.readthedocs.org/en/latest/hgmozilla/installing.html">Mercurial for Mozillians Installation Page</a>.</p>]]></content:encoded>
    </item>
    <item>
      <title>Automatically Redirecting Mercurial Pushes</title>
      <link>http://gregoryszorc.com/blog/2015/04/30/automatically-redirecting-mercurial-pushes</link>
      <pubDate>Thu, 30 Apr 2015 12:30:00 PDT</pubDate>
      <category><![CDATA[Mercurial]]></category>
      <category><![CDATA[Mozilla]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2015/04/30/automatically-redirecting-mercurial-pushes</guid>
      <description>Automatically Redirecting Mercurial Pushes</description>
      <content:encoded><![CDATA[<p>Managing URLs in distributed version control tools can be a pain,
especially if multiple repositories are involved. For example,
with Mozilla's repository-based code review workflow (you push to a
special review repository to initiate code review - this is conceptually
similar to GitHub pull requests), there exist
<a href="https://reviewboard-hg.mozilla.org/">separate code review repositories</a>
for each logical repository. Figuring out how repositories map to each
other and setting up remote paths for each new clone can be a pain and
time sink.</p>
<p>As of today, we can now do something better.</p>
<p>If you push to <em>ssh://reviewboard-hg.mozilla.org/autoreview</em>,
Mercurial will automatically figure out the appropriate review
repository and redirect your push automatically. In other words, if we
have <a href="https://reviewboard.mozilla.org/">MozReview</a> set up to review
whatever repository you are working on,
your push and review request will automatically go through. No need to
figure out what the appropriate review repo is or configure
repository URLs!</p>
<p>Here's what it looks like:</p>
<div class="pygments_murphy"><pre>$ hg push review
pushing to ssh://reviewboard-hg.mozilla.org/autoreview
searching for appropriate review repository
redirecting push to ssh://reviewboard-hg.mozilla.org/version-control-tools/
searching for changes
remote: adding changesets
remote: adding manifests
remote: adding file changes
remote: added 1 changesets with 1 changes to 1 files
remote: Trying to insert into pushlog.
remote: Inserted into the pushlog db successfully.
submitting 1 changesets for review

changeset:  11043:b65b087a81be
summary:    mozreview: create per-commit identifiers (bug 1160266)
review:     https://reviewboard.mozilla.org/r/7953 (draft)

review id:  bz://1160266/gps
review url: https://reviewboard.mozilla.org/r/7951 (draft)
(visit review url to publish this review request so others can see it)
</pre></div>

<p>Read the <a href="https://mozilla-version-control-tools.readthedocs.org/en/latest/mozreview/install.html#review-repositories">full instructions</a>
for more details.</p>
<p>This requires an updated <a href="https://hg.mozilla.org/hgcustom/version-control-tools/">version-control-tools</a>
repository, which you can get by running <em>mach mercurial-setup</em> from a
Firefox repository.</p>
<p>For those that are curious, the <em>autoreview</em> repo/server advertises a list
of repository URLs and their root commit SHA-1. The client automatically
sends the push to a URL sharing the same root commit. The
<a href="https://hg.mozilla.org/hgcustom/version-control-tools/rev/0b02cd388590">code</a>
is quite simple.</p>
<p>While this is only implemented for MozReview, I could envision us doing
something similar for other centralized repository-centric services, such
as <em>Try</em> and <em>Autoland</em>. Stay tuned.</p>]]></content:encoded>
    </item>
    <item>
      <title>My Current Thoughts on System Administration</title>
      <link>http://gregoryszorc.com/blog/2015/04/17/my-current-thoughts-on-system-administration</link>
      <pubDate>Fri, 17 Apr 2015 15:35:00 PDT</pubDate>
      <category><![CDATA[sysadmin]]></category>
      <category><![CDATA[Mozilla]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2015/04/17/my-current-thoughts-on-system-administration</guid>
      <description>My Current Thoughts on System Administration</description>
      <content:encoded><![CDATA[<p>I attended PyCon last week. It's a great conference. You should attend.
While I should write up a detailed trip report, I wanted to quickly
share one of my takeaways.</p>
<p><a href="http://www.ansible.com/home">Ansible</a> was talked about a lot at PyCon.
Sitting through a few presentations and talking with others helped me
articulate why I've been drawn to Ansible (over say Puppet, Chef, Salt,
etc) lately.</p>
<p>First, Ansible doesn't require a central server. Administration is done
remotely Ansible establishes a SSH connection to a remote machine and
does stuff. Having Ruby, Python, support libraries, etc installed on
production systems just for system administration never really jived
with me. I love Ansible's default hands off approach. (Yes, you can use
a central server for Ansible, but that's not the default behavior. While
tools like Puppet could be used without a central server, it felt like
they were optimized for central server use and thus local mode felt
awkward.)</p>
<p>Related to central servers, I never liked how that model consists of
clients periodically polling for and applying updates. I like the idea
of immutable server images and periodic updates work against this goal.
The central model also has a major bazooka pointed at you: at any time, you
are only one mistake away from completely hosing every machine doing
continuous polling. e.g. if you accidentally update firewall configs and
lock out central server and SSH connectivity, every machine will pick up
these changes during periodic polling and by the time anyone realizes
what's happened, your machines are all effectively bricked. (Yes, I've
seen this happen.) I like having humans control exactly when my systems
apply changes, thank you. I concede periodic updates and central control
have some benefits.</p>
<p>Choosing not to use a central server by default means that hosts are
modeled as a set of applied Ansible playbooks, not necessarily as a host
with a set of Ansible playbooks attached. Although, Ansible does support
both models. I can easily apply a playbook to a host in a one-off
manner. This means I can have playbooks represent common, one-off tasks
and I can easily run these tasks without having to muck around with the
host to playbook configuration. More on this later.</p>
<p>I love the simplicity of Ansible's configuration. It is just YAML files.
Not some Ruby-inspired DSL that takes hours to learn. With Ansible, I'm
learning what modules are available and how they work, not complicated
syntax. Yes, there is complexity in Ansible's configuration. But at
least I'm not trying to figure out the file syntax as part of learning
it.</p>
<p>Along that vein, I appreciate the readability of Ansible playbooks.
They are simple, linear lists of tasks. Conceptually, I love the promise
of full dependency graphs and concurrent execution. But I've spent hours
debugging race conditions and cyclic dependencies in Puppet that I'm left
unconvinced the complexity and power is worth it. I do wish Ansible
could run faster by running things concurrently. But I think they made
the right decision by following KISS.</p>
<p>I enjoy how Ansible playbooks are effectively high-level scripts. If I
have a shell script or block of code, I can usually port it to Ansible
pretty easily. One pass to do the conversion 1:1. Another pass to
Ansibilize it. Simple.</p>
<p>I love how Ansible playbooks can be checked in to source control and
live next to the code and applications they manage. I frequently see
people maintain separate source control repositories for configuration
management from the code it is managing. This always bothered me. When I
write a service, I want the code for deploying and managing that service
to live next to it in version control. That way, I get the configuration
management and the code versioned in the same timeline. If I check out a
release from 2 years ago, I should still be able to use its exact
configuration management code. This becomes difficult to impossible when
your organization is maintaining configuration management code in a
separate repository where a central server is required to do
deployments (see Puppet).</p>
<p>Before PyCon, I was having an internal monolog about adopting the policy
that all changes to remote servers be implemented with Ansible
playbooks. I'm pleased to report that a fellow contributor to the
Mercurial project has adopted this workflow himself and he only has
great things to say! So, starting today, I'm going to try to enforce
that every change I make to a remote server is performed via Ansible and
that the Ansible playbooks are checked into version control. The Ansible
playbooks will become implicit documentation of every process involved
with maintaining a server.</p>
<p>I've already applied this principle to deploying MozReview. Before,
there was some internal Mozilla wiki documenting commands to execute in
a terminal to deploy MozReview. I have replaced that documentation with
a one-liner that invokes Ansible. And, the Ansible files are now
<a href="https://hg.mozilla.org/hgcustom/version-control-tools/file/1e85a23c4175/ansible">in a public repository</a>.</p>
<p>If you poke around that repository, you'll see that I have Ansible
playbooks referencing Docker. I have Ansible provisioning Docker images
used by the test and development environment. That same Ansible code
is used to configure our production systems (or is at least in the
process of being used in that way). Having dev, test, and prod using the
same configuration management has been a pipe dream of mine and I
finally achieved it! I attempted this before with Puppet but was unable
to make it work just right. The flexibility that Ansible's design
decisions have enabled has made this finally possible.</p>
<p>Ansible is my go to system management tool right now. And I still feel
like I have a lot to learn about its hidden powers.</p>
<p>If you are still using Puppet, Chef, or other tools invented in previous
generations, I urge you to check out Ansible. I think you'll be
pleasantly surprised.</p>]]></content:encoded>
    </item>
    <item>
      <title>Notes from Facebook's Developer Infrastructure at Scale F8 Talk</title>
      <link>http://gregoryszorc.com/blog/2015/03/28/notes-from-facebook's-developer-infrastructure-at-scale-f8-talk</link>
      <pubDate>Sat, 28 Mar 2015 11:45:00 PDT</pubDate>
      <category><![CDATA[Mozilla]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2015/03/28/notes-from-facebook's-developer-infrastructure-at-scale-f8-talk</guid>
      <description>Notes from Facebook's Developer Infrastructure at Scale F8 Talk</description>
      <content:encoded><![CDATA[<p>Any time Facebook talks about technical matters I tend to listen.
They have a track record of demonstrating engineering leadership
in several spaces. And, unlike many companies that just talk, Facebook
often gives others access to those ideas via source code and healthy
open source projects. It's rare to see a company operating on the
frontier of the computing field provide so much insight into their
inner workings. You can gain so much by riding their cotails and
following their lead instead of clinging to and cargo culting from
the past.</p>
<p>The Facebook F8 developer conference was this past week. All the
talks are <a href="https://developers.facebooklive.com/">now available online</a>.
<strong>I encourage you to glimpse through the list of talks and watch
whatever is relevant to you.</strong> There's really a little bit for
everyone.</p>
<p>Of particular interest to me is the
<a href="https://developers.facebooklive.com/videos/561/big-code-developer-infrastructure-at-facebook-s-scale">Big Code: Developer Infrastructure at Facebook's Scale</a>
talk. This is highly relevant to my job role as Developer Productivity
Engineer at Mozilla.</p>
<p>My notes for this talk follow.</p>
<p><strong>"We don't want humans waiting on computers. We want computers waiting
on humans."</strong> (This is the common theme of the talk.)</p>
<p>In 2005, Facebook was on Subversion. In 2007 moved to Git. Deployed
a bridge so people worked in Git and had distributed workflow but
pushed to Subversion under the hood.</p>
<p>New platforms over time. Server code, iOS, Android. One Git repo
per platform/project -&gt; 3 Git repos. Initially no code sharing, so
no problem. Over time, code sharing between all repos. Lots of code
copying and confusion as to what is where and who owns what.</p>
<p>Facebook is mere weeks away from completing their migration to
consolidate the big three repos to a Mercurial monorepo. (See also
<a href="/blog/2014/09/09/on-monolithic-repositories/">my post about monorepos</a>.)</p>
<p>Reasons:</p>
<ol>
<li>Easier code sharing.</li>
<li>Easier large-scale changes. Rewrite the universe at once.</li>
<li>Unified set of tooling.</li>
</ol>
<p>Facebook employees run &gt;1M source control commands per day. &gt;100k
commits per week. VCS tool needs to be fast to prevent distractions
and context switching, which slow people down.</p>
<p>Facebook implemented sparse checkout and shallow history in Mercurial.
Necessary to scale distributed version control to large repos.</p>
<p><strong>Quote from Google: "We're excited about the work Facebook is doing with
Mercurial and glad to be collaborating with Facebook on Mercurial
development."</strong> (Well, I guess the cat is finally out of the bag:
Google is working on Mercurial. This was kind of an open secret for
months. But I guess now it is official.)</p>
<p>Push-pull-rebase bottleneck: if you rebase and push and someone beats
you to it, you have to pull, rebase, and try again. This gets worse
as commit rate increases and people do needless legwork. <strong>Facebook
has moved to server-side rebasing on push</strong> to mostly eliminate this
pain point. (This is part of a still-experimental feature in Mercurial,
which should hopefully lose its experimental flag soon.)</p>
<p>Starting 13:00 in we have a speaker change and move away from version
control.</p>
<p>IDEs don't scale to Facebook scale. <strong>"Developing in Xcode at Facebook
is an exercise in frustration."</strong> On average 3.5 minutes to open
Facebook for iOS in Xcode. 5 minutes on average to index. Pegs CPU
and makes not very responsive. 50 Xcode crashes per day across all
Facebook iOS developers.</p>
<p><strong>Facebook measures everything about tools. Mercurial operation times.
Xcode times. Build times. Data tells them what tools and workflows
need to be worked on.</strong></p>
<p>Facebook believes IDEs are worth the pain because they make people
more productive.</p>
<p>Facebook wants to support all editors and IDEs since people want to
use whatever is most comfortable.</p>
<p>React Native changed things. Supported developing on multiple
platforms, which no single IDE supports. People launched several
editors and tools to do React Native development. People needed 4
windows to do development. That experience was "not acceptable."
So they built their own IDE. Set of plugins on top of ATOM. Not
a fork. They like hackable and web-y nature of ATOM.</p>
<p>The demo showing iOS development looks very nice! Doing Objective-C,
JavaScript, simulator integration, and version control in one window!</p>
<p>It can connect to remote servers and transparently save and
deploy changes. It can also get real-time compilation errors and hints
from the remote server! (Demo was with Hack. Not sure if others langs
supported. Having beefy central servers for e.g. Gecko development
would be a fun experiment.)</p>
<p>Starting at 32:00 presentation shifts to continuous integration.</p>
<p>Number one goal of CI at Facebook is developer efficiency. <strong>We
don't want developers waiting on computers to build and test diffs.</strong></p>
<p>3 goals for CI:</p>
<ol>
<li>High-signal feedback. Don't want developers chasing failures that
   aren't their fault. Wastes time.</li>
<li>Must provide rapid feedback. Developers don't want to wait.</li>
<li>Provide frequent feedback. Developers should know as soon as
   possible after they did something. (I think this refers to local
   feedback.)</li>
</ol>
<p>Sandcastle is their CI system.</p>
<p>Diff lifecycle discussion.</p>
<p>Basic tests and lint run locally. (My understanding from talking
with Facebookers is "local" often means on a Facebook server, not
local laptop. Machines at developers fingertips are often dumb
terminals.)</p>
<p>They appear to use code coverage to determine what tests to run.
"We're not going to run a test unless your diff might actually have
broken it."</p>
<p>They run flaky tests less often.</p>
<p>They run slow tests less often.</p>
<p><strong>Goal is to get feedback to developers in under 10 minutes.</strong></p>
<p><strong>If they run fewer tests and get back to developers quicker,
things are less likely to break than if they run more tests but
take longer to give feedback.</strong></p>
<p>They also want feedback quickly so reviewers can see results at
review time.</p>
<p>They use Web Driver heavily. Love cross-platform nature of Web Driver.</p>
<p>In addition to test results, performance and size metrics are reported.</p>
<p>They have a "Ship It" button on the diff.</p>
<p>Landcastle handles landing diff.</p>
<p>"It is not OK at Facebook to land a diff without using Landcastle."
(Read: developers don't push directly to the master repo.)</p>
<p>Once Landcastle lands something, it runs tests again. If an issue
is found, a task is filed. Task can be "push blocking."
Code won't ship to users until the "push blocking" issue resolved.
(Tweets confirm they do backouts "fairly aggressively." A valid
resolution to a push blocking task is to backout. But fixing forward
is fine as well.)</p>
<p>After a while, branch cut occurs. Some cherry picks onto release
branches.</p>
<p>In addition to diff-based testing, they do continuous testing runs.
Much more comprehensive. No time restrictions. Continuous runs on
master and release candidate branches. Auto bisect to pin down
regressions.</p>
<p>Sandcastle processes &gt;1000 test results per second. 5 years of machine
work per day. Thousands of machines in 5 data centers.</p>
<p>They started with buildbot. Single master. Hit scaling limits of
single thread single master. Master could not push work to workers
fast enough. Sandcastle has distributed queue. Workers just pull
jobs from distributed queue.</p>
<p>"High-signal feedback is critical." "Flaky failures erode developer
confidence." "We need developers to trust Sandcastle."</p>
<p>Extremely careful separating infra failures from other failures.
Developers don't see infra failures. Infra failures only reported
to Sandcastle team.</p>
<p>Bots look for flaky tests. Stress test individual tests. Run tests
in parallel with themselves. Goal: developers don't see flaky tests.</p>
<p>There is a "not my fault" button that developers can use to report
bad signals.</p>
<p><strong>"Whatever the scale of your engineering organization, developer
efficiency is the key thing that your infrastructure teams should be
striving for. This is why at Facebook we have some of our top
engineers working on developer infrastructure."</strong> (Preach it.)</p>
<p>Excellent talk. <strong>Mozillians doing infra work or who are in charge
of head count for infra work should watch this video.</strong></p>
<p><em>Update 2015-03-28 21:35 UTC - Clarified some bits in response to
new info Tweeted at me. Added link to my monorepos blog post.</em></p>]]></content:encoded>
    </item>
    <item>
      <title>New High Scores for hg.mozilla.org</title>
      <link>http://gregoryszorc.com/blog/2015/03/19/new-high-scores-for-hg.mozilla.org</link>
      <pubDate>Thu, 19 Mar 2015 20:20:00 PDT</pubDate>
      <category><![CDATA[Mercurial]]></category>
      <category><![CDATA[Mozilla]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2015/03/19/new-high-scores-for-hg.mozilla.org</guid>
      <description>New High Scores for hg.mozilla.org</description>
      <content:encoded><![CDATA[<p>It's been a <a href="/blog/2015/03/18/network-events/">rough week</a>.</p>
<p>The very short summary of events this week is that both the Firefox
and Firefox OS release automation has been performing a denial of
service attack against
<a href="https://hg.mozilla.org/">hg.mozilla.org</a>.</p>
<p>On the face of it, this is nothing new. The release automation
is by far the top consumer of hg.mozilla.org data, requesting several
terabytes per day via several million HTTP requests from thousands of
machines in multiple data centers. The very nature of their existence
makes them a significant denial of service threat.</p>
<p>Lots of things went wrong this week. While a post mortem will shed
light on them, many fall under the umbrella of <em>release automation was
making more requests than it should have and was doing so in a way
that both increased the chances of an outage occurring and increased
the chances of a prolonged outage.</em> This resulted in the hg.mozilla.org
servers working harder than they ever have. As a result, we have some
new <em>high scores</em> to share.</p>
<ul>
<li>
<p>On UTC day March 19, hg.mozilla.org transferred 7.4 TB of data.
  This is a significant increase from the ~4 TB we expect on a typical
  weekday. (Even more significant when you consider that most load is
  generated during peak hours.)</p>
</li>
<li>
<p>During the 1300 UTC hour of March 17, the cluster received 1,363,628
  HTTP requests. No HTTP 503 Service Not Available errors were
  encountered in that window! 300,000 to 400,000 requests per hour is
  typical.</p>
</li>
<li>
<p>During the 0800 UTC hour of March 19, the cluster transferred 776 GB
  of repository data. That comes out to at least 1.725 Gbps on average
  (I didn't calculate TCP and other overhead). Anything greater than 250
  GB per hour is not very common. No HTTP 503 errors were served from
  the origin servers during this hour!</p>
</li>
</ul>
<p>We encountered many periods where hg.mozilla.org was operating more than
twice its normal and expected operating capacity and it was able to
handle the load just fine. As a server operator, I'm proud of this.
The servers were provisioned beyond what is normally needed of them and
it took a truly exceptional event (or two) to bring the service down.
This is generally a good way to do hosted services (you rarely want to be
barely provisioned because you fall over at the slighest change and you
don't want to be grossly over-provisioned because you are wasting money
on idle resources).</p>
<p>Unfortunately, the hg.mozilla.org service did fall over. Multiple times,
in fact. There is room to improve. As proud as I am that the service
operated well beyond its expected limits, I can't help but feel ashamed
that it did eventual cave in under even extreme load and that people are
probably making under-informed general assumptions like <em>Mercurial can't
scale</em>. The simple fact of the matter is that clients cumulatively
generated an exceptional amount of traffic to hg.mozilla.org this week.
All servers have capacity limits. And this week we encountered the limit
for the current configuration of hg.mozilla.org. Cause and effect.</p>]]></content:encoded>
    </item>
    <item>
      <title>Network Events</title>
      <link>http://gregoryszorc.com/blog/2015/03/18/network-events</link>
      <pubDate>Wed, 18 Mar 2015 14:25:00 PDT</pubDate>
      <category><![CDATA[Mozilla]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2015/03/18/network-events</guid>
      <description>Network Events</description>
      <content:encoded><![CDATA[<p>The Firefox source repositories and automation have been closed the past
few days due to a couple of outages.</p>
<p>Yesterday, aggregate CPU usage on many of the machines in the hg.mozilla.org
cluster hit 100%. Previously, whenever hg.mozilla.org was under high load,
we'd run out of network bandwidth before we ran out of CPU on the machines.
In other words, Mercurial was generating data faster than the network could
accept it.</p>
<p>When this happened, the service started issuing HTTP 503 Service Not
Available responses. This is the universal server signal for <em>I'm down, go
away</em>. Unfortunately, not all clients did this.</p>
<p>Parts of Firefox's release automation retried failing requests
immediately, or with insufficient jitter in their backoff interval.
Actively retrying requests against a server that's experiencing load
issues only makes the problem worse. This effectively prolonged the
outage.</p>
<p>Today, we had a similar but different network issue. The load balancer
fronting hg.mozilla.org can only handle so much bandwidth. Today, we hit
that limit. The load balancer started throttling connections. Load on
hg.mozilla.org skyrocketed and request latency increased. From the
perspective of clients, the service grinded to a halt.</p>
<p>hg.mozilla.org was partially sharing a load balancer with
ftp.mozilla.org. That meant if one of the services experienced very high
load, the other service could effectively be <em>locked out</em> of bandwidth.
We saw this happening this morning. ftp.mozilla.org load was high (it
looks like downloads of Firefox Developer Edition are a major
contributor - these don't go through the CDN for reasons unknown to me)
and there wasn't enough bandwidth to go around.</p>
<p>Separately today, hg.mozilla.org again hit 100% CPU. At that time, it
also set a new record for network throughput: ~3 Gbps. It normally
consumes between 200 and 500 Mbps, with periodic spikes to 750 Mbps.
(Yesterday's event saw a spike to around ~2 Gbps.)</p>
<p>Going back through the hg.mozilla.org server logs, an offender is
quite obvious. Before March 9, total outbound transfer for the
<a href="https://hg.mozilla.org/build/tools/">build/tools</a> repo was around
1 tebibyte per day. Starting on March 9, it increased to 3 tebibytes per
day! This is quite remarkable, as a clone of this repo is only about
20 MiB. This means the repo was getting cloned about 150,000 times per
day! (Note: I think all these numbers may be low by ~20% - stay tuned
for the final analysis.)</p>
<p>2 TiB/day is statistically significant because we transfer less than 10
TiB/day across all of hg.mozilla.org. And, 1 TiB/day is close to 100
Mbps, assuming requests are evenly spread out (which of course they
aren't).</p>
<p>Multiple things went wrong. If only one or two happened, we'd likely be
fine. Maybe there would have been a short blip. But not the major event
we've been firefighting the last ~24 hours.</p>
<p>This post is only a summary of what went wrong. I'm sure there will be a
post-mortem and that it will contain lots of details for those who want
to know more.</p>]]></content:encoded>
    </item>
    <item>
      <title>Lost Productivity Due to Non-Unified Repositories</title>
      <link>http://gregoryszorc.com/blog/2015/02/17/lost-productivity-due-to-non-unified-repositories</link>
      <pubDate>Tue, 17 Feb 2015 14:50:00 PST</pubDate>
      <category><![CDATA[Mozilla]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2015/02/17/lost-productivity-due-to-non-unified-repositories</guid>
      <description>Lost Productivity Due to Non-Unified Repositories</description>
      <content:encoded><![CDATA[<p>I'm currently working on
<a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1132771">annotating moz.build files with metadata</a>
that defines things like which bug component and code reviewers map to
which files. It's going to enable a lot of awesomeness.</p>
<p>As part of this project, I'm implementing a new moz.build processing
mode. Instead of reading moz.build files by traversing <em>DIRS</em> variables
from previously-executed moz.build files, we're evaluating moz.build
files according to filesystem topology. This has uncovered a few
cases where a moz.build file errors because of assumptions that no
longer hold. For example, for directories that are only active on
Windows, the moz.build file might assume that <em>if Windows</em> is always true.</p>
<p>One such problem was with
<a href="https://dxr.mozilla.org/mozilla-central/source/gfx/angle/src/libGLESv2/moz.build">gfx/angle/srx/libGLESv2/moz.build</a>.
This file contained code similar to the following:</p>
<pre><code>if CONFIG['IS_WINDOWS']:
    SOURCES += ['foo.cpp']
...
SOURCES['foo.cpp'].flags += ['-DBAR']
</code></pre>
<p>This always ran without issue because this moz.build was only included
if building for Windows. This assumption is of course invalid when in
filesystem traversal mode.</p>
<p>Anyway, as part of updating this trouble file, I lost maybe an hour
of productivity. Here's how.</p>
<p>The top of the trouble moz.build file has a comment:</p>
<pre><code># Please note this file is autogenerated from generate_mozbuild.py,
# so do not modify it directly
</code></pre>
<p>OK. So, I need to modify <em>generate_mozbuild.py</em>. First thing's first: I
need to locate it:</p>
<pre><code>$ hg locate generate_mozbuild.py
gfx/skia/generate_mozbuild.py
</code></pre>
<p>So I load up this file. I see a <em>main()</em>. I run the script in my shell
and get an error. Weird. I look around <em>gfx/skia</em> and see a README_MOZILLA
file. I open it. README_MOZILLA contains some instructions. They aren't
very good. I hop in #gfx on IRC and ask around. They tell me to do a
Subversion clone of Skia and to check out the commit referenced in
README_MOZILLA. There is no repo URL in README_MOZILLA. I search Google.
I find a Git URL. I notice that README_MOZILLA contains a SHA-1 commit,
not a Subversion integer revision. I figure the Git repo is what was
meant. I clone the Git repo. I attempt to run the generation script
referenced by README_MOZILLA. It fails. I ask again in #gfx. They are
baffled at first. I dig around the source code. I see a reference in
Skia's upstream code to a path that doesn't exist. I tell the #gfx
people. They tell me sub-repos are likly involved and to use <em>gclient</em>
to clone the repo. I search for the proper Skia source code docs and
type the necessary gclient commands. (Fortunately I've used gclient
before, so this wasn't completely alien to me.)</p>
<p>I get the Skia clone in the proper state. I run the generation script
and all works. But I don't see it writing the trouble moz.build file I
set out to fix. I set some breakpoints. I run the code again. I'm
baffled.</p>
<p>Suddenly it hits me: I've been poking around with <em>gfx/skia</em> which is
separate from <em>gfx/angle</em>! I look around <em>gfx/angle</em> and see
a README.mozilla file. I open it. It reveals the existence of the Git
repo <a href="https://github.com/mozilla/angle">https://github.com/mozilla/angle</a>.
I open GitHub in my browser. I see a <em>generate_mozbuild.py</em> script.</p>
<p>I now realize there are multiple files named <em>generate_mozbuild.py</em>.
Unfortunately, the one I care about - the ANGLE one - is not checked
into mozilla-central. So, my search for it with <em>hg files</em> did not
reveal its existence. Between me trying to get the Skia code cloned and
generating moz.build files, I probably lost an hour of work. All because
a file with a similar name wasn't checked into mozilla-central!</p>
<p>I assumed that the single <em>generate_mozbuild.py</em> I found under source
control was the only file of that name and that it <em>must</em> be the file
I was interested in.</p>
<p>Maybe I should have known to look at <em>gfx/angle/README.mozilla</em> first.
Maybe I should have known that <em>gfx/angle</em> and <em>gfx/skia</em> are completely
independent.</p>
<p>But I didn't. My ignorance cost me.</p>
<p>Had the contents of the separate ANGLE repository been checked into
mozilla-central, I would have seen the multiple <em>generate_mozbuild.py</em>
files and I would likely have found the correct one immediately. But
they weren't and I lost an hour of my time.</p>
<p>And I'm not done. Now I have to figure out how the separate ANGLE repo
integrates with mozilla-central. I'll have to figure out how to submit the
patch I still need to write. The GitHub description of this repo says
<em>Talk to vlad, jgilbert, or kamidphish for more info</em>. So now I have to
bother them before I can submit my patch. Maybe I'll just submit a pull
request and see what happens.</p>
<p>I'm convinced I wouldn't have encountered this problem if a
<a href="/blog/2014/09/09/on-monolithic-repositories/">monolithic repository</a>
were used. I would have found the separate <em>generate_mozbuild.py</em> file
immediately. And, the change process would likely have been known to me
since all the code was in a repository I already knew how to submit
patches from.</p>
<p>Separate repos are just lots of pain. You can bet I'll link to this post
when people propose splitting up mozilla-central into multiple
repositories.</p>]]></content:encoded>
    </item>
    <item>
      <title>Branch Cleanup in Firefox Repositories</title>
      <link>http://gregoryszorc.com/blog/2015/01/28/branch-cleanup-in-firefox-repositories</link>
      <pubDate>Wed, 28 Jan 2015 20:35:00 PST</pubDate>
      <category><![CDATA[Mercurial]]></category>
      <category><![CDATA[Mozilla]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2015/01/28/branch-cleanup-in-firefox-repositories</guid>
      <description>Branch Cleanup in Firefox Repositories</description>
      <content:encoded><![CDATA[<p>Mozilla has historically done some funky things with the Firefox
Mercurial repositories. One of the things we've done is create
a bunch of named branches to track the Firefox release process.
These are branch names like <em>GECKO20b12_2011022218_RELBRANCH</em>.</p>
<p>Over in
<a href="https://bugzilla.mozilla.org/show_bug.cgi?id=927219">bug 927219</a>,
we started the process of cleaning up some cruft left over from
many of these old branches.</p>
<p>For starters, the old named branches in the Firefox repositories
are being actively closed. When you <em>hg commit --close-branch</em>,
Mercurial creates a special commit that says <em>this branch is closed</em>.
Branches that are closed are automatically hidden from the output
of <em>hg branches</em> and <em>hg heads</em>. As a result, the output of these
commands is now much more usable.</p>
<p>Closed branches still constitute <em>heads</em> on the DAG. And several heads
lead to degraded performance in some situations (notably push and pull
times - the same thing happens in Git). I'd like to eventually merge
these old heads so that repositories only have 1 or a small number of
DAG heads. However, extra care must be taken before that step. Stay
tuned.</p>
<p>Anyway, for the average person reading, you probably won't be impacted
by these changes at all. The greatest impact will be from the person who
lands the first change on top of any repository whose last commit
was a branch close. If you commit on top of the <em>tip</em> commit,
you'll be committing on top of a previously closed branch! You'll
instead want to <em>hg up default</em> after you pull to ensure you are on
the proper DAG head! And even then, if you have local commits, you may
not be based on top of the appropriate commit! A simple run of
<em>hg log --graph</em> should help you decipther the state of the world.
(Please note that the usability problems around discovering the
appropriate head to land on are a result of our poor branching strategy
for the Firefox repositories. We probably should have named branches
tracking the active Gecko releases. But that ship sailed years ago
and fixing that is pretty far down the priority list. Wallpapering
over things with the
<a href="https://mozilla-version-control-tools.readthedocs.org/en/latest/hgmozilla/firefoxtree.html">firefoxtree extensions</a>
is my recommended solution until matters are fixed.)</p>]]></content:encoded>
    </item>
  </channel>
</rss>

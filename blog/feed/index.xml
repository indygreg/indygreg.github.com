<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0"
     xmlns:content="http://purl.org/rss/1.0/modules/content/"
     xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
     xmlns:atom="http://www.w3.org/2005/Atom"
     xmlns:dc="http://purl.org/dc/elements/1.1/"
     xmlns:wfw="http://wellformedweb.org/CommentAPI/"
     >
  <channel>
    <title>Gregory Szorc'c Digital Home</title>
    <link>http://gregoryszorc.com/blog</link>
    <description>Rambling on</description>
    <pubDate>Fri, 01 Mar 2013 04:09:38 GMT</pubDate>
    <generator>Blogofile</generator>
    <sy:updatePeriod>hourly</sy:updatePeriod>
    <sy:updateFrequency>1</sy:updateFrequency>
    <item>
      <title>moz.build Files and the Firefox Build System</title>
      <link>http://gregoryszorc.com/blog/2013/02/28/moz.build-files-and-the-firefox-build-system</link>
      <pubDate>Thu, 28 Feb 2013 19:45:00 PST</pubDate>
      <category><![CDATA[Mozilla]]></category>
      <category><![CDATA[Firefox]]></category>
      <category><![CDATA[build system]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2013/02/28/moz.build-files-and-the-firefox-build-system</guid>
      <description>moz.build Files and the Firefox Build System</description>
      <content:encoded><![CDATA[<p>The next time you update mozilla-central you may notice some significant
changes with the build system. That's because this morning we finally
landed the start of a massive overhaul of the build system! There are
many end goals to this effort. The important ones for most will be
faster build times and a build system that is friendlier to make changes
to.</p>
<h2>Introducing moz.build Files</h2>
<p>If you look in the tree, you'll notice that nearly every directory
now has a <em>moz.build</em> file.</p>
<p><em>moz.build</em> files are what we are using to define the build system. Think of
them each as a descriptor that describes how to build its own part of the
tree. An individual <em>moz.build</em> file will contain the C++ sources to
compile, the headers to export, the tests to run, etc. Eventually.
Currently, they are limited to directory traversal information.</p>
<p><em>moz.build</em> files essentially add a level of indirection between the
build system definition and how the tree is actually built. Before
<em>moz.build</em> files, the same metadata we are now capturing in <em>moz.build</em>
files (or plan to capture) was captured in <em>Makefile.in</em> files. We
performed simple variable substitution on these <em>Makefile.in</em> files to
produce <em>Makefile</em> files in the object directory. These <em>Makefile</em> files
were used by GNU Make (or Pymake on Windows) to build the tree.</p>
<p>As I outlined in <a href="/blog/2012/06/25/improving-mozilla's-build-system/">Improving Mozilla's Build System</a>,
<em>Makefile.in</em> are suboptimal for a number of reasons. The important
bit is they essentially tie us to the use of make (recursive or otherwise).
We are very interested in supporting modern build systems such as Tup
(the theory being they will build the tree faster).</p>
<p>Enter <em>moz.build</em> files. Storing our build configuration in
<em>moz.build</em> files allows us to decouple the definition of the build
system from the tool used to build it.</p>
<h2>How moz.build Files Work</h2>
<p>At the tail end of <em>configure</em>, the build system invokes the
<em>config.status</em> script in the object directory. The role of
<em>config.status</em> is to combine the information collected during
<em>configure</em> with the build configuration obtained from <em>moz.build</em> files
and take the necessary actions to ensure the build backend (make) can
build the tree.</p>
<p>Before today, <em>config.status</em> essentially iterated over the source tree
and converted <em>Makefile.in</em> files to <em>Makefile</em> in the object directory.
Things are now slightly more complicated with <em>moz.build</em> files.</p>
<p>When <em>config.status</em> runs, it starts with the
<a href="https://hg.mozilla.org/mozilla-central/file/87de54667483/moz.build">root moz.build</a>
from the source tree. It feeds this file into a Python interpreter. It
then looks for special variables like <em>DIRS</em> and <em>PARALLEL_DIRS</em> to determine
which directories contain additional <em>moz.build</em> files. It then descends
into all the referenced directories, reading their <em>moz.build</em> files.
While this is happening, we are converting the results of each
<em>moz.build</em> file execution into <em>backend.mk</em> files that make knows how
to build. It also performs the <em>Makefile.in</em> to <em>Makefile</em> conversion
like it always has. When the whole process has finished, the object
directory contains a bunch of <em>Makefile</em> and <em>backend.mk</em> files. make
runs like it always has. The only real difference is some variables are
coming from the <em>moz.build</em>-derived <em>backend.mk</em> files instead of
<em>Makefile</em>.</p>
<p>This is just a brief overview, of course. If you want to know more,
see the code in <em>/python/mozbuild/mozbuild/frontend</em> and
<em>/python/mozbuild/mozbuild/backend</em>.</p>
<h2>Build System Future</h2>
<p>With the introduction of <em>moz.build</em> files, the intent is to <strong>eventually
completely eliminate Makefile.in and have all the build definition live
in moz.build files.</strong></p>
<p>Doing this all at once would have been next to impossible. So, we
decided to eliminate <em>Makefile.in</em> gradually. The first step is what
landed today: essentially moving <em>DIRS</em> variables out of <em>Makefile.in</em>
and into <em>moz.build</em> files. Next, we will be eliminating empty
<em>Makefile.in</em> (<a href="https://bugzilla.mozilla.org/show_bug.cgi?id=844635">bug 844635</a>)
and will be moving more parts of the build definition from <em>Makefile.in</em>
to <em>moz.build</em> files. The next part to move over will likely be IDLs
(<a href="https://bugzilla.mozilla.org/show_bug.cgi?id=818246">bug 818246</a>).
After that, it may be exported files (<em>EXPORTS</em> in <em>Makefile.in</em>
parlance). And repeat until we have no more <em>Makefile.in</em> in the tree.</p>
<p>Each migration of build definition data to <em>moz.build</em> files will likely
occur in two phases:</p>
<ol>
<li>A largely mechanical move of the variables from <em>Makefile.in</em> to
   <em>moz.build</em>.</li>
<li>Better build backend integration resulting from the move.</li>
</ol>
<p>In phase #1, we will essentially cut and paste variable assignments to
<em>moz.build</em> files. make will read the same variables it does today and
perform the same actions. The only difference is the values in these
variables will be defined in <em>moz.build</em> files.</p>
<p>In phase #2, we will leverage the fact that our build definition now has
an API. We will change our existing make backend to be more efficient.
For example, we should soon be able to compile IDLs and copy exported
headers without make traversing through the directory tree at build time.
We will be able to do this because the <em>moz.build</em> traversal at
pre-build time sees data from all <em>moz.build</em> files and with this
<em>complete world view</em> is able to produce more optimal make files than
what you would get if you recursed into multiple directories. In short:
it will make the build faster.</p>
<p>Once we have a sufficient portion of the build definition moved to
<em>moz.build</em> files we will be able to experiment with new build backends
(like Tup), look into automatic Visual Studio project generation, and
more easily experiment with different ways of building (such as
precompiled headers, fewer compiler process invocations, etc). These
should all contribute to faster build times.</p>
<h2>Frequently Asked Questions</h2>
<h4>What impact will I see from this change?</h4>
<p>If you never touched <em>Makefile.in</em> files in the tree, you should not
notice anything different about how the tree builds or how the build
system works. You should have nothing to fear.</p>
<p>The most obvious changes to the source tree are:</p>
<ol>
<li>There is a <em>moz.build</em> file in almost every directory now.</li>
<li>The variables related to directory traversal (those containing <em>DIRS</em>
   in their name) are now defined in <em>moz.build</em> files instead of
   <em>Makefile.in</em>.</li>
<li>If your <em>Makefile.in</em> contains a variable that has been moved to
   <em>moz.build</em> files, make will spew an error when processing that file
   and the build will fail.</li>
</ol>
<h4>Will this change how I build?</h4>
<p>It shouldn't. You should build the tree just like you always have. Most
of you won't notice any differences.</p>
<p>If you see something weird, speak up in #build or file a bug if you are
really confident it is wrong.</p>
<h4>What are the risks to this change?</h4>
<p>The migration of variables from <em>Makefile.in</em> to <em>moz.build</em> files is
largely mechanical and significant portions are done manually. This can
be a mind-numbing and tedious process. Not helping things is the fact
that Splinter's review interface for these kinds of patches is hard to
read.</p>
<p>This all means that there is a non-trivial risk for <em>transcription</em>
errors. All it takes is an inverted conditional block and all of a
sudden large parts of the tree are no longer built, for example.</p>
<p>We have established <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=846425">bug 846825</a>
to investigate any oddities from the initial transfer. Developers are
encouraged to help with the effort. Please spot check that your
directories are still being built, tests run, etc. Pay special attention
to changes made in the last 4 months as these parts of <em>Makefile.in</em>
would have been bit rotted and more prone to data loss.</p>
<p>Parts of the tree not enabled in standard configurations are more prone
to breakage due to less testing. i.e. build configurations not captured
by TBPL have a higher chance of breaking.</p>
<h4>Will this make the tree build faster?</h4>
<p>Not yet. But eventually it will. This initial landing paves the
groundwork to making the tree build faster (see the <em>Future</em> section
above).</p>
<h4>I see a lot of empty moz.build files!</h4>
<p>Yes. Sorry about that. The good news is they shouldn't be empty for
long. As things move from <em>Makefile.in</em> to <em>moz.build</em> we'll see fewer
and fewer empty <em>moz.build</em> files. We'll also see fewer and fewer
<em>Makefile.in</em> files once we start deleting empty <em>Makefile.in</em>.</p>
<p>If you want to know why we must have empty files, it's mainly for
validation. If we allowed <em>moz.build</em> files to be optional, how would
you detect a typo in a directory name? Directory exists? What if that
directory exists but isn't supposed to have a <em>moz.build</em> file?</p>
<h4>You bitrotted my patches!</h4>
<p>Yes. I'm sorry. The transition period to <em>moz.build</em> files could be a
little messy. There will be lots of changes to <em>Makefile.in</em> and
<em>moz.build</em> files and lots of chances for bit rot. Uplifts could be
especially nasty. (Although I don't believe many uplifts involve
significant changes to the build configuration.)</p>
<p>This all means there is a strong incentive for us to complete the
transition as quickly as possible.</p>
<h4>Can I help with the transition to moz.build files?</h4>
<p>Yes!</p>
<p>The transition is largely mechanical (at least phase #1). If you are
interested in moving a variable or set of variables, hop in #build on
IRC and speak up!</p>
<h4>You said moz.build files are actually Python files?!</h4>
<p>Yes they are! However, they are executed in a very tightly controlled
sandbox. You can't import modules, open files, etc. UPPERCASE variable
names are reserved and only a few functions are exposed. If you attempt
to assign to an unknown UPPERCASE variable or assign an invalid value,
an error will occur. This is already much better than Makefile because
we can now detect errors earlier in the build process (rather than 15
minutes into a build).</p>
<h4>What variables and functions are available in moz.build files?</h4>
<p>If you run |./mach mozbuild-reference| you will see a print-out of
all the variables, functions, and symbols that are exposed to the Python
sandbox that moz.build files execute in. There are even tests that will
fail the build if the sandbox contains symbols not in this output!</p>
<p>The output should be valid reSTructuredText (in case you want to convert
to HTML for reading in your browser).</p>
<h4>What if a moz.build file contains an error?</h4>
<p>The build will break.</p>
<p>A lot of work has gone into making the output of moz.build errors human
friendly and actionable. If you do something wrong, it won't just
complain: it will tell you how to fix it!</p>
<h4>Besides build times, how else will this improve the build system?</h4>
<p>There are several ways!</p>
<p>As mentioned above, moz.build are more strict about what data is allowed
to be defined. If you assign to an UPPERCASE variable, that variable
must be known to the sandbox or else the assignment will error. This
means that if you assign to an UPPERCASE variable, you know it has a
side-effect. No more cargo culting of old, meaningless variables!</p>
<p>To change the behavior of moz.build files (add new variables or
functions, change how makefile generation works, etc) will require
changes to the code in /python/mozbuild. This code belongs squarely to
the build module and requires appropriate review. A problem with
Makefiles is that they have lots of foot guns by default and its easy
for self-inflicted wounds to land in the tree without explicit build
peer review. This problem largely goes away with moz.build files because
the sandbox takes away all of make's foot guns.</p>
<p>The output of a moz.build execution is essentially a static data
structure. It's easy to validate them for conformance. If we discover
bad practices in our build definition, we can centrally add tests for
them and enforce best practices.</p>
<p>We will also see user experience wins by moving data to moz.build files.
Take mochitests for an example. We currently have several flavors
(plain, browser, chrome, etc). Sometimes you cannot distinguish the
flavor by the filename alone. With moz.build files, it will be easier to
answer questions like "what mochitest flavor is this file?" mach could
hook into this so you can type |mach mochitest path/to/file.html|
instead of |mach mochitest-plain path/to/file.html|. Even better, you
should just be able to type |mach path/to/test.html| and mach knows from
the build definition that path/to/test.html is a plain mochitest file
and assumes you want to run it. There are dozens of small development
workflow wins to be gained here!</p>
<h4>If I change a moz.build file, what happens?</h4>
<p>If you change a moz.build file, then make should detect that it
has changed and it will update the dynamically generated <em>backend.mk</em>
file and reinvoke the requested build action. This should all happen
automatically (just like <em>Makefile.in</em> to <em>Makefile</em> conversion works
automatically).</p>
<h4>My build seems to pause for a few seconds before starting!</h4>
<p>A change to <em>any</em> moz.build file will cause a full traversal of the
entire moz.build tree. On modern machines, this should only take 1-3
seconds. If your source tree is not in the page cache (and you need
to load moz.build files from disk) or if you are on older hardware, this
could be a little slower.</p>
<p>This is an unfortunate side-effect of having a whole world view of the
build definition. The build delay incurred by these full scans should
eventually be cancelled out by build backend optimizations resulting
from having this whole world view, however.</p>
<p>The good news is this full scan should only occur if a mozbuild file
changes. And, if you are performing make recursion, it should only
happen once (not in every directory). If you notice multiple moz.build
scanning-related pauses during builds, please file a bug in Core ::
Build Config!</p>
<p>Finally, we are performing the reads on a single thread currently. We
can throw more cores at the task if someone codes up a patch.</p>
<h4>What happened to allmakefiles.sh?</h4>
<p>It has been sacked. allmakefiles.sh was an optimization to perform all
the Makefile.in to Makefile conversion in one go. The directory
traversal performed by moz.build reading effectively replaces the role
of allmakefiles.sh. Not only that, but the moz.build build definition is
always up to date! allmakefiles.sh was typically out of sync with
reality and was a burden to maintain.</p>
<h4>Did we just invent our own build system?</h4>
<p>Kinda. We invented a generic Python sandboxing infrastructure. Then we
hooked up code to populate it with variables from our build system and
told it how to perform file traversal by reading specific variables set
during file execution. Then we hooked up code for taking the evaluated
results of all those sandboxes and convert them into make files.</p>
<p>Conceptually, what we invented is like GYP but with a different config
file format. We have dabbled with the idea of converting the parsed
build definition into GYP classes and then leveraging GYP to produce
Makefiles, Ninja files, Visual Studio Projects, etc. This would an
interesting experiment!</p>
<p>If you want to call it a build system, call it a build system. However,
it is currently tightly coupled to Mozilla's needs, so you can't just
use it anywhere. The concept might be worth exploring, however.</p>
<h4>Is there anything else you'd like to share?</h4>
<p>I think we set the record for most parts in a bug: 61. Although, they
are numbered 1-17, 19-20. Part 18 has 30+ sub-parts using letters from
the English and Greek alphabet for identifiers. Part 61 uses the
infinity symbol as its number. See the
<a href="https://hg.mozilla.org/mozilla-central/pushloghtml?changeset=c65d59d33aa8">pushlog</a>.</p>
<p>Finally, I'd like to thank everyone who helped with this effort. The bug
itself was <em>only</em> 6 months old and had active development off and on for
a lot of it. Ted Mielczarek and Mike Hommey provided invaluable feedback
on the core build system patches. A number of module owners stepped in
to lend eyes to the mechanical conversion of their files. Last but not
least, Ms2ger provided invaluable review aid on many of the patches. The
work was so good that we all agreed that an Ms2ger f+ was good enough
for a build peer rs! If reviewing the patches wasn't enough, Ms2ger
also oversaw the tree closure and merging of the landing. I don't know
how I will repay this debt.</p>
<h4>Any more questions?</h4>
<p>If you have more questions, drop in #build on irc.mozilla.org and ask
away.</p>]]></content:encoded>
    </item>
    <item>
      <title>Thoughts on Logging - Part 1 - Structured Logging</title>
      <link>http://gregoryszorc.com/blog/2012/12/06/thoughts-on-logging---part-1---structured-logging</link>
      <pubDate>Thu, 06 Dec 2012 21:55:00 PST</pubDate>
      <category><![CDATA[Mozilla]]></category>
      <category><![CDATA[logging]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2012/12/06/thoughts-on-logging---part-1---structured-logging</guid>
      <description>Thoughts on Logging - Part 1 - Structured Logging</description>
      <content:encoded><![CDATA[<h2>Forward</h2>
<p>For reasons I can't explain, I have a keen interest in computer logging
systems. Perhaps it stems from my frustration that such a
seemingly trivial task can often be quite complicated. Perhaps it's from
the scaling challenges inherent with inevitably needing to analyze
terabytes of logged data. Perhaps it's from my experience designing
highly-available distributed server application and needing to use
logging for real-time monitoring and decision making. Whatever the
root causes, I've always wanted to create a brain dump of my thoughts
on logging. So, here we go!</p>
<h2>Introduction</h2>
<p>Logging. Practically every computer program does it in some form or
another. The standard libraries for many programming languages provide
a logging facility. But, have you ever stopped to think about the side
effects of logging or why you log the way you do?</p>
<p>In this first post, I will briefly define what I consider logging and
will talk about something called <em>structured logging</em>.</p>
<h2>The Definition of Logging</h2>
<p>For the scope of these posts, I will define logging to be:</p>
<pre><code>An emitted stream of distinct messages from a running application
</code></pre>
<p>A <em>message</em> can be pretty much anything. They are often strings. But,
they could be binary data. The transport mechanism is undefined.
Although, most logging systems typically write to a file (eventually).</p>
<p>I should also call out the difference between <em>reading</em>/<em>interpreting</em>
logs and <em>transporting</em> them. When I say <em>reading a log</em>, I'm referring
to a process where an entity (either human or machine) <em>reads</em> the
individual message content and infers something from it. By contrast, a
<em>transporter</em> treats messages as a black box and just shuffles bits
around. <em>Transporters</em> also typically send or store logging messages
somewhere.</p>
<p>It's worth explicitly noting that this definition encompasses both
tradtional <em>print()</em> based logging (writing a line to a file or output)
as well as things like transaction logs or recording events as new rows
in a database.</p>
<h2>The Multiple Use Cases of Logging</h2>
<p>Before I get to some of the interesting aspects of logging, let's
identify <em>some</em> of the use cases of logging (in no particular order):</p>
<ol>
<li>Recording application errors (e.g. exceptions and stack traces)</li>
<li>Low-level information to aid in debugging or human observation</li>
<li>Application monitoring (includes metrics and alerting)</li>
<li>Business analytics (use the logged data to help make decisions)</li>
<li>Security auditing</li>
</ol>
<p>Now that we have the basics out of the way, let's move on to the crux of
this post.</p>
<h2>Logging for Humans or Machines</h2>
<p>I would argue that every log message is intended to be <em>interpreted</em>
by either a human or a machine.</p>
<p>If a log message is only going to be used to help a human debug a system
or casually see what's going on, it's intended for humans. If a log
message is going to be used for machine analysis later (e.g. monitoring,
alerting, business analytics, security auditing, etc), it's intended for
machines.</p>
<p>This is an important distinction. If it isn't clear, please re-read the
prior paragraphs. Now might also be a good time for you to audit the
logs in your life and assign <em>human</em> or <em>machine</em> labels to them.</p>
<h2>The Problem</h2>
<p><strong>The problem with logging as most of us know it is that it is optimized
for human consumption, even if machines are the intended consumer.</strong></p>
<p>At the heart of most logging systems we log a string, a char *, an
array of characters that can easily be read by humans. For example, a
simple web application might produce log files containing lines like:</p>
<pre><code>2012:11:24T17:32:23.3435 INFO gps@mozilla.com successfully logged in
</code></pre>
<p>Human readability is a great feature to have. However, it comes at a
price: more difficult machine consumption.</p>
<p><strong>By optimizing for human consumption, we've introduced a new problem:
log parsing.</strong></p>
<p>In the above example log message, what the program has done is combined
a few distinct fields of data (an event - <em>logged in</em>, the
username, and the time) into a single text string. This is great for
humans glancing at a log file. But that decision now necessitates
downstream machine consumers to <em>parse</em>/<em>decode</em> that text back into its
original, distinct fields.</p>
<p>I call this style of logging <em>destructured</em> or <em>unstructured</em> logging. We
start with distinct fields of data and then <em>destructure</em> them into a
fully free-form or semi-formed text. By doing so, we lose part
of the original data structure and necessitate the need to reconstruct
it (via parsing).</p>
<p>This seems silly to me.</p>
<p>Why are we formatting distinct fields of data into unified strings only
to have them parsed into their original components again? If lossless
and simple round-trip encoding/decoding is your goal, there are much
better ways than freely-formed text! Going <em>to</em> text is easy
(<em>printf()</em> or string concatenation are how it is commonly performed).
Getting back to the original representation is the real problem.</p>
<p>Every persisted log message destined for machines will need to be decoded
to some degree to facilitate analysis. However, by using destructured
text, nearly every program and logging system forces this decoding
problem onto its users by using an encoding that is difficult to code.</p>
<p>Every time you are analyzing a log file containing destructured text you
need to solve the parsing/decoding problem. Every time you write an
<em>awk</em>, <em>grep</em>, <em>sed</em>, <em>perl</em>, etc script, use regular expressions, etc
to scrape logs you are solving this problem. <strong>How much time do you
spend decoding/parsing logs versus actually doing something useful with
the data?</strong> From my personal experience, a lot. And, parsing
destructured text is a very fragile solution. What happens when the
string changes, even by just a little? Does that throw off your
decoder/parser?</p>
<p>Decoding log messages is a hard problem. Furthermore, it's a problem I'd
rather not have.</p>
<h2>Structured Logging to the Rescue</h2>
<p><img alt="What if I told you there's a better way to log" src="/images/morpheus_logging.jpg" /></p>
<p>One of my early professional mentors taught me that one of the best ways
to solve a problem is to change the requirements so that the problem no
longer exists or becomes easier to solve. (If that didn't immediately
click with you, you might want to read it again because I feel it's an
invaluable skill and one that many engineers don't employ).
Applying that lesson here, how do we eliminate the problem of
parsing destructured textual data back into its original fields?  Simple:
we don't emit destructured text! Instead, we emit data that is easily
machine readable, where each individual field is preserved. <strong>This is
called structured logging.</strong></p>
<p>The primary benefit of structured logging is that you eliminate the
parsing problem (or at least you change it to be a solved problem by
using a common or well-defined encoding format). No tedious searching
for data within free-form text and extracting it. Instead, the fields
from the event-producing application are seamlessly preserved and
loaded into downstream machine consumers.</p>
<h3>A Closer Look</h3>
<p>What does structured logging look like? Well, it depends. There is no
standard for structured logging. When I first started writing this post
in June 2011, a Google search for <em>structured logging</em> didn't yield
much. Today, things aren't that much better. Although, there has been
significant activity in the last 15 months. rsyslog has launched
<a href="http://blog.gerhards.net/2012/02/announcing-project-lumberjack.html">project Lumberjack</a>,
which has structured logging as a pillar. There is even someone sitting
10 ft from me at Mozilla's SF office doing structured logging for server
applications (<a href="https://wiki.mozilla.org/Services/Sagrada/Metlog">metlog</a>).</p>
<p>All structured logging systems are different. But, the concepts
are mostly the same.</p>
<p>From the perspective of a log-producing application, a function call to
produce an unstructured message may look something like:</p>
<div class="pygments_murphy"><pre><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="n">username</span> <span class="o">+</span> <span class="s">&quot; successfully logged in&quot;</span><span class="p">)</span>
</pre></div>

<p>Whereas a call to produce that same message in structured form may look
like:</p>
<div class="pygments_murphy"><pre><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s">&quot;successful_login&quot;</span><span class="p">,</span> <span class="n">who</span><span class="o">=</span><span class="n">username</span><span class="p">)</span>
</pre></div>

<p>You'll immediately notice that the API retains the original pieces of
data. In this example, we just have the event name (<em>successful_login</em>)
and <em>who</em> (some variable value).</p>
<p>This will look different in every programming language, of course. And,
depending on the structured logging mechanism, the common metadata
produced with each message (e.g. time and log level) may differ. But the
general idea is you preserve the rich data fields as they cross over
into the logging facility rather than <em>casting</em> them to a string.
Behind the scenes, the structured logger retains the fields (the
original structure) and encodes them into something machine readable.</p>
<p>Put another way, structured loggers log objects, not strings.</p>
<p>How objects are encoded again depends on the structured logger
implementation. JSON is a well-understood encoding format. An example
JSON encoding of our example message might look like:</p>
<div class="pygments_murphy"><pre><span class="p">[</span><span class="mf">1354317157.034534</span><span class="p">,</span> <span class="s">&quot;INFO&quot;</span><span class="p">,</span> <span class="s">&quot;sucessful_login&quot;</span><span class="p">,</span> <span class="p">{</span><span class="s">&quot;who&quot;</span><span class="p">:</span> <span class="s">&quot;gps@mozilla.com&quot;</span><span class="p">}]</span>
</pre></div>

<p>That's an array of current time, log level, event name/message type, and
a mapping of the data fields.</p>
<p>Or, you could opt for a more efficient encoding format, such as
protocol buffers (which is binary).</p>
<p>The actual encoding theoretically doesn't really matter as long as it can
easily be decoded back into its original data. A good way to identify this
trait is whether you need to implement a decoder/parser more than once
per programming language or if you need to apply different decoding
mechanisms based on the content of a message. If you do, you probably
don't have a generic enough encoding format.</p>
<p>Going back to implementation details, what about a consumer? In
JavaScript, our JSON structured logger from above might look something
like:</p>
<div class="pygments_murphy"><pre><span class="k">for</span> <span class="p">(</span><span class="nx">let</span> <span class="nx">line</span> <span class="nx">of</span> <span class="nx">log</span><span class="p">)</span> <span class="p">{</span>
  <span class="nx">let</span> <span class="p">[</span><span class="nx">time</span><span class="p">,</span> <span class="nx">level</span><span class="p">,</span> <span class="nx">action</span><span class="p">,</span> <span class="nx">fields</span><span class="p">]</span> <span class="o">=</span> <span class="nx">JSON</span><span class="p">.</span><span class="nx">parse</span><span class="p">(</span><span class="nx">line</span><span class="p">)</span>

  <span class="k">if</span> <span class="p">(</span><span class="nx">action</span> <span class="o">==</span> <span class="s2">&quot;successful_login&quot;</span><span class="p">)</span> <span class="p">{</span>
    <span class="nx">let</span> <span class="nx">who</span> <span class="o">=</span> <span class="nx">fields</span><span class="p">.</span><span class="nx">who</span><span class="p">;</span>

    <span class="c1">// Do something interesting.</span>
  <span class="p">}</span> <span class="k">else</span> <span class="k">if</span> <span class="p">(</span><span class="nx">action</span> <span class="o">==</span> <span class="s2">&quot;XXX&quot;</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1">// ...</span>
  <span class="p">}</span>
<span class="p">}</span>
</pre></div>

<p>Compare this with a destructured parser:</p>
<div class="pygments_murphy"><pre><span class="k">for</span> <span class="p">(</span><span class="nx">let</span> <span class="nx">line</span> <span class="nx">of</span> <span class="nx">log</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">if</span> <span class="p">(</span><span class="nx">line</span><span class="p">.</span><span class="nx">endsWith</span><span class="p">(</span><span class="s2">&quot;successfully logged in&quot;</span><span class="p">))</span> <span class="p">{</span>
    <span class="c1">// Find username.</span>
    <span class="nx">let</span> <span class="nx">spaceIndex</span> <span class="o">=</span> <span class="nx">line</span><span class="p">.</span><span class="nx">indexOf</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="p">);</span>
    <span class="nx">let</span> <span class="nx">username</span> <span class="o">=</span> <span class="nx">line</span><span class="p">.</span><span class="nx">substr</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nx">spaceIndex</span> <span class="o">-</span> <span class="mi">1</span><span class="p">);</span>

    <span class="c1">// Do something interesting.</span>
  <span class="p">}</span> <span class="k">else</span> <span class="k">if</span> <span class="p">(</span><span class="nx">line</span><span class="p">.</span><span class="nx">indexOf</span><span class="p">(...))</span> <span class="p">{</span>
    <span class="c1">//</span>
  <span class="p">}</span>
<span class="p">}</span>
</pre></div>

<p>This example admittedly isn't very complex. But, you can already see
some differences. Notably, it takes 1 line to completely parse <em>any</em>
message using the structured logger. The unstructured text parser
requires multiple lines of code <em>per message type</em>. One of those scales
better than the other.</p>
<p>Before I conclude, it's worth mentioning something about event
names/enumerations. You may notice I gave <em>event name</em> a dedicated
field in my example. This is intentional. In a subsequent post,
I will talk about the importance of event names/enumerations and
schemas. Until then, just assume that having a unique event name is
an important aspect of a well-designed structured logging system.
Hence its inclusion in my examples above.</p>
<h2>Conclusion</h2>
<p>I think the vast majority of application logging is done suboptimally
because it forces a <strong>hard</strong> problem of decoding/parsing onto consumers.
Given that many logs are intended for machines, I think this is a
problem that shouldn't need to exist and the software industry should
rethink its current approach to logging by offering first-class
structured logging facilities and subsequently encouraging their use.</p>
<p>If application logging retained the data behind messages, the problem
of parsing/decoding log messages would largely vanish and we would be
left with the real problem: actually doing something useful with
logged data. That is the point of logging after all, isn't it?</p>
<p>Now, this may sound like I'm waiving my hands a bit. You are right:
structured logging by itself is no silver bullet. There are many ways to
do it suboptimally and to use it when you shouldn't. In subsequent posts
to this series, I will dive deeper into how to properly design a
(structured) logging system. I will also investigate some of the
performance implications associated with different logging mechanisms.
Stay tuned.</p>]]></content:encoded>
    </item>
    <item>
      <title>Changes to How mach Loads mozconfigs</title>
      <link>http://gregoryszorc.com/blog/2012/12/05/changes-to-how-mach-loads-mozconfigs</link>
      <pubDate>Wed, 05 Dec 2012 10:30:00 PST</pubDate>
      <category><![CDATA[Mozilla]]></category>
      <category><![CDATA[Firefox]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2012/12/05/changes-to-how-mach-loads-mozconfigs</guid>
      <description>Changes to How mach Loads mozconfigs</description>
      <content:encoded><![CDATA[<p>If you use mach to build Firefox, there is a chance you may run into
some errors from a change that landed on mozilla-central Wednesday.</p>
<p>I reimplemented the mozconfig finding and loading logic in Python and
with a nice shell wrapper that lets us more easily inspect side-effects
of execution. The overall goal is to speed up mozconfig loading (fewer
shell invocations) as well as to have a strong API for interacting with
mozconfigs so we can do things like better validation. As a side effect,
we now have test coverage of mozconfig behavior!</p>
<p>Anyway, because the logic for mozconfig finding and loading is going
through a completely new mechanism, you may notice some weird behavior.
Until a few minutes ago, $topsrcdir wasn't defined in the execution
environment. This is now fixed. I think most complaints stemmed from
this.</p>
<p>Another significant change is that mozconfig shell scripts are now
executed in a shell that has <em>set -e</em>. This means that failing commands
in your mozconfig will now abort execution. Before, execution would have
silently continued unless you checked the exit code in your mozconfig.
I think this is a change for the better, but it may break your
mozconfigs.</p>
<p>If you encounter an issue, please follow the instructions in mach's
output to file a bug. Please attach or link to your mozconfig.</p>]]></content:encoded>
    </item>
    <item>
      <title>Firefox Build System Presentation</title>
      <link>http://gregoryszorc.com/blog/2012/11/30/firefox-build-system-presentation</link>
      <pubDate>Fri, 30 Nov 2012 14:00:00 PST</pubDate>
      <category><![CDATA[Mozilla]]></category>
      <category><![CDATA[Firefox]]></category>
      <category><![CDATA[build system]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2012/11/30/firefox-build-system-presentation</guid>
      <description>Firefox Build System Presentation</description>
      <content:encoded><![CDATA[<p>In case you missed it, I gave a presentation on the state of Firefox's
build system yesterday.</p>
<p>You can <a href="https://air.mozilla.org/the-future-of-the-firefox-build-system/">watch</a>
it and <a href="http://gregoryszorc.com/presentations/2012-11-29-firefox-build-system/#1">view</a>
the slides online.</p>
<p>If you build Firefox from source regularly, you should definitely at
least skim through the slide deck.</p>
<p>I'm not an HTML expert, so my apogolies for bad UI on the interactive
slides. You may need to press <strong>enter</strong> to select items in
dropdown menus. Also, the interactive slides are a bit resource
intensive. If the slide deck is really slow, turn off those elements.
I've also only tested the slides in Firefox 19 and 20. My apologies if
they don't work everywhere.</p>]]></content:encoded>
    </item>
    <item>
      <title>Mach Has Landed</title>
      <link>http://gregoryszorc.com/blog/2012/09/26/mach-has-landed</link>
      <pubDate>Wed, 26 Sep 2012 17:30:00 PDT</pubDate>
      <category><![CDATA[Mozilla]]></category>
      <category><![CDATA[Firefox]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2012/09/26/mach-has-landed</guid>
      <description>Mach Has Landed</description>
      <content:encoded><![CDATA[<p>Hacking on Firefox is <em>hard</em>. Compiling the source tree for the first
time is a daunting experience. If you manage to do that, you still need
to figure out how to submit patches, write and run tests, etc. There is
no shortage of points where people can get confused, make mistakes, or
just give up because the barrier is too high.</p>
<p>I have strong convictions about making the overall developer experience
better (developers are users too, after all). The easier you make the
developer experience, the greater the chances of retaining developers.
And retaining developers means more contributions: more features and
fewer bugs. This translates to a better state for the Mozilla Project.
This makes the world a better place.</p>
<p>Since practically my first day working at Mozilla, I've been
experimenting with ways to make contributing to Firefox easier by
improving the build system or interaction with it.</p>
<p>With a lot of help, I've finally succeeded in landing something into the
Firefox source tree that I think will ultimately lead to a much better
developer experience.</p>
<p>It's called <strong>mach</strong> (German for <em>do</em>) and if you pull the latest
version of <a href="https://hg.mozilla.org/mozilla-central">mozilla-central</a>
(Firefox's main source code repository), you can run mach today.</p>
<h2>Running Mach</h2>
<p>You can run Mach by simply executing it from the root directory in the
repository:</p>
<pre><code>$ ./mach
</code></pre>
<p>Ideally, I shouldn't have to tell you anything else: mach's output
should provide you all the instruction you need to use it. If it
doesn't, that is a bug and it should be fixed.</p>
<p>Realistically, mach is still in early development and its user
experience still leaves a lot to be desired.</p>
<p>Because technical usage docs belong in a medium that is easily
discoverable and where the community can improve on them (not a
post on a personal blog), you can find additional usage information in
the
<a href="https://developer.mozilla.org/En/Developer_Guide/mach">mach article</a> on
MDN. The
<a href="https://hg.mozilla.org/mozilla-central/file/default/python/mach/README.rst">mach README</a>
holds more technical information for people wanting to poke at the inner
workings.</p>
<p>Mach does require Python 2.7. The build system will likely soon require
Python 2.7 as well. So, if you don't have Python 2.7, you should upgrade
now before you lose the ability to build the tree. Conveniently, the
tree now has a
<a href="http://gregoryszorc.com/blog/2012/09/18/bootstrap-your-system-to-build-firefox">bootstrap script</a>
which covers the installation of Python. So, Python 2.7 should just be a
simple command away.</p>
<h2>Features</h2>
<p>Why would you use mach? Good question! Compared to the existing
out-of-the-box experience, mach adds:</p>
<ul>
<li>Ability to run xpcshell and mochitest tests from the source directory.
  This means you can tab-complete test filenames in your shell and it
  <em>just works</em>.</li>
<li>Parsing of compiler warnings (currently supports Clang 3.1 and MSVC
  formats) into a unified warnings database (actually a JSON file).
  After builds you can run <em>./mach warnings-list</em> or <em>./mach
  warnings-summary</em> to get a concise list without having to look at
  build logs.</li>
<li>A single command-line interface where you can easily discover new
  functionality. Just run <em>./mach help</em> (try doing that with make!).</li>
</ul>
<p>Naysayers will say this is a paltry list. They are correct. I have
bigger plans. But, you have to start somewhere.</p>
<h2>Goals and Future Ideas</h2>
<p>The overall goal of mach is to improve the developer experience of
developing Firefox and other Gecko applications. It does this by
providing a convenient, central command in the root directory of
the repository that acts as both an oracle to discover new commands
and functionality (<em>./mach help</em>) as well as a proxy to execute them.
You don't need to worry about environment variables, working directories,
or finding some random script hidden in the bowells of the source tree.
You just run a single command and the world is brought to you. No build
documentation. No outdated wikis. No copying commands into your shell.
No having to scour random blogs for useful tools. You just clone the
repository, run a command, see what you can do, and get stuff done. Your
shell-literate grandmother could do it.</p>
<p>Mach should be your springboard to in-tree developer tools and increased
productivity. You shouldn't need anything except a copy of the source
tree and mach.</p>
<p>Like Git and Mercurial, mach is powered by the concept of
sub-commands/actions. So, one simply needs to register a new sub-command
with the mach driver. This involves writing a Python class. Once you do
that, people can discover and use that functionality. All batteries are
included with a copy of mozilla-central.</p>
<p>As stated above, the current set of commands is rather small. But, the
sky is the limit. Here are some of my ideas:</p>
<ul>
<li>Ability to upload, download, and apply patches from Bugzilla (Burak
  YiÄŸit Kaya, Jeff Hammel, Clint Talbert and I have already talked about
  this -- progress tracked in
  <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=774141">bug 774141</a>).</li>
<li>Automatically configure Mercurial with optimal settings (ensure user
  info is set, proper lines of diff context, enable mqext, etc). Tracked
  <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=794580">bug 794580</a>.</li>
<li>Submit Try builds. The <a href="https://github.com/pbiggar/trychooser">trychooser</a>
  Mercurial extension could easily live as a mach subcommand! Tracked in
  <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=774137">bug 774137</a>.</li>
<li>Identify Bugzilla components and/or reviewers from files touched by
  patch. It's all in the history of the touched files and the history of
  the old bugs referenced by those commits!
  <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=774145">Bug 774145</a>.</li>
<li>Interaction with the <em>self-serve</em> build API. That thing on TBPL to
  control individual builds - we could make a CLI interface for it.
  (Steve Fink and/or Jeff Hammel already have
  <a href="https://hg.mozilla.org/users/josh_joshmatthews.net/self-serve-tools">code</a>
  for this - it just needs to be integrated).
  <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=774147">Bug 774147</a>.</li>
</ul>
<p>If you have an idea for a feature, please
<a href="https://bugzilla.mozilla.org/enter_bug.cgi?product=Core&amp;component=mach">file a bug</a>.
Please note there are many features on file already. However, some
obvious ones such as integration with missing test suites have yet to be
filed (at least at the time I wrote this post).</p>
<p>If you wrote an awesome developer tool and would like others to use it
(without having to rely on people discovering it by reading a corner of the
Internet), add it to mach! Use mach as a wedge to get more exposure and
users. File a bug. I will happily r+ patches that add useful developer
functionality to the tree.</p>
<h2>What this Means / Longer Term Future</h2>
<p>While there is no timetable, mach will eventually replace <em>client.mk</em>.
client.mk, like mach, is essentially a CLI driver for the build
system. Having the driver implemented in Python rather than make has many
compelling advantages. I could probably write a whole post on it, but
I'll spare the world from my ranting.</p>
<p>Anyway, this all means that you may want to start re-training your
muscle memory now. Stop typing <em>make</em> and start typing <em>mach</em>. If you
need to type <em>make</em> because mach does not provide a feature, this is a
missing feature from mach.
<a href="https://bugzilla.mozilla.org/enter_bug.cgi?product=Core&amp;component=mach">File a bug</a>
and request a new mach feature!</p>
<p>I want to condition people to stop typing <em>make</em>, especially in the object
directory. There are drastic changes to the build system in the works
(<a href="https://bugzilla.mozilla.org/show_bug.cgi?id=784841">bug 784841</a> is
the tip of the iceburg). These changes will require the build system to
be treated as a black box. So, things like invoking make from the object
directory will eventually break. You'll need to go through an
intelligent tool to invoke the build system. Mach will be that tool.</p>
<h2>Thanks</h2>
<p>I would like to single out the following individuals for helping to land
mach:</p>
<ul>
<li>Jeff Hammel for doing the bulk of the reviewing. He shares my vision
  for mach and how it will make the overall developer experience much
  more pleasant and how this will translate to better things for The
  Project. In my mind, Jeff deserves as much credit for landing mach as
  I do.</li>
<li>Mike Hommey and Ms2ger for review help. Mike Hommey helped identify a
  lot of issues with build system integration. Ms2ger provided lots of
  general feedback on Python issues and API design.</li>
<li>Mike Connor (my manager) for allowing me to work on this. It's not
  related to my job description in any way so he <em>could</em> say I shouldn't
  be spending time on this. But, he realizes the positive impact this can
  have and has been supportive of it.</li>
</ul>
<p>I hope you find mach useful!</p>]]></content:encoded>
    </item>
    <item>
      <title>Bootstrap Your System to Build Firefox</title>
      <link>http://gregoryszorc.com/blog/2012/09/18/bootstrap-your-system-to-build-firefox</link>
      <pubDate>Tue, 18 Sep 2012 17:00:00 PDT</pubDate>
      <category><![CDATA[Mozilla]]></category>
      <category><![CDATA[Firefox]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2012/09/18/bootstrap-your-system-to-build-firefox</guid>
      <description>Bootstrap Your System to Build Firefox</description>
      <content:encoded><![CDATA[<p>If you've looked at the
<a href="https://developer.mozilla.org/en-US/docs/Simple_Firefox_build">build instructions</a>
for Firefox lately, you may have noticed something new: support for
system bootstrapping!</p>
<p>Now checked in to mozilla-central is a
<a href="https://hg.mozilla.org/mozilla-central/file/default/python/mozboot/">framework</a>
for ensuring your system is capable of building mozilla-central and
Firefox. You just need to download and run a single Python script and it
performs magic.</p>
<p>Kudos go out to a community contributor, kmm (name wasn't revealed) for
doing the legwork for tracking down and verifying things worked on all
the Linux distros. Richard Newman and Jeff Hammel also helped with code
reviews. Just hours after it landed, Landry Breuil contributed support
for OpenBSD!</p>
<p>Currently, bootstrapping works for the following:</p>
<ul>
<li>Ubuntu Linux</li>
<li>Mint</li>
<li>CentOS 6.x</li>
<li>Fedora</li>
<li>OS X 10.6, 10.7, and 10.8</li>
<li>OpenBSD</li>
</ul>
<p>If you want to add support for an additional OS, please file a
<a href="https://bugzilla.mozilla.org/enter_bug.cgi?product=Core&amp;component=Build%20Config">Core : Build Config</a>
bug. Likewise, if you encounter an issue, please file a bug so others
don't get tripped up by it!</p>
<p>Bootstrap support is definitely in its infancy. It still needs features
like better prompting and opportunities for user choice (e.g. support
MacPorts on OS X - currently it only works with Homebrew). But, I think
it is much better than what existed previously, especially on OS X.</p>
<p>I consider this bootstrapping component just one piece in a larger
mission to make developing and building Firefox (and other Gecko
applications) easier. This should (hopefully) lead to more development
involvement. The next component to land will likely be
<a href="http://gregoryszorc.com/blog/2012/07/25/mozilla-build-system-plan-of-attack">mach</a>.
It's (finally) been getting some review love (thanks Jeff Hammel!), so
I'm optimistic it will land in the next few weeks.</p>]]></content:encoded>
    </item>
    <item>
      <title>Visual Studio Project Generation for mozilla-central</title>
      <link>http://gregoryszorc.com/blog/2012/08/28/visual-studio-project-generation-for-mozilla-central</link>
      <pubDate>Tue, 28 Aug 2012 12:00:00 PDT</pubDate>
      <category><![CDATA[Mozilla]]></category>
      <category><![CDATA[Firefox]]></category>
      <category><![CDATA[build system]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2012/08/28/visual-studio-project-generation-for-mozilla-central</guid>
      <description>Visual Studio Project Generation for mozilla-central</description>
      <content:encoded><![CDATA[<p>I have very alpha support for Visual Studio project generation for
mozilla-central that daring people can dogfood.</p>
<p>I want to emphasize that this is extremely alpha. Normally, I wouldn't
release things as fragile as they are. But, I know Windows developers
sorely miss Visual Studio, especially IntelliSense. The current Visual
Studio projects support IntelliSense, so I want to get this in the hands
of Windows developers ASAP.</p>
<p>The current directions for making this all work are a bit hacky. Things
will change once things have matured. For now, please excuse the mess.</p>
<p>First, you will need to grab the code. If you use Git, set up a remote
to my repository:</p>
<pre><code>git remote add indygreg git://github.com/indygreg/mozilla-central.git
git fetch indygreg
</code></pre>
<p>The branch of interest is <em>build-splendid</em>. I periodically rebase this
branch on top of master. You have been warned.</p>
<p>You can switch to this branch:</p>
<pre><code>git checkout -b build-splendid indygreg/build-splendid
</code></pre>
<p>Alternatively, you can squash it down to a single commit and merge it
into your local branch. Once you've done that, you can record the SHA-1
of the commit and cherry-pick that wherever you like!</p>
<pre><code>git merge --squash indygreg/build-splendid
git commit
</code></pre>
<p>In the current state, you need to build the tree or the Visual Studio
projects will complain about missing files. It doesn't matter if you
build the tree before or after Visual Studio projects are generated.
But, we might as well get it out of the way. From your MozillaBuild
environment, run:</p>
<pre><code>./mach build
</code></pre>
<p>That should <em>just work</em>. If it doesn't, you may need to configure
mach.ini. See my <a href="http://gregoryszorc.com/blog/2012/08/15/build-firefox-faster-with-build-splendid/">previous post</a>
on how to configure mach.ini. As a reference, my Windows config is:</p>
<pre><code>[build]

configure_extra = --disable-webgl

[compiler]

[paths]
source_directory = c:\dev\src\mozilla-central-git
object_directory = c:\dev\src\mozilla-central-git\objdir
</code></pre>
<p>Now, to generate Visual Studio project files:</p>
<pre><code>./mach backendconfig visualstudio
</code></pre>
<p>That should take about a minute to finish. When it's done, it should
have created <em>objdir/msvc/mozilla.sln</em>. You should be able to load that
in Visual Studio!</p>
<p>You will need to regenerate Visual Studio project files when the build
config changes. As a rule of thumb, do this every time you pull source.
You don't need to perform a full build before you generate Visual Studio
files (you do need to perform configure, however). However, if you have
not performed a full build, Visual Studio may not be able to find some
files, like headers generated from IDLs.</p>
<p><strong>Please close the solution before regenerating the project files.</strong> If
you don't, Visual Studio puts up a modal dialog for each project file
that changed and you have to click through over a hundred of these. It's
extremely frustrating. I'm investigating workarounds.</p>
<h2>Current State</h2>
<p>Currently, it only generates projects for C/C++ compilation (libraries).
I still need to add support for IDL, headers, etc. However, each
project has proper compiler flags, header search paths, etc. So,
IntelliSense is happy and some things do manage to compile!</p>
<p>Many parts are broken and sub-par.</p>
<p>I've only tested on Visual Studio 2008. If you are running Visual Studio
\2010, you can try to upgrade the solution. This <em>may</em> work. The backend
supports generating solutions for different versions. But, I haven't
tested things work on non-2008 and I don't want to expose untested behavior.</p>
<p>Compiling within Visual Studio works for some things. On my system, I
get a lot of <em>nullptr not defined</em> errors. I'm not sure why. This will
hopefully be worked out soon.</p>
<p>If you do manager to compile within Visual Studio, the output files
don't go in the right places. So, if you do a build from the
command-line, it will have to re-compile to pick up changes.</p>
<p>Project names are based on the name of the library they produce. I'm not
sure if this is the best solution.</p>
<p>Project dependencies are not set up. They will be added later.</p>
<p>Projects for linking libxul or building firefox.exe are not yet
provided. Along the same vein, debugging support is not built-in. I'm
working on it.</p>
<p>Basically, IntelliSense works. You can now use Visual Studio as a rich
editor. Hopefully this is a step in the right direction.</p>
<p>I'm anxious to hear if this works for other people. Please leave
comments!</p>]]></content:encoded>
    </item>
    <item>
      <title>Build Firefox Faster with Build Splendid</title>
      <link>http://gregoryszorc.com/blog/2012/08/15/build-firefox-faster-with-build-splendid</link>
      <pubDate>Wed, 15 Aug 2012 14:30:00 PDT</pubDate>
      <category><![CDATA[Mozilla]]></category>
      <category><![CDATA[Firefox]]></category>
      <category><![CDATA[build system]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2012/08/15/build-firefox-faster-with-build-splendid</guid>
      <description>Build Firefox Faster with Build Splendid</description>
      <content:encoded><![CDATA[<p>Would you like to build Firefox faster? If so, do the following:</p>
<pre><code>hg qimport http://people.mozilla.org/~gszorc/build-splendid.patch
hg qpush
rm .mozconfig* (you may want to create a backup first)
./mach build
</code></pre>
<p>This should <em>just work</em> on OS X, Linux, and other Unix-style systems.
<strong>Windows support is currently broken, sorry.</strong></p>
<p><em>mach</em> can do much more than build. Run the following to see what:</p>
<pre><code>./mach --help
</code></pre>
<h2>Important Info</h2>
<p><em>mach</em> replaces client.mk. <em>mach</em> has its own configuration file. The
first time you run <em>mach</em>, it will create the file <em>mach.ini</em> in the
same directory as the <em>mach</em> script. This is your new <em>mozconfig</em> file.</p>
<p>The default <em>mach.ini</em> places the object directory into the directory
<em>objdir</em> under the top source directory. It also builds an optimized
binary without debug info.</p>
<p>Run the following to see which config settings you can add to
<em>mach.ini</em>:</p>
<pre><code>./mach settings-create
./mach settings-list
</code></pre>
<p>This <em>may</em> fail because I'm still working out the kinks with <em>gettext</em>.
If it doesn't work, open <em>python/mozbuild-bs/mozbuild/base.py</em> and search
for <em>_register_settings</em>. Open
<em>python/mozbuild-bs/mozbuild/locale/en-US/LC_MESSAGES/mozbuild.po</em> for
the help messages.</p>
<p>As a point of reference, my <em>mach.ini</em> looks like the following:</p>
<pre><code>[build]
application = browser

configure_extra = --enable-dtrace --enable-tests

[compiler]
cc = /usr/local/llvm/bin/clang
cxx = /usr/local/llvm/bin/clang++

cflags = -fcolor-diagnostics
cxxflags = -fcolor-diagnostics

[paths]
source_directory = /Users/gps/src/mozilla-central-git
object_directory = /Users/gps/src/mozilla-central-git/objdir
</code></pre>
<p>I am on OS X and am using a locally-built version of LLVM/Clang, which I
have installed to <em>/usr/local/llvm</em>.</p>
<p>You'll notice there are no options to configure make. The patch
automatically selects optimal settings for your platform!</p>
<h2>Known Issues and Caveats</h2>
<p>This is alpha. It works in scenarios in which I have tested it, mainly
building the <em>browser</em> application on OS X and Linux. There are many
features missing and likely many bugs.</p>
<p>I have been using this as part of my day-to-day development for weeks.
However, your mileage may vary.</p>
<p>As stated above, Windows support is lacking. It will appear to work, but
things will blow up during building. Don't even try to use it on
Windows.</p>
<p>There are likely many bugs. Please don't file Bugzilla bugs, as this
isn't part of the build system just yet.</p>
<p><strong>This patch takes over the build system. Do not attempt to use
client.mk or run make directly with this patch applied.</strong></p>
<p>If you encounter an issue, your methods of recourse are:</p>
<ol>
<li>Post a comment on this blog post</li>
<li>Ping me on irc.mozilla.org. My nick is <em>gps</em>. Try the #buildfaster
   channel.</li>
<li>Send an email to gps@mozilla.com</li>
</ol>
<p>I am particularly interested in exceptions and build failures.</p>
<p>If you encounter an issue building with this, just reverse the patch and
build like you have always done (don't forget to restore your mozconfig
file).</p>
<p>If <em>mach.ini</em> does not support everything you were doing in your
mozconfig, please send me a copy of your mozconfig so I can implement
whatever you need.</p>
<h2>Other Info</h2>
<p>I will likely write a follow-up post detailing what's going on. If you
are curious, the code lives in <em>python/mozbuild-bs</em>. The <em>backend</em> and
<em>frontend</em> sub-packages are where the magic is at. Once the backend has
been configured, check out <em>hybridmake.mk</em> and all of the <em>splendid.mk</em>
files in the object directory.</p>
<p>I am particularly interested in the real-world impact of this patch on
people's build times. In this early version of the patch, you likely
won't see drastic speed increases. On my MacBook Pro with an SSD, I see
end-to-end clobber build times decrease by over a minute. With a little
more work, I should be able to shave another minute or two off of that.</p>
<p>I will try to keep the patch up-to-date as I improve the build system.
Refresh early and often.</p>]]></content:encoded>
    </item>
    <item>
      <title>mozilla-central Build Times</title>
      <link>http://gregoryszorc.com/blog/2012/07/29/mozilla-central-build-times</link>
      <pubDate>Sun, 29 Jul 2012 13:20:00 PDT</pubDate>
      <category><![CDATA[Mozilla]]></category>
      <category><![CDATA[build system]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2012/07/29/mozilla-central-build-times</guid>
      <description>mozilla-central Build Times</description>
      <content:encoded><![CDATA[<p>In my previous post, I
<a href="http://gregoryszorc.com/blog/2012/07/29/mozilla-build-system-overview/">explained how Mozilla's build system works</a>.
In this post, I want to give people a feel for where time is spent and to
identify obvious areas that need improvement.</p>
<p>To my knowledge, nobody has provided such a comprehensive collection of
measurements before. Thus, most of our thoughts on where build time goes
have been largely lacking scientific rigor. I hope this post changes
matters and injects some much-needed quantitative figures into the
discussion.</p>
<h2>Methodology</h2>
<p>All the times reported in this post were obtained from my 2011
generation MacBook Pro. It has 8 GB of RAM and a magnetic hard drive. It
is not an ultimate build rig by any stretch of the imagination. However,
it's no slouch either. The machine has 1 physical Core i7 processor with
4 cores, each clocked at 2.3GHz. Each core has hyperthreading, so to the
OS there appears to be 8 cores. For the remainder of this post, I'll
simply state that my machine has 8 cores.</p>
<p>When I obtained measurements, I tried to limit the number of processes
running elsewhere on the machine. I also performed multiple runs and
reported the best timings. This means that the results are likely
synthetic and highly influenced by the page cache. More on that later.</p>
<p>I configured make to use up to 8 parallel processes (adding -j8 to the
make flags). I also used silent builds (the -s flag to make). Silent
builds are important because terminal rendering can add many seconds of
wall time to builds, especially on slow terminals (like Windows). I
measured results with make output being written to the terminal. In
hindsight, I wish I hadn't done this. Next time.</p>
<p>To obtain the times, I used the ubiquitous <em>time</em> utility. Wall times
are the <em>real</em> times from <em>time</em>. CPU time is the sum of the <em>user</em> and
<em>sys</em> times.</p>
<p>CPU utilization is the percentage of CPU cores busy during the wall time
of execution. In other words, I divided the CPU time by 8 times the wall
time (8 for the number of cores in my machine). 100% is impossible to
obtain, as obviously the CPU on my machine is doing other things during
measurement. But, I tried to limit the number of background processes
running, so there shouldn't have been that much noise.</p>
<p>I built a debug version of Firefox (the <em>browser</em> app in
mozilla-central) using r160922 of the Clang compiler (pulled and built
the day I did this measurement). The revision of mozilla-central being
built was <a href="https://hg.mozilla.org/mozilla-central/rev/08428deb1e89">08428edb1e89</a>.
I also had <em>--enable-tests</em>, which adds a significant amount of extra
work to builds.</p>
<h2>Configure</h2>
<p><em>time</em> reported the following for running <em>configure</em>:</p>
<pre><code>real 0m25.927s
user 0m9.595s
sys  0m8.729s
</code></pre>
<p>This is a one-time cost. Unless you are hacking on the build system or
pull down a tree change that modified the build system, you typically
don't need to worry about this.</p>
<h2>Clobber Builds with Empty ccache</h2>
<p>I built each tier separately with an empty ccache on a
recently-configured object directory. This measures the optimal worst
case time for building mozilla-central. In other words, we have nothing
cached in the object directory, so the maximum amount of work needs to
be performed. Since I measured multiple times and used the best results,
this is what I mean by <em>optimal</em>.</p>
<p>The table below contains the measurements. I omitted CPU utilization
calculation for small time values because I don't feel it is relevant.</p>
<table border="1">
  <tr>
    <th>Tier - Sub-tier</th>
    <th>Wall Time (s)</th>
    <th>CPU Time (s)</th>
    <th>CPU Utilization</th>
  </tr>
  <tr>
    <td>base export</td>
    <td>0.714</td>
    <td>0.774</td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>base libs</td>
    <td>5.081</td>
    <td>8.620</td>
    <td>21%</td>
  </tr>
  <tr>
    <td>base tools</td>
    <td>0.453</td>
    <td>0.444</td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>base (total)</td>
    <td>6.248</td>
    <td>9.838</td>
    <td>19.7%</td>
  </tr>
  <tr>
    <td>nspr</td>
    <td>9.309</td>
    <td>8.404</td>
    <td>11.3%</td>
  </tr>
  <tr>
    <td>js export</td>
    <td>1.030</td>
    <td>1.877</td>
    <td>22.8%</td>
  </tr>
  <tr>
    <td>js libs</td>
    <td>71.450</td>
    <td>416.718</td>
    <td>52%</td>
  </tr>
  <tr>
    <td>js tools</td>
    <td>0.324</td>
    <td>0.246</td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>js (total)</td>
    <td>72.804</td>
    <td>418.841</td>
    <td>71.9%</td>
  </tr>
  <tr>
    <td>platform export</td>
    <td>40.487</td>
    <td>141.704</td>
    <td>43.7%</td>
  </tr>
  <tr>
    <td>platform libs</td>
    <td>1211</td>
    <td>4896</td>
    <td>50.5%</td>
  </tr>
  <tr>
    <td>platform tools</td>
    <td>70.416</td>
    <td>90.917</td>
    <td>16.1%</td>
  </tr>
  <tr>
    <td>platform (total)</td>
    <td>1312</td>
    <td>5129</td>
    <td>48.9%</td>
  </tr>
  <tr>
    <td>app export</td>
    <td>4.383</td>
    <td>3.059</td>
    <td>8.7%</td>
  </tr>
  <tr>
    <td>app libs</td>
    <td>18.727</td>
    <td>18.976</td>
    <td>12.7%</td>
  </tr>
  <tr>
    <td>app tools (no-op)</td>
    <td>0.519s</td>
    <td>0.968</td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>app (total)</td>
    <td>23.629</td>
    <td>23.003</td>
    <td>12.2%</td>
  </tr>
  <tr>
    <td>Total</td>
    <td>1424 (23:44)</td>
    <td>5589 (93:15)</td>
    <td>49.1%</td>
  </tr>
</table>

<p>It's worth mentioning that linking libxul is part of the platform libs
tier. libxul linking should be called out because it is unlike other
parts of the build in that it is more I/O bound and can't use multiple
cores. On my machine, libxul linking (not using gold) takes ~61s. During
this time, only 1 CPU core is in use. The ~61s wall time corresponds to
roughly 5% of platform libs' wall time. Yet, even if we subtract this ~61s
from the effective CPU calculation, the percentage doesn't change much.</p>
<h2>Clobber with Populated ccache</h2>
<p>Using the ccache from a recently built tree to make C/C++ compilation
faster, I measured how long it took each tier to build on a clobber
build.</p>
<p>This measurement can be used to estimate the overhead of C/C++ compilation
during builds. In theory, the difference between CPU times between this
and the former measurement will be the amount of CPU time spent in the
C/C++ compiler.</p>
<p>This will also isolate how much time we spend <em>not</em> in the C/C++
compiler. It will arguably be very difficult to
make the C/C++ compiler faster (although things like reducing the abuse
of templates can have a measureable impact). However, we do have control
over many of the other things we do. If we find that CPU time spent
outside the C/C++ compiler is large, we can look for pieces to optimize.</p>
<p>Tiers not containing compiled files are omitted from the data.</p>
<table border="1">
  <tr>
    <th>Tier - Sub-tier</th>
    <th>Wall Time (s)</th>
    <th>CPU Time (s)</th>
    <th>ccache savings (s) (Time in Compiler)</th>
  </tr>
  <tr>
    <td>base libs</td>
    <td>1.075</td>
    <td>1.525</td>
    <td>7.095</td>
  </tr>
  <tr>
    <td>base tools</td>
    <td>1.957</td>
    <td>0.933</td>
    <td>1.522</td>
  </tr>
  <tr>
    <td>nspr</td>
    <td>5.582</td>
    <td>1.688</td>
    <td>6.716</td>
  </tr>
  <tr>
    <td>js libs</td>
    <td>22.829</td>
    <td>9.529</td>
    <td>407.189</td>
  </tr>
  <tr>
    <td>platform libs</td>
    <td>431</td>
    <td>328</td>
    <td>4568</td>
  </tr>
  <tr>
    <td>platform tools</td>
    <td>14.498</td>
    <td>25.744</td>
    <td>65.173</td>
  </tr>
  <tr>
    <td>app libs</td>
    <td>10.193</td>
    <td>15.809</td>
    <td>3.167</td>
  </tr>
  <tr>
    <td>Total</td>
    <td>487.134 (6:07)</td>
    <td>383.229 (6:23)</td>
    <td>5059 (84:19)</td>
  </tr>
</table>

<h2>No-op Build</h2>
<p>A <em>no-op</em> build is a build performed in an object directory that was
just built. Nothing changed in the source repository nor object
directory, so theoretically the build should do nothing. And, it should
be fast.</p>
<p>In reality, our build system isn't smart and performs some redundant
work. One part of redundant work is because one of the first things the
main Makefile does before invoking the tiers is delete a large chunk of
the <em>dist/</em> directory and the entirety of the <em>_tests/</em> directory from
the object directory.</p>
<p>In these measurements, I bypassed the deletion of these directories. In
other words, I measure what <em>no-op</em> builds are if we eliminate the
clown shoes around blowing away large parts of the object directory.</p>
<table border="1">
  <tr>
    <th>Tier - Sub-tier</th>
    <th>Wall Time (s)</th>
    <th>CPU Time (s)</th>
  </tr>
  <tr>
    <td>base export</td>
    <td>0.524</td>
    <td>0.537</td>
  </tr>
  <tr>
    <td>base libs</td>
    <td>0.625</td>
    <td>0.599</td>
  </tr>
  <tr>
    <td>base tools</td>
    <td>0.447</td>
    <td>0.437</td>
  </tr>
  <tr>
    <td>nspr</td>
    <td>0.809</td>
    <td>0.752</td>
  </tr>
  <tr>
    <td>js export</td>
    <td>0.334</td>
    <td>0.324</td>
  </tr>
  <tr>
    <td>js libs</td>
    <td>0.375</td>
    <td>0.361</td>
  </tr>
  <tr>
    <td>platform export</td>
    <td>10.904</td>
    <td>13.136</td>
  </tr>
  <tr>
    <td>platform libs</td>
    <td>30.969</td>
    <td>44.25</td>
  </tr>
  <tr>
    <td>platform tools</td>
    <td>8.213</td>
    <td>10.737</td>
  </tr>
  <tr>
    <td>app export</td>
    <td>0.524</td>
    <td>1.006</td>
  </tr>
  <tr>
    <td>app libs</td>
    <td>6.090</td>
    <td>13.753</td>
  </tr>
  <tr>
    <td>Total</td>
    <td>59.814</td>
    <td>85.892</td>
  </tr>
</table>

<p>So, no-op builds use ~60s of wall time and only make use of 17.9% of
available CPU resources.</p>
<h2>No-op Build With Directory Removal Silliness</h2>
<p>As mentioned above, before the tiers are iterated, the top-level
Makefile blows away large parts of <em>dist/</em> and the entirety of
<em>_tests/</em>. What impact does this have?</p>
<p>In this section, I try to isolate how much time was thrown away by doing
this.</p>
<p>First, we have to account for the deletion of these directories. On my
test build, deleting 15,005 files in these directories took ~3 seconds.</p>
<p>The table below contains my results. This is a more accurate reading
than the above on how long no-op builds takes because this is actually
what we do during normal builds. The time delta column contains the
difference between this build and a build without the removal silliness.
Positive times can be attributes to overhead associated with
repopulating <em>dist/</em> and <em>_tests/</em>.</p>
<table border="1">
  <tr>
    <th>Tier - Sub-tier</th>
    <th>Wall Time (s)</th>
    <th>Wall Time Delta (s)</th>
    <th>CPU Time (s)</th>
    <th>CPU Time Delta (s)</th>
  </tr>
  <tr>
    <td>base export</td>
    <td>0.544</td>
    <td>Negligible</td>
    <td>0.559</td>
    <td>Negligible</td>
  </tr>
  <tr>
    <td>base libs</td>
    <td>0.616</td>
    <td>Negligible</td>
    <td>0.594</td>
    <td>Negligible</td>
  </tr>
  <tr>
    <td>base tools</td>
    <td>0.447</td>
    <td>Negligible</td>
    <td>0.436</td>
    <td>Negligible</td>
  </tr>
  <tr>
    <td>nspr</td>
    <td>0.803</td>
    <td>Negligible</td>
    <td>0.743</td>
    <td>Negligible</td>
  </tr>
  <tr>
    <td>js export</td>
    <td>0.338</td>
    <td>Negligible</td>
    <td>0.329</td>
    <td>Negligible</td>
  </tr>
  <tr>
    <td>js libs</td>
    <td>0.378</td>
    <td>Negligible</td>
    <td>0.363</td>
    <td>Negligible</td>
  </tr>
  <tr>
    <td>platform export</td>
    <td>13.140</td>
    <td>2.236</td>
    <td>13.314</td>
    <td>Negligible</td>
  </tr>
  <tr>
    <td>platform libs</td>
    <td>35.290</td>
    <td>4.329</td>
    <td>43.059</td>
    <td>-1.191 (normal variance?)</td>
  </tr>
  <tr>
    <td>platform tools</td>
    <td>8.819</td>
    <td>0.606</td>
    <td>10.983</td>
    <td>0.246</td>
  </tr>
  <tr>
    <td>app export</td>
    <td>0.525</td>
    <td>Negligible</td>
    <td>1.012</td>
    <td>Negligible</td>
  </tr>
  <tr>
    <td>app libs</td>
    <td>8.876</td>
    <td>2.786</td>
    <td>13.527</td>
    <td>-0.226</td>
  </tr>
  <tr>
    <td>Total</td>
    <td>69.776</td>
    <td>9.962</td>
    <td>84.919</td>
    <td>-0.973 (normal variance)</td>
  </tr>
</table>

<p>If a delta is listed as negligible, it was within 100ms of the original
value and I figured this was either due to expected variance between
runs or below our threshold for caring. In the case of base, nspr, and
js tiers, the delta was actually much smaller than 100ms, often less
then 10ms.</p>
<p>It certainly appears that the penalty for deleting large parts of
<em>dist/</em> and the entirety of <em>_tests/</em> is about 10 seconds.</p>
<h2>The Overhead of Make</h2>
<p>We've measured supposed no-op build times. As I stated above, our no-op
builds actually aren't no-op builds. Even if we bypass the deletion of
<em>dist/</em> and <em>_tests/</em> we always evaluate some make rules. Can we measure how
much work it takes to just load the make files without actually doing
anything? This would allow us to get a rough estimate of how much we are
wasting by doing redundant work. It will also help us establish a
baseline for make overhead.</p>
<p>Turns out we can! Make has a <em>--dry-run</em> argument which evaluates the make
file but doesn't actually do anything. It simply prints what would have
been done.</p>
<p>Using <em>--dry-run</em>, I timed the different tiers. The difference from a no-op
build should roughly be the overhead associated with make itself. It is
possible that <em>--dry-run</em> adds a little overhead because it prints the
commands that would have been executed. (Previous timings were using
<em>-s</em>, which suppresses this.)</p>
<p>The delta times in the following table are the difference in times
between the true no-op build from above (the one where we don't delete
<em>dist/</em> and <em>_tests/</em>) and the times measured here. It roughly isolates
the amount of time spent outside of make, doing redundant work.</p>
<table border="1">
  <tr>
    <th>Tier - Sub-tier</th>
    <th>Wall Time (s)</th>
    <th>Wall Time Delta (s)</th>
    <th>CPU Time (s)</th>
    <th>CPU Time Delta (s)</th>
  </tr>
  <tr>
    <td>base export</td>
    <td>0.369</td>
    <td>0.155</td>
    <td>0.365</td>
    <td>0.172</td>
  </tr>
  <tr>
    <td>base libs</td>
    <td>0.441</td>
    <td>0.184</td>
    <td>0.431</td>
    <td>0.168</td>
  </tr>
  <tr>
    <td>base tools</td>
    <td>0.368</td>
    <td>0.079</td>
    <td>0.364</td>
    <td>0.073</td>
  </tr>
  <tr>
    <td>nspr</td>
    <td>0.636</td>
    <td>0.173</td>
    <td>0.591</td>
    <td>0.161</td>
  </tr>
  <tr>
    <td>js export</td>
    <td>0.225</td>
    <td>0.109</td>
    <td>0.225</td>
    <td>0.099</td>
  </tr>
  <tr>
    <td>js libs</td>
    <td>0.278</td>
    <td>0.097</td>
    <td>0.273</td>
    <td>0.088</td>
  </tr>
  <tr>
    <td>platform export</td>
    <td>3.841</td>
    <td>7.063</td>
    <td>6.108</td>
    <td>7.028</td>
  </tr>
  <tr>
    <td>platform libs</td>
    <td>8.938</td>
    <td>22.031</td>
    <td>14.723</td>
    <td>29.527</td>
  </tr>
  <tr>
    <td>platform tools</td>
    <td>3.962</td>
    <td>4.251</td>
    <td>6.185</td>
    <td>4.552</td>
  </tr>
  <tr>
    <td>app export</td>
    <td>0.422</td>
    <td>0.102</td>
    <td>0.865</td>
    <td>0.141</td>
  </tr>
  <tr>
    <td>app libs</td>
    <td>0.536</td>
    <td>5.554</td>
    <td>1.148</td>
    <td>12.605</td>
  </tr>
  <tr>
    <td>Total</td>
    <td>20.016</td>
    <td>39.798</td>
    <td>31.278</td>
    <td>54.614</td>
  </tr>
</table>

<h2>Observations</h2>
<p>The numbers say a lot. I'll get to key takeaways in a bit.</p>
<p>First, what the numbers don't show is the variance between runs.
Subsequent runs are almost always <em>significantly</em> faster than the
initial, even on no-op builds. I suspect this is due mostly to
I/O wait. In the initial tier run, files are loaded into the page cache.
Then, in subsequent runs, all I/O comes from physical memory rather than
waiting on a magnetic hard drive.</p>
<p>Because of the suspected I/O related variance, I fear that the numbers I
obtained are highly synthetic, at least for my machine. It is unlikely
I'll ever see all these numbers in one mozilla-central build. Instead,
it requires a specific sequence of events to obtain the best times
possible. And, this sequence of events is not likely to correspond with
real-world usage.</p>
<p>That being said, I think these numbers are important. If you remove I/O
from the equation - say you have an SSD with near 0 service times or
have enough memory so you don't need a hard drive - these numbers will
tell what limits you are brushing up against. And, as computers get more
powerful, I think more and more people will cross this threshold and
will be more limited by the build system than the capabilities of their
hardware. (A few months ago, I
<a href="https://groups.google.com/forum/#!topic/mozilla.dev.builds/FJclsTA_OBQ/discussion">measured resource usage</a>
when compiling mozilla-central on Linux and concluded you need roughly
9GB of dedicated memory to compile and link mozilla-central without page
cache eviction. In other words, if building on a machine with only 8GB
of RAM, your hard drive will play a role.)</p>
<p>Anyway, to the numbers.</p>
<p>I think the most important number in the above tables is <strong>49.1%</strong>. That
is the effective CPU utilization during a clobber build. This means that
<strong>during a build, on average half of the available CPU cores are
unused.</strong> Now, I could be generous and bump this number to 50.7%. That's
what the effective CPU utilization is if you remove the ~60s of libxul
linking from the calculation.</p>
<p>The 49.1% has me reaching the following conclusions:</p>
<ol>
<li>I/O wait really matters.</li>
<li>Our recursive use of make is incapable of executing more than 4 items
   at a time on average (assuming 8 cores).</li>
<li>My test machine had more CPU wait than I think.</li>
</ol>
<p>I/O wait is easy to prove: compare times on an SSD or with a similar I/O
bus with near zero service times (e.g. a filled page cache with no
eviction - like a machine with 16+ GB of memory that has built
mozilla-central recently).</p>
<p>A derived time not captured in any table is 11:39. This is the total
CPU time of a clobber build (93:15) divided by the number of cores (8).
<strong>If we had 100% CPU utilization across all cores during builds, we should
be able to build mozilla-central in 11:39.</strong> This is an ideal figure and
won't be reached. As mentioned above, libxul linking takes ~60s itself! I
think 13:00 is a more realistic optimal compilation time for a modern 8
core machine. This points out a startling number: <strong>we are wasting
~12 minutes of wall time due to not fully utilizing CPU cores during
clobber builds.</strong></p>
<p>Another important number is 5059 out of 5589, or 90.5%. That is the CPU
time in a clobber build spent in the C/C++ compiler, as measured by the
speedup of using ccache. It's unlikely we are going to make the C/C++
compiler go much faster (short of not compiling things). So, this is a
big fat block of time we will never be able to optimize. On my machine
<strong>compiling mozilla-central will always take at least ~10:30 wall time,
just in compiling C/C++.</strong></p>
<p>A clobber build with a saturated ccache took 487s wall time but only 383s
CPU time. That's only about 10% total CPU utilization. And, this
represents only 6.8% of total CPU time from the original clobber build.
Although, it is 34.2% of total wall time.</p>
<p>The above means that everything not the C/C++ compiler is horribly
inefficient. These are clown shoes of epic proportions. We're not even
using 1 full core doing build actions outside of the C/C++ compiler!</p>
<p>Because we are inefficient when it comes to core usage, I
think a key takeaway is that throwing more cores at the existing build
system will have diminishing returns. Sure, some parts of the build system
today could benefit from it (mainly js, layout, and dom, as they have
Makefile's with large numbers of source files). But, most of the build
system won't take advantage of many cores. <strong>If you want to throw money
at a build machine, I think your first choice should be an SSD. If you
can't do that, have as much memory as you can so most of your filesystem
I/O is serviced by the page cache, not your disk drive.</strong></p>
<p>In the final table, we isolated how much time <em>make</em> is spending to
just to figure out what to do. That amounts to ~20 seconds wall
time and ~31s CPU time. That leaves ~40s wall and ~55s CPU for non-make
work during no-op builds. Translation: we are doing 40s of wall time work
during no-op builds. Nothing changed. <strong>We are throwing 40s of wall time
away because the build system isn't using proper dependencies and is
doing redundant work.</strong></p>
<p>I've long been a critic of us blowing away parts of <em>dist/</em> and
<em>_tests/</em> at the top of builds. Well, after measuring it, I have mixed
reactions. It only amounts to about ~10s of added time to builds. This
doesn't seem like a lot in the grand scheme of things. However, this is
~10s on top of the ~60s it actually takes to iterate through the tiers.
So, in terms of percentages for no-op builds, it is actually quite
significant.</p>
<p>No-op builds with the existing build system take ~70s under ideal
conditions. In order of time, the breakdown is roughly:</p>
<ul>
<li>~40s for doing redundant work in Makefiles</li>
<li>~20s for make traversal and loading overhead</li>
<li>~10s for repopulating deleted content from <em>dist/</em> and <em>_tests/</em></li>
</ul>
<p>In other words, <strong>~50s of ~70s no-op build times are spent doing work
we have already done.</strong> This is almost purely clown shoes. Assuming we
can't make make traversal and loading faster, the shortest possible
no-op build time will be ~20s.</p>
<p>Splitting things up a bit more:</p>
<ul>
<li>~22s - platform libs make evaluation</li>
<li>~20s - make file traversal and loading (readying for evaluation)</li>
<li>~10s - repopulating deleted content from <em>dist/</em> and <em>_tests/</em></li>
<li>~7s - platform export make evaluation</li>
<li>~5.5 - app libs make evaluation</li>
<li>~4s - platform tools</li>
</ul>
<p>The ~20s for make file traversal and loading is interesting. I suspect
(although I haven't yet measured) that a lot of this is due to the sheer
size of rules.mk. As I
<a href="http://gregoryszorc.com/blog/2012/07/28/makefile-execution-times/">measured</a>
on Friday, the overhead of rules.mk with pymake is significant. I
hypothesized that it would have a similar impact on GNU make. I think a
good amount of this ~20s is similar overhead. I need to isolate,
however. I am tempted to say that if we truly did no-op builds and make
Makefile's load into make faster, we could attain no-op build times in
the ~10s range. I think this is pretty damn good! Even ~20s isn't too
bad. As surprising as it is for me to say it, <strong>recursive make is not
(a significant) part of our no-op build problem</strong>.</p>
<h2>Why is the Build System Slow?</h2>
<p>People often ask the question above. As the data has told me, the
answer, like many to complicated problems, is nuanced.</p>
<p>If you are doing a clobber build on a fresh machine, the build system is
slow because 1) compiling all the C/C++ takes a lot of time (84:19 CPU
time actually) 2) we don't make efficient use of all available cores
when building. Half of the CPU horsepower during a fresh build is
unharnessed.</p>
<p>If you are doing a no-op build, the build system is slow mainly because
it is performing a lot of needless and redundant work. A significant
contributor is the overhead of make, probably due to rules.mk being
large.</p>
<p>If you are doing an incremental build, you will fall somewhere between
either extreme. You will likely get nipped by both inefficient core
usage as well as redundant work. Which one hurts the most depends on the
scope of the incremental change.</p>
<p>If you are building on a machine with a magnetic hard drive (not an
SSD), your builds are slow because you are waiting on I/O. You can
combat this by putting 8+GB of memory in your system and doing your best
to ensure that building mozilla-central can use as much of it as
possible. I highly recommend 12GB, if not 16GB.</p>
<h2>Follow-ups</h2>
<p>The measurements reported in this post are only the tip of the iceberg.
If I had infinite time, I would:</p>
<ul>
<li>Measure other applications, not just browser/Firefox. I've heard that
  mobile/Fennec's build config is far from optimal, for example. I would
  love to quantify that.</li>
<li>Set up buildbot to record and post measurements so we have a dashboard
  of build times. We have some of this today, but the granularity isn't
  as fine as what I captured.</li>
<li>Record per-directory times.</li>
<li>Isolate time spent in different processes (DTrace could be used here).</li>
<li>Capture I/O numbers.</li>
<li>Correlate the impact of I/O service times on build times.</li>
<li>Isolate the overhead of ccache (mainly in terms of I/O).</li>
<li>Obtain numbers on other platforms and systems. Ensure results can be
  reproduced.</li>
</ul>
<h2>Next Steps</h2>
<p>If we want to make our existing recursive make build backend faster, I
recommend the following actions (in no particular order):</p>
<ol>
<li>Factor pieces out of rules.mk into separate .mk files and
   conditionally load based on presence of specific variables. In other
   words, finish what we have started. This definitely cuts down on the
   overhead with pymake (as measured on Friday) and <em>likely</em> makes GNU
   make faster as well.</li>
<li>Don't blow away parts of <em>dist/</em> and <em>_tests/</em> at the top of builds.
   I know this introduces a problem where we could leave orphaned files
   in the object directory. We should solve this problem by having
   proper manifests for everything so we can detect and delete orphans.
   The cheap man's solution is to periodically clobber these
   directories.</li>
<li>Don't perform unnecessary work during no-op builds. I suspect a lot
   of redundant work is due to rules in Makefile's not the rules in
   rules.mk. As we eliminate rules from Makefile's, this problem should
   gradually go away since rules.mk is generally intelligent about these
   things.</li>
<li>More parallelism. I'm not sure how we're going to solve this with
   recursive make short of using PARALLEL_DIRS more and/or consolidating
   Makefile's together.</li>
</ol>
<p>Again, these steps apply to our current recursive make build backend.</p>
<p>Because the most significant losses are due to ungained parallelism, <strong>our
focus should be on increasing parallelism.</strong> We can only do this so much
with recursive make. It is clear now more than ever that recursive make
needs to be replaced with something that can fully realize the potential
of multiple CPU cores. That could be non-recursive make or a separate
build backend altogether.</p>
<p>We will likely not have an official alternate build backend soon. Until
then, there are no shortage of clown shoes that can be looked at.</p>
<p>The redundant work during no-op builds is definitely tempting to
address, as I think that has significant impact to most developers.
Eliminating the absurdly long no-op build times removes the needs for
hacks like <em>smart-make</em> and instills a culture of <em>trust the build
system.</em></p>
<p>I suspect a lot of the redundant work during no-op builds is due to
poorly implemented rules in individual Makefiles rather than on
silliness in rules.mk. Therefore,
<a href="https://bugzilla.mozilla.org/show_bug.cgi?id=769378">removing rules from Makefile's</a>
again seems to be one of the most important things we can do to make the
build system faster. It also prepares us for implementing newer build
backends, so it is a win-win!</p>]]></content:encoded>
    </item>
    <item>
      <title>Mozilla Build System Overview</title>
      <link>http://gregoryszorc.com/blog/2012/07/29/mozilla-build-system-overview</link>
      <pubDate>Sun, 29 Jul 2012 13:15:00 PDT</pubDate>
      <category><![CDATA[Mozilla]]></category>
      <category><![CDATA[build system]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2012/07/29/mozilla-build-system-overview</guid>
      <description>Mozilla Build System Overview</description>
      <content:encoded><![CDATA[<p>Mozilla's build system is a black box to many. This post attempts to
shed some light onto how it works.</p>
<h2>Configuration File</h2>
<p>The first part of building is creating a configuration file. This
defines what application to build (Firefox, Firefox OS, Fennec, etc) as
well as build options, like to create a release or debug build. This
step isn't technically required, but most people do it.</p>
<p>Configuration files currently exist as <em>mozconfig</em> files. By default,
most people create a <em>.mozconfig</em> file in the root directory of
mozilla-central.</p>
<h2>Interaction</h2>
<p>All interaction with the build system is currently gated through the
<em>client.mk</em> file in the root directory of mozilla-central. Although, I'm
trying to land an alternate (and eventual replacement) to <em>client.mk</em>
called <em>mach</em>. You can read about it in previous posts on this blog.</p>
<p>When you run <em>make -f client.mk</em>, you are invoking the build system and
telling it to do whatever it needs to do build the tree.</p>
<h2>Running Configure</h2>
<p>The first thing <em>client.mk</em> does to a fresh tree is invoke <em>configure</em>.
<em>configure</em> is a shell script in the root directory of the repository.
It is generated from the checked-in <em>configure.in</em> file using the GNU
<em>autoconf</em> utility. I won't go into detail on how autoconf works because
I don't have a beard.</p>
<p><em>configure</em> accomplishes some important tasks.</p>
<p>First, it validates that the build environment is sane. It performs some
sanity testing on the directory tree then looks at the system and build
configuration to make sure everything should work.</p>
<p>It identifies the active compiler, locations of common tools and
utilities, and ensures everything works as needed. It figures out how to
convert desired traits into system-specific options. e.g. the exact
argument to pass to the compiler to enable warnings.</p>
<p>Once <em>configure</em> determines the environment is sane, it writes out what
it learned.</p>
<p>Currently, <em>configure</em> takes what it has learned and invokes the
<em>allmakefiles.sh</em> script in the root directory. This script prints out
the set of Makefile's that will be used to build the tree for the
current configuration. <em>configure</em> takes the output of filenames and
then procedes to generate those files.</p>
<p>Generation of Makefile's is rather simple. In the source tree are a
bunch of <em>.in</em> files, typically <em>Makefile.in</em>. These contain special
markers. <em>configure</em> takes the set of determined configuration variables
and performs substitution of the variable markers in the <em>.in</em> files with
them. The <em>.in</em> files with variables substitutes are written out in the
object directory. There are also some GYP files in the source tree.
<em>configure</em> invokes a tool to convert these into Mozilla-style
Makefile's.</p>
<p><em>configure</em> also invokes <em>configure</em> for other managed projects
in mozilla-central, such as the SpiderMonkey source in <em>js/src</em>.</p>
<p><em>configure</em> finishes by writing out other miscellaneous files in the
object directory.</p>
<h2>Running Make</h2>
<p>The next step of the build is running make. <em>client.mk</em> simply points
GNU make (or pymake) at the <em>Makefile</em> in the top-level directory of the
object directory and essentially says <em>evaluate</em>.</p>
<h3>Build System Tiers</h3>
<p>The build system is broken up into different tiers. Each tier represents
a major phase or product in the build system. Most builds have the
following tiers:</p>
<ol>
<li>base - Builds global dependencies</li>
<li>nspr - Builds NSPR</li>
<li>js - Builds SpiderMonkey</li>
<li>platform - Builds the Gecko platform</li>
<li>app - Builds the configured application (e.g. Firefox, Fennec,
   Firefox OS)</li>
</ol>
<p>Inside each tier are the distinct sub-tiers:</p>
<ol>
<li>export</li>
<li>libs</li>
<li>tools</li>
</ol>
<p>A Makefile generally belongs to 1 main tier. Inside Makefile's or in
other included .mk files (make files that are not typically called
directly by make) are statements which define which directories
belong to which tiers. See
<a href="https://mxr.mozilla.org/mozilla-central/source/toolkit/toolkit-tiers.mk">toolkit-tiers.mk</a>
for an example.</p>
<p>When the top-level Makefile is invoked, it iterates through every tier
and every sub-tier within it. It starts at the first tier and evaluates
the <em>export</em> target on every Makefile/directory defined in it. It then
moves on to the <em>libs</em> target then finally the <em>tools</em> target. When it's
done with the <em>tools</em> target, it moves on to the next tier and does the
same iteration.</p>
<p>For example, we first start by evaluating the <em>export</em> target of the
<em>base</em> tier. Then we evaluate <em>base</em>'s <em>libs</em> and <em>tools</em> tiers. We then
move on to <em>nspr</em> and do the same. And, we keep going. In other words,
the build system makes 3 passes through each tier.</p>
<p>Tiers are composed of directory members. e.g. <em>dom</em> or <em>layout</em>. When
make descends into a tier member directory, it looks for specially named
variables that tell it what sub-directories are also part of this
directory. The <em>DIRS</em> variable is the most common. But, we also use
<em>TEST_DIRS</em>, <em>PARALLEL_DIRS</em>, <em>TOOL_DIRS</em>, and a few others. make will
invoke make for all defined child directories and for the children of
the children, and so on. This is what we mean by <em>recursive make</em>. make
essentially recurses into directory trees, evaluating all the
directories linearly.</p>
<p>Getting back to the tiers, the sub-tiers <em>export</em>, <em>libs</em>, and <em>tools</em>
can be thought of as <em>pre-build</em>, <em>build</em>, and <em>post-build</em> events.
Although, this analogy is far from perfect.</p>
<p><em>export</em> generally prepares the object directory for more comprehensive
building. It copies C/C++ header files into a unified object directory,
generates header files from IDLs files, etc.</p>
<p><em>libs</em> does most of the work. It compiles C++ code and performs lots of
other main work, such as Jar manifest creation.</p>
<p><em>tools</em> does a lot of miscellaneous work. If you have tests enabled,
this is where tests are typically compiled and/or installed, for
example.</p>
<h3>Processing a Makefile</h3>
<p>For each directory inside a tier, make evaluates the Makefile in that
directory for the target/sub-tier specified.</p>
<p>The basic gist of Makefile execution is actually pretty simple.</p>
<p>Mozilla's Makefiles typically look like:</p>
<pre><code>DEPTH := .
topsrcdir := @top_srcdir@
srcdir := @srcdir@
VPATH := @srcdir@

include $(DEPTH)/config/autoconf.mk

IDLSRCS := foo.idl bar.idl
CPPSRCS := hello.cpp world.cpp

include $(topsrcdir)/config/rules.mk
</code></pre>
<p>All the magic in Makefile processing happens in <em>rules.mk</em>. This make file
simply looks for specially named variables (like <em>IDLSRCS</em> or <em>CPPSRCS</em>)
and magically converts them into targets for make to evaluate.</p>
<p>In the above sample Makefile, the <em>IDLSRCS</em> variable will result in an
implicit <em>export</em> target which copies IDLs into the object directory and
compiles them to .h files. <em>CPPSRCS</em> will result in a <em>libs</em> target that
results in each .cpp file being compiled into a .o file.</p>
<p>Of course, there is nothing stopping you from defining targets/rules in
Makefile's themselves. This practice is actually quite widespread.
Unfortunately, it is a bad practice, so you shouldn't do it. The
preferred behavior is to define variables in a Makefile and have
rules.mk magically provide the make targets/rules to do stuff with them.
<a href="https://bugzilla.mozilla.org/show_bug.cgi?id=769378">Bug 769378</a> tracks
fixing this bad practice.</p>
<h2>Conclusion</h2>
<p>So, there you have it: a very brief overview of how Mozilla's build
system works!</p>
<p>In my next post, I will shed some light onto how much times goes into
different parts of the build system.</p>]]></content:encoded>
    </item>
  </channel>
</rss>

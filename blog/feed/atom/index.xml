<?xml version="1.0" encoding="UTF-8"?>
<feed
  xmlns="http://www.w3.org/2005/Atom"
  xmlns:thr="http://purl.org/syndication/thread/1.0"
  xml:lang="en"
   >
  <title type="text">Gregory Szorc's Digital Home</title>
  <subtitle type="text">Rambling on</subtitle>

  <updated>2024-03-19T02:19:52Z</updated>
  <generator uri="http://blogofile.com/">Blogofile</generator>

  <link rel="alternate" type="text/html" href="http://gregoryszorc.com/blog" />
  <id>http://gregoryszorc.com/blog/feed/atom/</id>
  <link rel="self" type="application/atom+xml" href="http://gregoryszorc.com/blog/feed/atom/" />
  <entry>
    <author>
      <name>Gregory Szorc</name>
      <uri>http://gregoryszorc.com/blog</uri>
    </author>
    <title type="html"><![CDATA[My Shifting Open Source Priorities]]></title>
    <link rel="alternate" type="text/html" href="http://gregoryszorc.com/blog/2024/03/17/my-shifting-open-source-priorities" />
    <id>http://gregoryszorc.com/blog/2024/03/17/my-shifting-open-source-priorities</id>
    <updated>2024-03-17T21:00:00Z</updated>
    <published>2024-03-17T21:00:00Z</published>
    <category scheme="http://gregoryszorc.com/blog" term="Personal" />
    <category scheme="http://gregoryszorc.com/blog" term="PyOxidizer" />
    <summary type="html"><![CDATA[My Shifting Open Source Priorities]]></summary>
    <content type="html" xml:base="http://gregoryszorc.com/blog/2024/03/17/my-shifting-open-source-priorities"><![CDATA[<p>I'm a maintainer of a handful of open source projects, some of which have
millions of downloads and/or are used in important workloads, including in
production.</p>
<p>I have a full time job as a software engineer and my open source work is
effectively a side job. (Albeit one I try very hard to not let intersect
with my day job.)</p>
<p>Historically, my biggest contributions to my open source projects have come
when I'm not working full time:</p>
<ul>
<li><a href="https://github.com/indygreg/python-zstandard">python-zstandard</a> was started
  when I was on medical leave, recovering from a surgery.</li>
<li><a href="https://github.com/indygreg/python-build-standalone">python-build-standalone</a>
  and <a href="https://github.com/indygreg/PyOxidizer">PyOxidizer</a> were mainly built
  when I was between jobs, after leaving Mozilla.</li>
<li><a href="https://github.com/indygreg/apple-platform-rs/tree/main/apple-codesign">apple-codesign</a>
  was built in the height of COVID when I took a voluntary leave of absence
  from work to reconstitute my mental and physical health.</li>
</ul>
<p>When working full time, my time to contribute to open source has been carved out
of weekday nights and weekends, especially in the winter months. I believe
that code is an art form and programming a form of creative expression. My open
source contributions provide a relaxing avenue for me to express my artistic
creativity, when able.</p>
<p>My open source contributions reflect my personal priorities of where and what
to spend my free time on.</p>
<p>The only constant in life is change.</p>
<p>In the middle of 2022, I switched job roles and found myself reinvigorated by my
new role - Infrastructure Performance - which is at the intersection of some of
my strongest technical and professional skills. I found myself willingly
pouring more energy and time into my day job. That had the side effect of
reducing my open source contributions.</p>
<p>In 2023Q1 I got married. In the months leading up to and after, I chose to
prioritize spending time with my now wife and all the time commitments that
entails. This also reduced the amount of time available for open source
contributions.</p>
<p>In 2023Q4 I became a father to a beautiful baby girl. While on my employer's
generous-for-the-United-States fourteen week paternity leave, I somehow found
some time to contribute to open source. As refreshing as that was, it didn't
last. My <em>man cave</em> where my desktop computer resides has been converted into
a nursery. And for the past few months it has been occupied by my mother-in-law,
who has been generously effectively serving as a live-in nanny. Even when I'm
able to sit down at my desktop, it's hard to get into a state of flow due to
the added entropy from the additional three people now living with me.</p>
<p>After realizing the new normal in 2024Q1, I purchased a <a href="https://www.wahoofitness.com/devices/indoor-cycling/bike-trainers/kickr-move-buy">Wahoo KICKR MOVE</a>
bicycle trainer and now spend considerable time doing virtual bicycle rides on
<a href="https://www.zwift.com/">Zwift</a> because its one of the few <em>leisure</em> activities
I can do at home without drawing scrutiny from my wife and mother-in-law (but
98% my mother-in-law because I've observed that my wife is effectively
infallible). I now get excited about virtually summiting famous climbs instead of
contributing to open source. (Today's was
<a href="https://pjammcycling.com/climb/40.Mont-Ventoux-Bedoin">Mont Ventoux</a> - an
absolute beast of a climb that reminded me a lot of my real world ride up
<a href="https://pjammcycling.com/climb/11.Pikes-Peak">Pike's Peak</a> in 2020.)</p>
<p>Various changes in the past eighteen or so months have created additional
time constraints and prioritization changes that have resulted in my open
source contributions withering.</p>
<p>In addition, my technical interests have been shifting.</p>
<p>I've always gravitated to more systems-level areas of computers. My degree
is in Computer Engineering and I have a stereotypical engineer mindset: I
have an insatiable curiosity about how things work and interact and I want
to always be tinkering. I prefer to be closer to hardware instead of abstracted
far away from it. I enjoy interacting with the building blocks of software
ecosystems: operating systems, filesystems, runtimes, file formats, compilers,
etc.</p>
<p>Historically, my open source contributions to my preferred areas of
computing were limited. Again, to me open source is an enjoyable form of
creative expression. That means I do it for <em>fun</em>. Historically, the
<em>systems-level</em> programming space was limited to languages like C and
C++, which I consider frustrating and painful to use. If I'm going to subject
myself to misery when programming, you are going to have to pay me well to
do it.</p>
<p>As part of creating PyOxidizer, I learned Rust.</p>
<p>When I became proficient in Rust, I realized that Rust unlocks all kinds of
systems-level problems that were effectively off-limits for my open source
contributions. Would I <a href="/blog/2022/01/03/rust-implementation-of-debian-packaging-primitives/">implement</a>
Debian packaging primitives in Python? Or a <a href="/blog/2022/01/09/bulk-analyze-linux-packages-with-linux-package-analyzer/">tool</a>
to bulk analyze Linux packages and peek inside ELF binaries for insights
about what compiler/linker features are used in the wild in Python/C/C++?
Not unless you pay me to do it!</p>
<p>As I learned Rust, I also found myself being drawn away from Python, my
prior go-to language. As I wrote in <a href="/blog/2021/04/13/rust-is-for-professionals/">Rust is for Professionals</a>,
Rust feels surprisingly high level. It isn't as terse as Python but it is
a lot closer than I thought it would be. And Rust gives you vastly stronger
compile-time guarantees and run-time performance than Python. I felt like
Rust's tooling ecosystem was supporting me instead of standing in my way. I
felt that when you consider the overall software development lifecycle - not
just the edit-build-run loop that people tend to fixate on, likely because
it is the easiest to measure - Rust was vastly more productive and a joy to
work with than Python. All those countless hours debugging, fixing, and
authoring tests for <code>TypeError</code> and <code>ValueError</code> Python exceptions you see
in production just don't happen with Rust and that time can be better spent
iterating on core functionality, which is what actually matters. </p>
<p>On top of the Rust undercurrents, I've also become somewhat disenchanted with
the Python ecosystem. As I wrote in 2020's <a href="/blog/2020/01/13/mercurial's-journey-to-and-reflections-on-python-3/">Mercurial's Journey to and Reflections on
Python 3</a>,
the Python 3 transition was bungled and resulted in years - if not a full
decade - of lost opportunity. As I wrote in 2023's <a href="/blog/2023/10/30/my-user-experience-porting-off-setup.py/">My User Experience Porting Off
setup.py</a>, the Python
packaging story feels as discombobulated and frustrating as ever. PyOxidizer
additionally brushed up against <a href="/docs/pyoxidizer/0.24.0/pyoxidizer_technotes.html#desired-changes-from-python-to-aid-pyoxidizer">several limitations</a>
in how Python is designed and implemented, many of which are not trivially
fixable. As a <em>systems-level guy</em>, I am frequently questioning various aspects
of the Python ecosystem which I have contrasting opinions on, including the
importance of correctness and performance.</p>
<p>Starting in 2021, I started gravitating towards writing more Rust code and solving
problems in the systems domain that were previously off-limits to me, like Apple
code signing. Initially the work was in support of PyOxidizer: I was going to
implement all these packaging primitives in pure Rust and enable people to
distribute Python applications without requiring access to a Windows or macOS
machine! Over time, this work consumed me. Apple code signing turned into a major
time sink because of its complexity and the fact I was having to reverse
engineer a lot of its internals. But I was having a ton of fun doing it: more
fun than swimming upstream against decades of encrusted technical debts in the
Python ecosystem.</p>
<p>By late 2021, I realized I made a series of <em>mistakes</em> with PyOxidizer.</p>
<p>I started PyOxidizer as a science experiment to see if it was possible to achieve
a single file executable Python application without requiring a <em>temporary</em>
filesystem at run-time. I succeeded. But the cost was compatibility with the
larger pre-built Python package ecosystem. I built all this complexity into
PyOxidizer to allow people to tweak how Python resources are packaged so they
could choose to build a single file application if they wanted. This ballooned
into a hot mess and was obviously not user-friendly. It violated various
personal principles about optimizing for end-user experience.</p>
<p>Armed with knowledge of all the pitfalls, I realized that there was a 90%
use case for Python application packaging that was simple for end users and
technically achievable using all the code primitives - like the
<a href="https://crates.io/crates/pyembed">pyembed Rust crate</a> - that I built out for
PyOxidizer.</p>
<p>Thus the <a href="/blog/2022/05/10/announcing-the-pyoxy-python-runner/">PyOxy project</a> was born
and released in May 2022.</p>
<p>While I believe PyOxy is already a generally useful primitive to have in the
Python ecosystem, I had bigger goals in mind.</p>
<p>My intent with PyOxy was to build in a simplified and opinionated <em>PyOxidizer
lite</em> mode. The <code>pyoxy</code> executable is already a chameleon: if you rename it to
<code>python</code> it behaves like a <code>python</code> executable. I wanted to extend this so you
could do something like <code>pyoxy build-app</code> and it would collect all dependencies,
assemble a
<a href="https://gregoryszorc.com/docs/pyoxidizer/0.24.0/oxidized_importer_packed_resources.html">Python packed resources</a>
blob, and embed that in a copy of the <code>pyoxy</code> binary as an ELF, Mach-O, or PE
segment. Then at run-time, the variant executable binary would load the application
configuration and Python resources metadata from its own binary and execute the
application. Essentially, PyOxy would evolve into a self-packaging Python
application. I <em>just</em> needed to evolve the Python packed resources format,
implement a very crude ELF, Mach-O, and PE <em>linker</em> to append resources data to an
executable, and teach <code>pyembed</code> to read resources data from an ELF, Mach-O, or
PE segment. All within my sphere of technical competency. And I was excited to
build it and forever alter people's perceptions of how easy it could be to produce
a distributable Python application.</p>
<p>Then the roller coaster of my personal life took over. I felt newly invigorated
with a new job role. I got engaged and married. I became a father.</p>
<p>By early 2023, it was clear my ability to contribute to open source would be
vastly diminished for the foreseeable future. PyOxidizer and PyOxy fell into a
state of neglect. Weeks went by without me even tinkering on my local computer,
much less push commits or publish a release. Weeks turned into months. Months
into quarters. At this point, I haven't pushed a commit to
<a href="https://github.com/indygreg/PyOxidizer">indygreg/PyOxidizer</a> since January 2023.
And I'm not sure when I next will, if ever.</p>
<p>In my limited open source contribution time, I've prioritized other projects
over PyOxidizer.</p>
<p><a href="https://github.com/indygreg/python-build-standalone">python-build-standalone</a>
has gained a life outside PyOxidizer. It is now used by
<a href="https://github.com/astral-sh/rye">rye</a>, Bazel's
<a href="https://github.com/bazelbuild/rules_python">rules_python</a>,
<a href="https://beeware.org/project/projects/tools/briefcase/">briefcase</a>, and a myriad
of other consumers. The release assets have been downloaded over 23 million
times and the download rate appears to be accelerating. I still actively
support python-build-standalone and intend for the project to be actively
supported for the indefinite future: it has become too important to abandon. I'm
actively recruiting assistance to help maintain the project and I'm not concerned
about its future.</p>
<p>Apple code signing still actively draws my engagement. What I love about the
project is it either works or it doesn't: there's limited extra features we can
add to it since Apple mostly dictates the feature set. And I perceive the current
project to be mostly <em>done</em>.</p>
<p><a href="https://github.com/indygreg/python-zstandard">python-zstandard</a> is downloaded
~8 million times per month. The project is long overdue for some modernization.
I'm sitting on a pile of commits to improve it, but progress has been slow. I
just learned this weekend that the maintainer of the other popular zstandard
Python package deleted their GitHub account recently and now users are looking to
onboard to my package. Nothing quite like unanticipated distractions!</p>
<p><strong>That's a very long-winded way of saying that PyOxidizer and all the projects under
its umbrella are effectively in a zombie state.</strong> I'm hesitant to say <em>dead</em> because
if I suddenly found myself with lots of free time I'd love to brush off the cobwebs
and bring the projects back to life. But who am I kidding: they are effectively dead
at the moment because with everything happening in my personal life, I don't see where
I find the time to resuscitate the project. And that assumes I even want to: again,
I've become somewhat disenchanted by the state of Python. The main thing that draws
me to it is the size of the community and the potential for impact. But to realize
that impact I feel like I'd be pushing Python in directions it isn't well-equipped
to go in. Quite franky - and, yes, selfishly - I don't want to subject myself to
the misery unless I'm being well paid to do it. Again, I view my open source
contributions as a <em>fun</em> outlet for my creative expression and nudging Python
packaging in directions it is obviously ill-equipped to go in just isn't <em>fun</em>.</p>
<p><strong>If anyone reading has an interest in taking ownership or maintenance responsibilities
of PyOxidizer, any projects under its umbrella, or any of my other open source projects,
I'm receptive to proposals.</strong> Send me an <a href="mailto:gregory.szorc@gmail.com">email</a>
or create an issue or discussion on GitHub if you want to do it publicly.</p>
<p>But I'm going to assume that PyOxidizer is going to wither and die - or at least
incur some massive backwards incompatible breaks if it continues to live. I've already
filed issues against python-build-standalone - such as
<a href="https://github.com/indygreg/python-build-standalone/issues/221">removing Windows static builds</a> -
to make the project easier to support and less work for future maintainers.</p>
<p>If I have one regret about how this has played out, it is my failure to
communicate developments in my open source commitments / expectations in a timely manner.
I knew the future was bleak in early 2023 but didn't publicly say anything.
I still thought there was a chance that things were going to change and I didn't
want to make a hard decision prematurely. Writing this post has been on my mind
since the middle of 2023 but I just couldn't bring myself to write it. And -
surprise - having a newborn at home is a giant time and mental commitment! I'm
writing this now because people are (finally!) noticing my lack of contributions to
PyOxidizer and asking questions. And I'm home alone for a few days and actually
have time to sit down and compose this post. (Yes, I'm that stretched for time in
my personal life.)</p>
<p>In 2023, I struggled with the idea of letting people down by declaring PyOxidizer
<em>dead</em>. But when I wake up every morning, walk into the nursery, and cause my daughter
to smile and flail her arms and legs with unbridled excitement when she sees me, I'd
have it no other way. When it comes to choosing between open source and family, I
choose family.</p>
<p>It feels appropriate to end this post with a link to <a href="https://xkcd.com/2347/">XKCD 2347: Dependency</a>.
But I'm not the <em>random person in Nebraska</em>: I'm a husband and father.</p>]]></content>
  </entry>
  <entry>
    <author>
      <name>Gregory Szorc</name>
      <uri>http://gregoryszorc.com/blog</uri>
    </author>
    <title type="html"><![CDATA[My User Experience Porting Off setup.py]]></title>
    <link rel="alternate" type="text/html" href="http://gregoryszorc.com/blog/2023/10/30/my-user-experience-porting-off-setup.py" />
    <id>http://gregoryszorc.com/blog/2023/10/30/my-user-experience-porting-off-setup.py</id>
    <updated>2023-10-30T06:00:00Z</updated>
    <published>2023-10-30T06:00:00Z</published>
    <category scheme="http://gregoryszorc.com/blog" term="Python" />
    <summary type="html"><![CDATA[My User Experience Porting Off setup.py]]></summary>
    <content type="html" xml:base="http://gregoryszorc.com/blog/2023/10/30/my-user-experience-porting-off-setup.py"><![CDATA[<p>In the past week I went to add Python 3.12 support to my
<a href="https://github.com/indygreg/python-zstandard">zstandard</a> Python package.
A few hours into the unexpected yak shave / rat hole, I decided to start
chronicling my experience so that I may share it with the broader Python
community. My hope is that by sharing my (unfortunately painful) end-user
experience that I can draw attention to aspects of Python packaging that
are confusing so that better informed and empowered people can improve
matters and help make future Python packaging decisions to help scenarios
like what I'm about to describe.</p>
<p>This blog post is purposefully verbose and contains a very lightly edited
stream of my mental thoughts. Think of it as a self-assessed user experience
study of Python packaging.</p>
<h2>Some Background</h2>
<p>I'm no stranger to the Python ecosystem or Python packaging. I've been
programming Python for 10+ years. I've even authored a Python application
packaging tool, <a href="https://gregoryszorc.com/docs/pyoxidizer/main/">PyOxidizer</a>.</p>
<p>When programming, I strive to understand how things work. I try to not blindly
copy-paste or cargo cult patterns unless I understand how they work. This
means I often scope bloat myself and slow down velocity in the short term.
But I justify this practice because I find it often pays dividends in the long
term because I actually understand how things work.</p>
<p>I also have a passion for security and supply chain robustness. After you've
helped maintain complex CI systems for multiple companies, you learn the hard
way that it is important to do things like transitively pin dependencies and
reduce surface area for failures so that build automation breaks in reaction
to code changes in your version control, not spooky-action-at-a-distance when
state on a third party server changes (e.g. a new package version is uploaded).</p>
<p>I've been aware of the emergence of <code>pyproject.toml</code>. But I've largely sat on
the sidelines and held off adopting them, mainly for <em>if it isn't broken, don't
fix it</em> reasons. Plus, my perception has been that the tooling still hasn't
stabilized: I'm not going to incur work now if it is going to invite avoidable
churn that could be avoided by sitting on my hands a little longer.</p>
<p>Now, on to my user experience of adding Python 3.12 to python-zstandard and
the epic packaging yak shave that entailed.</p>
<h2>The Journey Begins</h2>
<p>When I attempted to run CI against Python 3.12 on GitHub Actions, running
<code>python setup.py</code>complained that <code>setuptools</code> couldn't be imported.</p>
<p>Huh? I thought <code>setuptools</code> was installed in pretty much every Python
distribution by default? It was certainly installed in all previous Python
versions by the <a href="https://github.com/actions/setup-python">actions/setup-python</a>
GitHub Action. I was aware <code>distutils</code> was removed from the Python 3.12
standard library. But setuptools and distutils are not the same! Why did
<code>setuptools</code> disappear?</p>
<p>I look at the CI logs for the passing Python 3.11 job and notice a message:</p>
<div class="pygments_murphy"><pre><span></span>********************************************************************************
Please avoid running ``setup.py`` directly.
Instead, use pypa/build, pypa/installer or other
standards-based tools.

See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.
********************************************************************************
</pre></div>

<p>I had several immediate reactions:</p>
<ol>
<li>OK, maybe this is a sign I should be modernizing to <code>pyproject.toml</code> and
   moving away from <code>python setup.py</code>. Maybe the missing <code>setuptools</code> in the
   3.12 CI environment is a side-effect of this <em>policy</em> shift?</li>
<li>What are <code>pypa/build</code> and <code>pypa/installer</code>? I've never heard of them. I know
   <code>pypa</code> is the Python Packaging Authority (I suspect most Python developers
   don't know this). Are these GitHub <em>org/repo</em> identifiers?</li>
<li>What exactly is a <em>standards-based tool</em>? Is pip not a <em>standards-based tool</em>?</li>
<li>Speaking of pip, why isn't it mentioned? I thought pip was the de facto
   packaging tool and had been for a while!</li>
<li>It's linking a URL for more info. But why is this a link to what looks like
   an individual's blog and not to some more official site, like the setuptools
   or pip docs? Or anything under python.org?</li>
</ol>
<h2>Learning That I Shouldn't Invoke <code>python setup.py</code></h2>
<p>I open <a href="https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html">https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html</a>
in my browser and see a 4,000+ word blog post. Oof. Do I really want/need to read
this? Fortunately, the author included a tl;dr and linked to a
<a href="https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html#summary">summary</a>
section telling me a lot of useful information! It informs me (my commentary in
parentheses):</p>
<ol>
<li><em>The setuptools project has stopped maintaining all direct invocations of setup.py
   years ago</em>. (What?!)</li>
<li><em>There are undoubtedly many ways that your setup.py-based system is broken today,
   even if it's not failing loudly or obviously.</em>. (What?! Surely this can't be
   true. I didn't see any warnings from tooling until recently. How was I supposed
   to know this?)</li>
<li><em>PEP 517, 518 and other standards-based packaging are the future of the Python
   ecosystem</em>. (A ha - a definition of <em>standards-based</em> tooling. I guess I have
   to look at PEP 517 and PEP 518 in more detail. I'm pretty sure these are
   the PEPs that define <code>pyproject.toml</code>.)</li>
<li><em>At this point you may be expecting me to give you a canonical list of the right
   way to do everything that setup.py used to do, and unfortunately the answer here
   is that it's complicated.</em> (You are telling me that we had a working
   <code>python setup.py</code> solution for 10+ years, this workflow is now quasi deprecated,
   and the recommended replacement is <em>it's complicated</em>?! I'm just trying to get my
   package modernized. Why does that need to be <em>complicated</em>?)</li>
<li><em>That said, I can give you some simple "works for most people" recommendations for
   some of the common commands.</em> (Great, this is exactly what I was looking for!)</li>
</ol>
<p>Then I look at the table mapping old ways to new ways. In the <em>new</em> column, it
references the following tools: <a href="https://pypa-build.readthedocs.io/en/stable/">build</a>,
pytest, <a href="https://tox.wiki/en/latest/">tox</a>, <a href="https://tox.wiki/en/latest/">nox</a>,
pip, and <a href="https://twine.readthedocs.io/en/latest/">twine</a>. That's quite the
tooling salad! (And that <code>build</code> tool must be the <em>pypa/build</em> referenced in the
setuptools warning message. One mystery solved!)</p>
<p>I scroll back to the top of the article and notice the date: October 2021. Two
years old. The summary section also mentioned that there's been a lot of
activity around packaging tooling occurring. So now I'm wondering if this blog
post is outdated. Either way, it is clear I have to perform some additional
research to figure out how to migrate off <code>python setup.py</code> so I can be
compliant with the new world order.</p>
<h2>Learning About <code>pyproject.toml</code> and Build Systems</h2>
<p>I had pre-existing knowledge of <code>pyproject.toml</code> as the modern way to define
build system metadata. So I decide to start my research by Googling
<code>pyproject.toml</code>. The first results are:</p>
<ol>
<li><a href="https://pip.pypa.io/en/stable/reference/build-system/pyproject-toml/">https://pip.pypa.io/en/stable/reference/build-system/pyproject-toml/</a></li>
<li><a href="https://stackoverflow.com/questions/62983756/what-is-pyproject-toml-file-for">https://stackoverflow.com/questions/62983756/what-is-pyproject-toml-file-for</a></li>
<li><a href="https://python-poetry.org/docs/pyproject/">https://python-poetry.org/docs/pyproject/</a></li>
<li><a href="https://setuptools.pypa.io/en/latest/userguide/pyproject_config.html">https://setuptools.pypa.io/en/latest/userguide/pyproject_config.html</a></li>
<li><a href="https://godatadriven.com/blog/a-practical-guide-to-setuptools-and-pyproject-toml/">https://godatadriven.com/blog/a-practical-guide-to-setuptools-and-pyproject-toml/</a></li>
<li><a href="https://towardsdatascience.com/pyproject-python-9df8cc092f61">https://towardsdatascience.com/pyproject-python-9df8cc092f61</a></li>
</ol>
<p>I click pip's documentation first because pip is known to me and it seems a
canonical source. Pip's documentation proceeds to link to
<a href="https://peps.python.org/pep-0518/">PEP-518</a>, <a href="https://peps.python.org/pep-0517/">PEP-517</a>,
<a href="https://peps.python.org/pep-0621/">PEP-621</a>, and <a href="https://peps.python.org/pep-0660/">PEP-660</a>
before telling me how projects with <code>pyproject.toml</code> are built, without giving
me - a package maintainer - much useful advice for what to do or how to port from
<code>setup.py</code>. This seems like a dead end.</p>
<p>Then I look at the Stack Overflow link. Again, telling me a lot of what I don't
really care about. (I've somewhat lost faith in Stack Overflow and only really
skimmed this page: I would much prefer to get an answer from a first party
source.)</p>
<p>I click on the <a href="https://python-poetry.org/docs/pyproject/">Poetry</a> link. It
documents TOML fields. But only for the <code>[tool.poetry]</code> section. While I've
heard about Poetry, I know that I probably don't want to scope bloat myself
to learn how Poetry works so I can use it. (No offence meant to the Poetry
project here but I don't perceive my project as needing whatever features
Poetry provides: I'm <em>just</em> trying to publish a simple library package.) I
go back to the search results.</p>
<p>I click on the <a href="https://setuptools.pypa.io/en/latest/userguide/pyproject_config.html">setuptools</a>
link. I'm using setuptools via <code>setup.py</code> so this content looks promising! It gives
me a nice example TOML of how to configure a <code>[build-system]</code> and <code>[project]</code>
metadata. It links to PyPA's <a href="https://packaging.python.org/en/latest/specifications/declaring-project-metadata/">Declaring project metadata</a>
content, which I open in a new tab, as the content seems useful. I continue
reading setuptools documentation. I land on its
<a href="https://setuptools.pypa.io/en/latest/userguide/quickstart.html">Quickstart</a>
documentation, which seems useful. I start reading it and it links to the
<a href="https://pypa-build.readthedocs.io/en/latest/">build</a> tool documentation.
That's the second link to the <code>build</code> tool. So I open that in a new tab.</p>
<p>At this point, I think I have all the documentation on <code>pyproject.toml</code>. But
I'm still trying to figure out what to replace <code>python setup.py</code> with. The
<code>build</code> tool certainly seems like a contender since I've seen multiple
references to it. But I'm still looking for <em>modern</em>, actively maintained
documentation pointing me in a blessed direction.</p>
<p>The next Google link is <a href="https://godatadriven.com/blog/a-practical-guide-to-setuptools-and-pyproject-toml/">A Practical Guide to Setuptools and Pyproject.toml</a>.
I start reading that. I'm immediately confused because it is recommending I
put setuptools metadata in <code>setup.cfg</code> files. But I just read all about defining
this metadata in <code>pyproject.toml</code> files in setuptools' own documentation! Is
this blog post out of date? March 12, 2022. Seems pretty modern. I look at the
setuptools documentation again and see the <code>pyproject.toml</code> metadata pieces are
in version 61.0.0 and newer. I go to
<a href="https://github.com/pypa/setuptools/releases/tag/v61.0.0">https://github.com/pypa/setuptools/releases/tag/v61.0.0</a>
and see version 61.0.0 was released on March 25, 2022. So the fifth Google link
was seemingly obsoleted 13 days after it was published. Good times. I pretend I
never read this content because it seems out of date.</p>
<p>The next Google link is
<a href="https://towardsdatascience.com/pyproject-python-9df8cc092f61">https://towardsdatascience.com/pyproject-python-9df8cc092f61</a>.
I click through. But Medium wants me to log in to read it all and it is unclear
it is going to tell me anything important, so I back out.</p>
<h2>Learning About the <code>build</code> Tool</h2>
<p>I give up on Google for the moment and start reading up on the <code>build</code> tool
from its docs.</p>
<p>The only usage documentation for the <code>build</code> tool is on its
<a href="https://pypa-build.readthedocs.io/en/latest/index.html">root documentation page</a>.
And that documentation basically prints what <code>python -m build --help</code> would
print: says what the tool does but doesn't give any guidance or where I should
be using it or how to replace existing tools (like <code>python setup.py</code> invocations).
Yes, I can piece the parts together and figure out that <code>python -m build</code> can be
used as a replacement for <code>python setup.py sdist</code> and <code>python setup.py bdist_wheel</code>
(and maybe <code>pip wheel</code>?). But <em>should</em> it be the replacement I choose? I make
use of <code>python setup.py develop</code> and the aforementioned blog post recommended
replacing that with <code>python -m pip install -e</code>. Perhaps I can use <code>pip</code> as the
singular replacement for building source distributions and binary wheels so I
have N-1 packaging tools? I keep researching.</p>
<h2>Exploring the Python Packaging User Guide</h2>
<p>I had previously opened <a href="https://packaging.python.org/en/latest/specifications/declaring-project-metadata/">https://packaging.python.org/en/latest/specifications/declaring-project-metadata/</a>
in a browser tab without really looking at it. On second glance, I see it is part
of a broader <a href="https://packaging.python.org/en/latest/">Python Packaging User Guide</a>.
Oh, this looks promising! A guide on how to do what I'm seeking maintained by the
Python Packaging Authority (PyPA), the group who I know to be the, well, authorities
on Python packaging. It is is published under the canonical <code>python.org</code> domain.
Surely the answer will be here.</p>
<p>I immediately click on the link to
<a href="https://packaging.python.org/en/latest/tutorials/packaging-projects/">Packaging Python Projects</a>
to hopefully see what the PyPA folks are recommending.</p>
<h3>Is Hatch the Answer?</h3>
<p>I skim through. I see recommendations to use a <code>pyproject.toml</code> with a
<code>[build-system]</code> to define the build backend. This matches my expectations.
But they are using <em>Hatchling</em> as their build backend. Another tool I don't
really know about. I click through some inline links and eventually arrive
at <a href="https://github.com/pypa/hatch">https://github.com/pypa/hatch</a>. (I'm kind of
confused why the PyPA tutorial said <em>Hatchling</em> when the project and tool is
apparently named <em>Hatch</em>. But whatever.)</p>
<p>I skim Hatch's GitHub README. It looks like a unified packaging tool. Build
system. Package uploading/publishing. Environment management (sounds like a
virtualenv alternative?). This tool actually seems quite nice! I start skimming
the docs. Like Poetry, it seems like this is yet another new tool that I'd need
to learn and would require me to blow up my existing <code>setup.py</code> in order to
adopt. Do I really want to put in that effort? I'm just trying to get
python-zstandard back on the paved road and avoid seemingly deprecated workflows:
I'm not looking to adopt new tooling stacks.</p>
<p>I'm also further confused by the existence of Hatch under the
<a href="https://github.com/pypa">PyPA GitHub Organization</a>. That's the same
GitHub organization hosting the Python packaging tools that are known to
me, namely
<a href="https://github.com/pypa/build">build</a>, <a href="https://github.com/pypa/pip">pip</a>,
and <a href="https://github.com/pypa/setuptools">setuptools</a>. Those three projects
are pinned repositories. (The other three pinned repositories are
<a href="https://github.com/pypa/virtualenv">virtualenv</a>,
<a href="https://github.com/pypa/wheel">wheel</a>, and
<a href="https://github.com/pypa/twine">twine</a>.) Hatch is seemingly a replacement
for pip, setuptools, virtualenv, twine, and possibly other tools. But it isn't
a pinned repository. Yet it is the default tool used in the PyPA maintained
<a href="https://packaging.python.org/en/latest/tutorials/packaging-projects/">Packaging Python Projects</a>
guide. (That guide also suggests using other tools like setuptools,
<a href="https://github.com/pypa/flit">flit</a>, and
<a href="https://github.com/pdm-project/pdm/.">pdm</a>. But the default is Hatch and that has me asking questions. Also,
I didn't initially notice that
<a href="https://packaging.python.org/en/latest/tutorials/packaging-projects/#creating-pyproject-toml">Creating pyproject.toml</a>
has multiple tabs for different backends.)</p>
<p>While Hatch looks interesting, I'm just not getting a strong signal that Hatch
is sufficiently stable or warrants my time investment to switch to. So I go
back to reading the
<a href="https://packaging.python.org/en/latest/">Python Packaging User Guide</a>.</p>
<h3>The PyPA User Guide Search Continues</h3>
<p>As I click around the User Guide, it is clear the PyPA folks really want me
to use <code>pyproject.toml</code> for packaging. I suppose that's the future and that's
a fair ask. But I'm still confused how I should migrate my <code>setup.py</code> to it.
What are the risks with replacing my <code>setup.py</code> with <code>pyproject.toml</code>? Could
I break someone installing my package on an old Linux distribution or old
virtualenv using an older version of setuptools or pip? Will my adoption of
build, hatch, poetry, whatever constitute a one way door where I lock out
users in older environments? My package is downloaded over one million times
per month and if I break packaging <em>someone</em> is likely to complain.</p>
<p>I'm desperately looking for guidance from the PyPA at
<a href="https://packaging.python.org/">https://packaging.python.org/</a> on how to
manage this migration. But I just... can't find it.
<a href="https://packaging.python.org/en/latest/guides/">Guides</a> surprisingly has
nothing on the topic.</p>
<h3>Outdated Tool Recommendations from the PyPA</h3>
<p>Finally I find <a href="https://packaging.python.org/en/latest/guides/tool-recommendations/">Tool recommendations</a>
in the PyPA User Guide. Under
<a href="https://packaging.python.org/en/latest/guides/tool-recommendations/#packaging-tool-recommendations">Packaging tool recommendations</a>
it says:</p>
<ul>
<li><em>Use setuptools to define projects.</em></li>
<li><em>Use build to create Source Distributions and wheels.</em></li>
<li><em>If you have binary extensions and want to distribute wheels for multiple
  platforms, use cibuildwheel as part of your CI setup to build distributable
  wheels.</em></li>
<li><em>Use twine for uploading distributions to PyPI.</em></li>
</ul>
<p>Finally, some canonical documentation from the PyPA that comes out and
suggests what to use!</p>
<p>But my relief immediately turns to questioning whether this tooling
recommendations documentation is up to date:</p>
<ol>
<li>If setuptools is recommended, why does the
   <a href="https://packaging.python.org/en/latest/tutorials/packaging-projects/">Packaging Python Projects</a>
   tutorial use Hatch?</li>
<li>How exactly should I be using setuptools to define projects? Is this
   referring to setuptools as a <code>[build-system]</code> backend? The existence of
   <em>define</em> seemingly implies using <code>setup.py</code> or <code>setup.cfg</code> to define metadata.
   But I thought these distutils/setuptools specific mechanisms were deprecated
   in favor of the more generic <code>pyproject.toml</code>?</li>
<li>Why aren't other tools like Hatch, pip, poetry, flit, and pdm mentioned on
   this page? Where's the guidance on when to use these alternative tools?</li>
<li>There are footnotes referencing <code>distutils</code> as if it is still a modern
   practice. No mention that it was removed from the standard library in
   Python 3.12.</li>
<li>But the <code>build</code> tool is referenced and that tool is relatively new. So the
   docs have to be somewhat up-to-date, right?</li>
</ol>
<p>Sadly, I reach the conclusion that this
<a href="https://packaging.python.org/en/latest/guides/tool-recommendations/">Tool recommendations</a>
documentation is inconsistent with newer documentation and can't be trusted.
But it did mention the <code>build</code> tool and we now have multiple independent
sources steering me in the direction of the <code>build</code> tool (at least for source
distribution and wheel building), so it seems like we have a winner on our
hands.</p>
<h2>Initial Failures Running <code>build</code></h2>
<p>So let's use the <code>build</code> tool. I remember docs saying to invoke it with
<code>python -m build</code>, so I try that:</p>
<div class="pygments_murphy"><pre><span></span>$ python3.12 -m build --help
No module named build.__main__; &#39;build&#39; is a package and cannot be directly executed
</pre></div>

<p>So the <code>build</code> package exists but it doesn't have a <code>__main__</code>. Ummm.</p>
<div class="pygments_murphy"><pre><span></span>$ python3.12R
Python 3.12.0 (main, Oct 23 2023, 19:58:35) [Clang 15.0.0 (clang-1500.0.40.1)] on darwin
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
&gt;&gt;&gt; import build
&gt;&gt;&gt; build.__spec__
ModuleSpec(name=&#39;build&#39;, loader=&lt;_frozen_importlib_external.NamespaceLoader object at 0x10d403bc0&gt;, submodule_search_locations=_NamespacePath([&#39;/Users/gps/src/python-zstandard/build&#39;]))
</pre></div>

<p>Oh, it picked up the <code>build</code> directory from my source checkout because
<code>sys.path</code> has the current directory by default. Good times.</p>
<div class="pygments_murphy"><pre><span></span>$ (cd ~ &amp;&amp; python3.12 -m build)
/Users/gps/.pyenv/versions/3.12.0/bin/python3.12: No module named build
</pre></div>

<p>I guess <code>build</code> isn't installed in my Python distribution / environment.
You used to be able to build packages using just the Python standard library.
I guess this battery is no longer included in the stdlib. I shrug and continue.</p>
<h2>Installing <code>build</code></h2>
<p>I go to the <a href="https://pypa-build.readthedocs.io/en/latest/installation.html">Build installation docs</a>.
It says to <code>pip install build</code>. (I thought I read years ago that one should
use <code>python3 -m pip</code> to invoke pip. Strange that a PyPA maintained tool is
telling me to invoke <code>pip</code> directly since I'm pretty sure a lot of the reasons
to use <code>python -m</code> to invoke tools are still valid. But I digress.)</p>
<p>I follow the instructions, installing it to the global <code>site-packages</code>
because I figure I'll use this tool a lot and I'm not a virtual environment
purist:</p>
<div class="pygments_murphy"><pre><span></span>$ python3.12 -m pip install build
Collecting build
  Obtaining dependency information for build from https://files.pythonhosted.org/packages/93/dd/b464b728b866aaa62785a609e0dd8c72201d62c5f7c53e7c20f4dceb085f/build-1.0.3-py3-none-any.whl.metadata
  Downloading build-1.0.3-py3-none-any.whl.metadata (4.2 kB)
Collecting packaging&gt;=19.0 (from build)
  Obtaining dependency information for packaging&gt;=19.0 from https://files.pythonhosted.org/packages/ec/1a/610693ac4ee14fcdf2d9bf3c493370e4f2ef7ae2e19217d7a237ff42367d/packaging-23.2-py3-none-any.whl.metadata
  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)
Collecting pyproject_hooks (from build)
  Using cached pyproject_hooks-1.0.0-py3-none-any.whl (9.3 kB)
Using cached build-1.0.3-py3-none-any.whl (18 kB)
Using cached packaging-23.2-py3-none-any.whl (53 kB)
Installing collected packages: pyproject_hooks, packaging, build
Successfully installed build-1.0.3 packaging-23.2 pyproject_hooks-1.0.0
</pre></div>

<p>That downloads and installs wheels for <code>build</code>, <code>packaging</code>, and
<code>pyproject_hooks</code>.</p>
<p>At this point the security aware part of my brain is screaming because we
didn't pin versions or SHA-256 digests of any of these packages
anywhere. So if a malicious version of any of these packages is somehow
uploaded to PyPI that's going to be a nightmare software supply chain
vulnerability having similar industry impact as
<a href="https://en.wikipedia.org/wiki/Log4Shell">log4shell</a>. Nowhere in build's
documentation does it mention this or say how to securely install build.
I suppose you have to just know about the supply chain gotchas with
<code>pip install</code> in order to mitigate this risk for yourself.</p>
<h2>Initial Results With <code>build</code> Are Promising</h2>
<p>After getting <code>build</code> installed, <code>python3.12 -m build --help</code> works now
and I can build a wheel:</p>
<div class="pygments_murphy"><pre><span></span>$ python3.12 -m build --wheel .
* Creating venv isolated environment...
* Installing packages in isolated environment... (setuptools &gt;= 40.8.0, wheel)
* Getting build dependencies for wheel...
...
* Installing packages in isolated environment... (wheel)
* Building wheel...
running bdist_wheel
running build
running build_py
...
Successfully built zstandard-0.22.0.dev0-cp312-cp312-macosx_14_0_x86_64.whl
</pre></div>

<p>That looks promising! It seems to have invoked my <code>setup.py</code> without me
having to define a <code>[build-system]</code> in my <code>pyproject.toml</code>! Yay for backwards
compatibility.</p>
<h2>The Mystery of the Missing <code>cffi</code> Package</h2>
<p>But I notice something.</p>
<p>My <code>setup.py</code> script conditionally builds a <code>zstandard._cffi</code> extension
module if <code>import cffi</code> succeeds. Building with <code>build</code> isn't building this
extension module.</p>
<p>Before using <code>build</code>, I had to run <code>setup.py</code> using a <code>python</code> having the
<code>cffi</code> package installed, usually a project-local virtualenv. So let's try
that:</p>
<div class="pygments_murphy"><pre><span></span>$ venv/bin/python -m pip install build cffi
...
$ venv/bin/python -m build --wheel .
...
</pre></div>

<p>And I get the same behavior: no CFFI extension module.</p>
<p>Staring at the output, I see what looks like a smoking gun:</p>
<div class="pygments_murphy"><pre><span></span>* Creating venv isolated environment...
* Installing packages in isolated environment... (setuptools &gt;= 40.8.0, wheel)
* Getting build dependencies for wheel...
...
* Installing packages in isolated environment... (wheel)
</pre></div>

<p>OK. So it looks like <code>build</code> is creating its own isolated environment
(disregarding the invoked Python environment having <code>cffi</code> installed),
installing <code>setuptools &gt;= 40.8.0</code> and <code>wheel</code> into it, and then executing
the build from that environment.</p>
<p>So <code>build</code> sandboxes builds in an ephemeral build environment. This actually
seems like a useful feature to help with deterministic and reproducible
builds: I like it! But at this moment it stands in the way of progress. So
I run <code>python -m build --help</code>, spot a <code>--no-isolation</code> argument and do the
obvious:</p>
<div class="pygments_murphy"><pre><span></span>$ venv/bin/python -m build --wheel --no-isolation .
...
building &#39;zstandard._cffi&#39; extension
...
</pre></div>

<p>Success!</p>
<p>And I don't see any deprecation warnings either. So I <em>think</em> I'm all good.</p>
<p>But obviously I've ventured off the paved road here, as we had to violate
the default constraints of <code>build</code> to get things to work. I'll get back to that
later.</p>
<h2>Reproducing Working Wheel Builds With <code>pip</code></h2>
<p>Just for good measure, let's see if we can use <code>pip wheel</code> to produce wheels,
as I've seen references that this is a supported mechanism for building wheels.</p>
<div class="pygments_murphy"><pre><span></span>$ venv/bin/python -m pip wheel .
Processing /Users/gps/src/python-zstandard
  Installing build dependencies ... done
  Getting requirements to build wheel ... done
  Preparing metadata (pyproject.toml) ... done
Building wheels for collected packages: zstandard
  Building wheel for zstandard (pyproject.toml) ... done
  Created wheel for zstandard: filename=zstandard-0.22.0.dev0-cp312-cp312-macosx_14_0_x86_64.whl size=407841 sha256=a2e1cc1ad570ab6b2c23999695165a71c8c9e30823f915b88db421443749f58e
  Stored in directory: /Users/gps/Library/Caches/pip/wheels/eb/6b/3e/89aae0b17b638c9cdcd2015d98b85ee7fb3ef00325bb44a572
Successfully built zstandard
</pre></div>

<p>That output is a bit terse, since the setuptools build logs are getting swallowed.
That's fine. Rather than run with <code>-v</code> to get those logs, I manually inspect
the built wheel:</p>
<div class="pygments_murphy"><pre><span></span>$ unzip -lv zstandard-0.22.0.dev0-cp312-cp312-macosx_14_0_x86_64.whl
Archive:  zstandard-0.22.0.dev0-cp312-cp312-macosx_14_0_x86_64.whl
 Length   Method    Size  Cmpr    Date    Time   CRC-32   Name
--------  ------  ------- ---- ---------- ----- --------  ----
    7107  Defl:N     2490  65% 10-23-2023 08:36 7bb42fff  zstandard/__init__.py
   13938  Defl:N     2498  82% 10-23-2023 08:36 8d8d1316  zstandard/__init__.pyi
  919352  Defl:N   366631  60% 10-26-2023 08:28 3aeefc48  zstandard/backend_c.cpython-312-darwin.so
  152430  Defl:N    32528  79% 10-26-2023 05:37 fc1a3c0c  zstandard/backend_cffi.py
       0  Defl:N        2   0% 12-26-2020 16:12 00000000  zstandard/py.typed
    1484  Defl:N      784  47% 10-26-2023 08:28 facba579  zstandard-0.22.0.dev0.dist-info/LICENSE
    2863  Defl:N      847  70% 10-26-2023 08:28 b8d80875  zstandard-0.22.0.dev0.dist-info/METADATA
     111  Defl:N      106   5% 10-26-2023 08:28 878098e6  zstandard-0.22.0.dev0.dist-info/WHEEL
      10  Defl:N       12 -20% 10-26-2023 08:28 a5f38e4e  zstandard-0.22.0.dev0.dist-info/top_level.txt
     841  Defl:N      509  40% 10-26-2023 08:28 e9a804ae  zstandard-0.22.0.dev0.dist-info/RECORD
--------          -------  ---                            -------
 1098136           406407  63%                            10 files
</pre></div>

<p>(Python wheels are just zip files with certain well-defined paths having special
meanings. I know this because I wrote Rust code for <em>parsing</em> wheels as part of
developing PyOxidizer.)</p>
<p>Looks like the <code>zstandard/_cffi.cpython-312-darwin.so</code> extension module is missing.
Well, at least <code>pip</code> is consistent with <code>build</code>! Although somewhat confusingly I don't
see any reference to a separate build environment in the pip output. But I suspect
it is there because <code>cffi</code> is installed in the virtual environment I invoke pip from!</p>
<p>Reading pip help output, I find the relevant argument to not spawn a new
environment and try again:</p>
<div class="pygments_murphy"><pre><span></span>$ venv/bin/python -m pip wheel --no-build-isolation .
&lt;same exact output except the wheel size and digest changes&gt;

$ unzip -lv zstandard-0.22.0.dev0-cp312-cp312-macosx_14_0_x86_64.whl
...
 1002664  Defl:N   379132  62% 10-26-2023 08:33 48afe5ba  zstandard/_cffi.cpython-312-darwin.so
...
</pre></div>

<p>(I'm happy to see <code>build</code> and <code>pip</code> agreeing on the <em>no isolation</em> terminology.)</p>
<p>OK, so I got <code>build</code> and <code>pip</code> to behave nearly identically. I feel like I
finally understand this!</p>
<p>I also run <code>pip -v wheel</code> and <code>pip -vv wheel</code> to peek under the covers and see
what it's doing. Interestingly, I don't see any hint of a virtual environment
or temporary directory until I go to <code>-vv</code>. I find it interesting that <code>build</code>
presents details about this by default but you have to put <code>pip</code> in very verbose
mode to get it. I'm glad I used <code>build</code> first because the ephemeral build
environment was the source of my missing dependency and <code>pip</code> buried this
important detail behind a ton of other output in <code>-vv</code>, making it much harder
to discover!</p>
<h2>Understanding How <code>setuptools</code> Gets Installed</h2>
<p>When looking at pip's verbose output, I also see references to installing the
<code>setuptools</code> and <code>wheel</code> packages:</p>
<div class="pygments_murphy"><pre><span></span>Processing /Users/gps/src/python-zstandard
  Running command pip subprocess to install build dependencies
  Collecting setuptools&gt;=40.8.0
    Using cached setuptools-68.2.2-py3-none-any.whl.metadata (6.3 kB)
  Collecting wheel
    Using cached wheel-0.41.2-py3-none-any.whl.metadata (2.2 kB)
  Using cached setuptools-68.2.2-py3-none-any.whl (807 kB)
  Using cached wheel-0.41.2-py3-none-any.whl (64 kB)
  Installing collected packages: wheel, setuptools
  Successfully installed setuptools-68.2.2 wheel-0.41.2
  Installing build dependencies ... done
</pre></div>

<p>There's that <code>setuptools&gt;=40.8.0</code> constraint again. (We also saw it in <code>build</code>.)
I <code>rg 40.8.0</code> my source checkout (note: the <code>.</code> in there are wildcard characters
since <code>40.8.0</code> is a regexp so this could over match) and come up with nothing.
If it's not coming from my code, where is it coming from?</p>
<p>In the pip documentation, <a href="https://pip.pypa.io/en/stable/reference/build-system/pyproject-toml/#fallback-behaviour">Fallback behaviour</a>
says that a missing <code>[build-system]</code> from <code>pyproject.toml</code> is implicitly
translated to the following:</p>
<div class="pygments_murphy"><pre><span></span><span class="k">[build-system]</span>
<span class="n">requires</span> <span class="o">=</span> <span class="k">[&quot;setuptools&gt;=40.8.0&quot;, &quot;wheel&quot;]</span>
<span class="n">build-backend</span> <span class="o">=</span> <span class="s">&quot;setuptools.build_meta:__legacy__&quot;</span>
</pre></div>

<p>For <code>build</code>, I go to the source code and discover that
<a href="https://github.com/pypa/build/commit/7504e2a7a8ac956b8f2991ae28aa45d8d73b9740">similar functionality</a>
was added in May 2020.</p>
<p>I'm not sure if this default behavior is specified in a PEP or what. But
<code>build</code> and <code>pip</code> seem to be agreeing on the behavior of adding
<code>setuptools&gt;=40.8.0</code> and <code>wheel</code> to their ephemeral build environments and
invoking <code>setuptools.build_meta:__legacy__</code> as the build backend as
implicit defaults if your <code>pyproject.toml</code> lacks a <code>[build-system]</code>. OK.</p>
<h2>Being Explicit About The Build System</h2>
<p>Perhaps I should consider defining <code>[build-system]</code> and being explicit
about things? After all, the tools aren't printing anything indicating they
are assuming implicit defaults and for all I know the defaults could change
in a backwards incompatible manner in any release and break my build. (Although
I would hope to see a deprecation warning before that occurs.)</p>
<p>So I modify my <code>pyproject.toml</code> accordingly:</p>
<div class="pygments_murphy"><pre><span></span><span class="k">[build-system]</span>
<span class="n">requires</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s">&quot;cffi==1.16.0&quot;</span><span class="p">,</span>
    <span class="s">&quot;setuptools==68.2.2&quot;</span><span class="p">,</span>
    <span class="s">&quot;wheel==0.41.2&quot;</span><span class="p">,</span>
<span class="p">]</span>
<span class="n">build-backend</span> <span class="o">=</span> <span class="s">&quot;setuptools.build_meta:__legacy__&quot;</span>
</pre></div>

<p>I pinned all the dependencies to specific versions because I like determinism
and reproducibility. I really don't like when the upload of a new package version
breaks my builds!</p>
<h2>Software Supply Chain Weaknesses in <code>pyproject.toml</code></h2>
<p>When I pinned dependencies in <code>[build-system]</code> in <code>pyproject.toml</code>, the
security part of my brain is screaming over the lack of SHA-256
digest pinning.</p>
<p>How am I sure that we're using well-known, trusted versions of
these dependencies? Are all the transitive dependencies even pinned?</p>
<p>Before <code>pyproject.toml</code>, I used <code>pip-compile</code> from
<a href="https://github.com/jazzband/pip-tools">pip-tools</a> to generate a <code>requirements.txt</code>
containing SHA-256 digests for all transitive dependencies. I would use
<code>python3 -m venv</code> to create a virtualenv,
<code>venv/bin/python -m pip install -r requirements.txt</code> to materialize a (highly
deterministic) set of packages, then run <code>venv/bin/python setup.py</code> to invoke
a build in this stable and securely created environment. (Some) software supply chain
risks averted! But, uh, how do I do that with <code>pyproject.toml</code>
<code>build-system.requires</code>? Does it even support pinning SHA-256 digests?</p>
<p>I skim the PEPs related to <code>pyproject.toml</code> and don't see anything. Surely
I'm missing something.</p>
<p>In desperation I check the pip-tools project and sure enough they
<a href="https://github.com/jazzband/pip-tools#requirements-from-pyprojecttoml">document pyproject.toml integration</a>.
However, they tell you how to feed <code>requirements.txt</code> files into the dynamic
dependencies consumed by the build backend: there's nothing on how to securely
install the build backend itself.</p>
<p>As far as I can tell <code>pyproject.toml</code> has no facilities for securely
installing (read: pinning content digests for all transitive dependencies)
the build backend itself. This is left as an exercise to the reader. But,
um, the build frontend (which I was also instructed to download insecurely
via <code>python -m pip install</code>) is the thing installing the build backend. How am
I supposed to subvert the build frontend to securely install the build backend?
Am I supposed to disable default behavior of using an ephemeral environment
in order to get secure backend installs? Doesn't the ephemeral environment
give me additional, desired protections for build determinism and
reproducibility? That seems <em>wrong</em>.</p>
<p>It kind of looks like <code>pyproject.toml</code> wasn't designed with software supply
chain risk mitigation as a criteria. This is extremely surprising for a build
system abstraction designed in the past few years. I shrug my shoulders and
move on.</p>
<h2>Porting <code>python setup.py develop</code> Invocations</h2>
<p>Now that I figure I have a working <code>pyproject.toml</code>, I move onto removing
<code>python setup.py</code> invocations.</p>
<p>First up is a <code>python setup.py develop --rust-backend</code> invocation.</p>
<p>My <code>setup.py</code> performs <a href="https://github.com/indygreg/python-zstandard/blob/f17569c645618a786ad11a3d51c3baa0b49a311c/setup.py#L65">very crude scanning</a>
of <code>sys.argv</code> looking for command arguments like <code>--system-zstd</code> and
<code>--rust-backend</code> as a way to influence the build. We just sniff these special
arguments and remove them from <code>sys.argv</code> so they don't confuse the setuptools
options parser. (I don't believe this is a blessed way of doing custom options
handling in distutils/setuptools. But it is simple and has worked since I
introduced the pattern in 2016.)</p>
<h2>Is <code>--global-option</code> the Answer?</h2>
<p>With <code>python setup.py</code> invocations going away and a build frontend invoking
<code>setup.py</code>, I need to find an alternative mechanism to pass settings into my
<code>setup.py</code>.</p>
<p><a href="https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html#summary">Why you shouldn't invoke setup.py directly</a>
tells me I should use <code>pip install -e</code>. I'm guessing there's a way to instruct
<code>pip install</code> to pass arguments to <code>setup.py</code>.</p>
<div class="pygments_murphy"><pre><span></span>$ venv/bin/python -m pip install --help
...
  -C, --config-settings &lt;settings&gt;
                              Configuration settings to be passed to the PEP 517 build backend. Settings take the form KEY=VALUE. Use multiple --config-settings options to pass multiple keys to the backend.
  --global-option &lt;options&gt;   Extra global options to be supplied to the setup.py call before the install or bdist_wheel command.
...
</pre></div>

<p>Hmmm. Not really sure which of these to use. But<code>--global-option</code> mentions
<code>setup.py</code> and I'm using <code>setup.py</code>. So I try that:</p>
<div class="pygments_murphy"><pre><span></span>$ venv/bin/python -m pip install --global-option --rust-backend -e .
Usage:
  /Users/gps/src/python-zstandard/venv/bin/python -m pip install [options] &lt;requirement specifier&gt; [package-index-options] ...
  /Users/gps/src/python-zstandard/venv/bin/python -m pip install [options] -r &lt;requirements file&gt; [package-index-options] ...
  /Users/gps/src/python-zstandard/venv/bin/python -m pip install [options] [-e] &lt;vcs project url&gt; ...
  /Users/gps/src/python-zstandard/venv/bin/python -m pip install [options] [-e] &lt;local project path&gt; ...
  /Users/gps/src/python-zstandard/venv/bin/python -m pip install [options] &lt;archive url/path&gt; ...

no such option: --rust-backend
</pre></div>

<p>Oh, duh, <code>--rust-backend</code> looks like an argument and makes pip's own argument
parsing ambiguous as to how to handle it. Let's try that again with
<code>--global-option=--rust-backend</code>:</p>
<div class="pygments_murphy"><pre><span></span>$ venv/bin/python -m pip install --global-option=--rust-backend -e .
DEPRECATION: --build-option and --global-option are deprecated. pip 24.0 will enforce this behaviour change. A possible replacement is to use --config-settings. Discussion can be found at https://github.com/pypa/pip/issues/11859
WARNING: Implying --no-binary=:all: due to the presence of --build-option / --global-option.
Obtaining file:///Users/gps/src/python-zstandard
  Installing build dependencies ... done
  Checking if build backend supports build_editable ... done
  Getting requirements to build editable ... done
  Preparing editable metadata (pyproject.toml) ... done
Building wheels for collected packages: zstandard
  WARNING: Ignoring --global-option when building zstandard using PEP 517
  Building editable for zstandard (pyproject.toml) ... done
  Created wheel for zstandard: filename=zstandard-0.22.0.dev0-0.editable-cp312-cp312-macosx_14_0_x86_64.whl size=4379 sha256=05669b0a5fd8951cac711923d687d9d4192f6a70a8268dca31bdf39012b140c8
  Stored in directory: /private/var/folders/dd/xb3jz0tj133_hgnvdttctwxc0000gn/T/pip-ephem-wheel-cache-6amdpg21/wheels/eb/6b/3e/89aae0b17b638c9cdcd2015d98b85ee7fb3ef00325bb44a572
Successfully built zstandard
Installing collected packages: zstandard
Successfully installed zstandard-0.22.0.dev0
</pre></div>

<p>I immediately see the three <code>DEPRECATION</code> and <code>WARNING</code> lines (which are color
highlighted in my terminal, yay):</p>
<div class="pygments_murphy"><pre><span></span>DEPRECATION: --build-option and --global-option are deprecated. pip 24.0 will enforce this behaviour change. A possible replacement is to use --config-settings. Discussion can be found at https://github.com/pypa/pip/issues/11859
WARNING: Implying --no-binary=:all: due to the presence of --build-option / --global-option.
WARNING: Ignoring --global-option when building zstandard using PEP 517
</pre></div>

<p>Yikes. It looks like <code>--global-option</code> is deprecated and will be removed in pip 24.0.
And, later it says <code>--global-option</code> was ignored. Is that true?!</p>
<div class="pygments_murphy"><pre><span></span>$ ls -al zstandard/*cpython-312*.so
-rwxr-xr-x  1 gps  staff  1002680 Oct 27 11:35 zstandard/_cffi.cpython-312-darwin.so
-rwxr-xr-x  1 gps  staff   919352 Oct 27 11:35 zstandard/backend_c.cpython-312-darwin.so
</pre></div>

<p>Not seeing a <code>backend_rust</code> library like I was expecting. So, yes, it does look
like <code>--global-option</code> was ignored.</p>
<p>This behavior is actually pretty concerning to me. It certainly
seems like at one time <code>--global-option</code> (and a <code>--build-option</code> which doesn't
exist on the <code>pip install</code> command I guess) did get threaded through to <code>setup.py</code>.
However, it no longer does.</p>
<p>I find an entry in the <a href="https://pip.pypa.io/en/stable/news/#v23-1">pip 23.1 changelog</a>:
<code>Deprecate --build-option and --global-option. Users are invited to switch
to --config-settings. (#11859)</code>. <em>Deprecate</em>. What is pip's definition of
<em>deprecate</em>? I click the link to <a href="https://github.com/pypa/pip/pull/11859">#11859</a>.
An open issue with a lot of comments. I scan the issue history to find
referenced PRs and click on <a href="https://github.com/pypa/pip/pull/11861">#11861</a>.
OK, it is just an advertisement. Maybe <code>--global-option</code> never got threaded
through to <code>setup.py</code>? But its help usage text clearly says it is related to
<code>setup.py</code>! Maybe the presence of <code>[build-system]</code> in <code>pyproject.toml</code> is
somehow engaging different semantics that result in <code>--global-option</code> not
being passed to <code>setup.py</code>? The warning message did say
<code>Ignoring --global-option when building zstandard using PEP 517</code>.</p>
<p>I try commenting out the <code>[build-system]</code> section in my <code>pyproject.toml</code>
and trying again. Same result. Huh? Reading the <code>pip install --help</code> output,
I see <code>--no-use-pep517</code> and try it:</p>
<div class="pygments_murphy"><pre><span></span>$ venv/bin/python -m pip install --global-option=--rust-backend --no-use-pep517 -e .
...
$ ls -al zstandard/*cpython-312*.so
-rwxr-xr-x  1 gps  staff  1002680 Oct 27 11:35 zstandard/_cffi.cpython-312-darwin.so
-rwxr-xr-x  1 gps  staff   919352 Oct 27 11:35 zstandard/backend_c.cpython-312-darwin.so
-rwxr-xr-x  1 gps  staff  2727920 Oct 27 11:53 zstandard/backend_rust.cpython-312-darwin.so
</pre></div>

<p>Ahh, so pip's default PEP-517 build mode is causing <code>--global-option</code> to get
ignored. So I guess older versions of pip honored <code>--global-option</code> and when
pip switched to PEP-517 build mode by default <code>--global-option</code> just stopped
working and emitted a warning instead. That's quite the backwards incompatible
behavior break! I really wish tools would fail fast when making these kinds of
breaks or at least offer a <code>--warnings-as-errors</code> mode so I can opt into fatal
errors when these kinds of breaks / deprecations are introduced. I would 100%
opt into this since these warnings are often the figurative needle in a haystack
of CI logs and easy to miss. Especially if the build environment is
non-deterministic and new versions of tools like pip get installed <em>randomly</em>
without a version control commit.</p>
<p>Pip's allowing me to specify <code>--global-option</code> but then only issuing a
warning when it is ignored doesn't sit well with me. But what can I do?</p>
<p>It is obvious <code>--global-option</code> is a non-starter here.</p>
<h2>Attempts at Using <code>--config-setting</code></h2>
<p>Fortunately, pip's deprecation message suggests a path forward:</p>
<div class="pygments_murphy"><pre><span></span>A possible replacement is to use --config-settings. Discussion can be found
at https://github.com/pypa/pip/issues/11859
</pre></div>

<p>First, kudos for actionable warning messages. However, the wording says
<em>possible replacement</em>. Are there other alternatives I didn't see in the
<code>pip install --help</code> output?</p>
<p>Anyway, I decide to go with that <code>--config-settings</code> suggestion.</p>
<div class="pygments_murphy"><pre><span></span>$ venv/bin/python -m pip install --config-settings=--rust-backend -e .

Usage:
  /Users/gps/src/python-zstandard/venv/bin/python -m pip install [options] &lt;requirement specifier&gt; [package-index-options] ...
  /Users/gps/src/python-zstandard/venv/bin/python -m pip install [options] -r &lt;requirements file&gt; [package-index-options] ...
  /Users/gps/src/python-zstandard/venv/bin/python -m pip install [options] [-e] &lt;vcs project url&gt; ...
  /Users/gps/src/python-zstandard/venv/bin/python -m pip install [options] [-e] &lt;local project path&gt; ...
  /Users/gps/src/python-zstandard/venv/bin/python -m pip install [options] &lt;archive url/path&gt; ...

Arguments to --config-settings must be of the form KEY=VAL
</pre></div>

<p>Hmmm. Let's try adding a trailing <code>=</code>?</p>
<div class="pygments_murphy"><pre><span></span>$ venv/bin/python -m pip install --config-settings=--rust-backend= -e .
Obtaining file:///Users/gps/src/python-zstandard
  Installing build dependencies ... done
  Checking if build backend supports build_editable ... done
  Getting requirements to build editable ... done
  Preparing editable metadata (pyproject.toml) ... done
Building wheels for collected packages: zstandard
  Building editable for zstandard (pyproject.toml) ... done
  Created wheel for zstandard: filename=zstandard-0.22.0.dev0-0.editable-cp312-cp312-macosx_14_0_x86_64.whl size=4379 sha256=619db9806bc4c39e973c3197a0ddb9b03b49fff53cd9ac3d7df301318d390b5e
  Stored in directory: /private/var/folders/dd/xb3jz0tj133_hgnvdttctwxc0000gn/T/pip-ephem-wheel-cache-gtsvw78d/wheels/eb/6b/3e/89aae0b17b638c9cdcd2015d98b85ee7fb3ef00325bb44a572
Successfully built zstandard
Installing collected packages: zstandard
  Attempting uninstall: zstandard
    Found existing installation: zstandard 0.22.0.dev0
    Uninstalling zstandard-0.22.0.dev0:
      Successfully uninstalled zstandard-0.22.0.dev0
Successfully installed zstandard-0.22.0.dev0
</pre></div>

<p>No warnings or deprecations. That's promising. Did it work?</p>
<div class="pygments_murphy"><pre><span></span>$ ls -al zstandard/*cpython-312*.so
-rwxr-xr-x  1 gps  staff  1002680 Oct 27 12:11 zstandard/_cffi.cpython-312-darwin.so
-rwxr-xr-x  1 gps  staff   919352 Oct 27 12:11 zstandard/backend_c.cpython-312-darwin.so
</pre></div>

<p>No <code>backend_rust</code> extension module. Boo. So what actually happened?</p>
<div class="pygments_murphy"><pre><span></span>$ venv/bin/python -m pip -v install --config-settings=--rust-backend= -e .
</pre></div>

<p>I don't see <code>--rust-backend</code> anywhere in that log output. I try with
more verbosity:</p>
<div class="pygments_murphy"><pre><span></span>$ venv/bin/python -m pip -vvvvv install --config-settings=--rust-backend= -e .
</pre></div>

<p>Still nothing!</p>
<p>Maybe That <code>--</code> prefix is wrong?</p>
<div class="pygments_murphy"><pre><span></span>$ venv/bin/python -m pip -vvvvv install --config-settings=rust-backend= -e .
</pre></div>

<p>Still nothing!</p>
<p>I have no clue how <code>--config-settings=</code> is getting passed
to <code>setup.py</code> nor where it is seemingly getting dropped on the floor.</p>
<h2>How Does setuptools Handle <code>--config-settings</code>?</h2>
<p>This must be documented in the setuptools project. So I open those docs in
my web browser and do a <a href="https://setuptools.pypa.io/en/latest/search.html?q=settings&amp;check_keywords=yes&amp;area=default">search for settings</a>.
I open the first three results in separate tabs:</p>
<ol>
<li><a href="https://setuptools.pypa.io/en/latest/deprecated/commands.html?highlight=settings">Running setuptools commands</a></li>
<li><a href="https://setuptools.pypa.io/en/latest/deprecated/commands.html?highlight=settings#configuration-file-options">Configuration File Options</a></li>
<li><a href="https://setuptools.pypa.io/en/latest/deprecated/commands.html?highlight=settings#develop-deploy-the-project-source-in-development-mode">develop - Deploy the project source in "Development Mode"</a></li>
</ol>
<p>That first link has docs on the deprecated setuptools commands and how to
invoke <code>python setup.py</code> directly. (Note: there is a warning box here saying
that <code>python setup.py</code> is deprecated. I guess I somehow missed this document
when looking at setuptools documentation earlier! In hindsight, it appears to
be buried at the figurative bottom of the docs tree as the last item under
a <code>Backward compatibility &amp; deprecated practice section</code>. Talk about burying
the lede!) These docs aren't useful.</p>
<p>The second link also takes me to deprecated documentation related to direct
<code>python setup.py</code> command invocations.</p>
<p>The third link is also useless.</p>
<p>I continue opening search results in new tabs. Surely the answer is in here.</p>
<p>I find an <a href="https://setuptools.pypa.io/en/latest/userguide/extension.html#adding-arguments">Adding Arguments</a>
section telling me that <code>Adding arguments to setup is discouraged as such
arguments are only supported through imperative execution and not supported
through declarative config.</code>. I <em>think</em> that's an obtuse of saying that
<code>sys.argv</code> arguments are only supported via <code>python setup.py</code> invocations
and not via <code>setup.cfg</code> or <code>pyproject.toml</code>? But the example only shows me
how to use <code>setup.cfg</code> and doesn't have any mention of <code>pyproject.toml</code>. So
is this documentation even relevant to <code>pyproject.toml</code>?</p>
<p>Eventually I stumble across
<a href="https://setuptools.pypa.io/en/latest/build_meta.html">Build System Support</a>.
In the <a href="https://setuptools.pypa.io/en/latest/build_meta.html#dynamic-build-dependencies-and-other-build-meta-tweaks">Dynamic build dependencies and other build_meta tweaks</a>
section, I notice the following example code:</p>
<div class="pygments_murphy"><pre><span></span><span class="kn">from</span> <span class="nn">setuptools</span> <span class="kn">import</span> <span class="n">build_meta</span> <span class="k">as</span> <span class="n">_orig</span>
<span class="kn">from</span> <span class="nn">setuptools.build_meta</span> <span class="kn">import</span> <span class="o">*</span>

<span class="k">def</span> <span class="nf">get_requires_for_build_wheel</span><span class="p">(</span><span class="n">config_settings</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_orig</span><span class="o">.</span><span class="n">get_requires_for_build_wheel</span><span class="p">(</span><span class="n">config_settings</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="o">...</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">get_requires_for_build_sdist</span><span class="p">(</span><span class="n">config_settings</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_orig</span><span class="o">.</span><span class="n">get_requires_for_build_sdist</span><span class="p">(</span><span class="n">config_settings</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="o">...</span><span class="p">]</span>
</pre></div>

<p><code>config_settings=None</code>. OK, this might be the <code>--config-settings</code>
values passed to the build frontend getting fed into the build backend.
I Google <code>get_requires_for_build_wheel</code>. One of the top results is
<a href="https://peps.python.org/pep-0517/">PEP-517</a>, which I click on.</p>
<p>I see that the <a href="https://peps.python.org/pep-0517/#build-backend-interface">Build backend interface</a>
consists of a handful of functions that are invoked by the build frontend.
These functions all seem to take a <code>config_settings=None</code> argument. Great,
now I know the interface between build frontends and backends at the Python
API level. Where was I in this yak shave?</p>
<p>I remember from <code>pyproject.toml</code> that one of the lines is
<code>build-backend = "setuptools.build_meta:__legacy__"</code>. That
<code>setuptools.build_meta:__legacy__</code> bit looks like a Python symbol reference.
Since the setuptools documentation didn't answer my question on how to
thread <code>--config-settings</code> into <code>setup.py</code> invocations, I
<a href="https://github.com/pypa/setuptools/blob/2384d915088b960999ca74fb81ce70bffd17b082/setuptools/build_meta.py">open the build_meta.py source code</a>.
(Aside: experience has taught me that when in doubt on how something works,
consult the source code: code doesn't lie.)</p>
<p>I search for <code>config_settings</code>. I immediately see
<code>class _ConfigSettingsTranslator:</code> whose purported job is
<code>Translate config_settings into distutils-style command arguments.
Only a limited number of options is currently supported.</code> Oh, this looks
relevant. But there's a fair bit of code in here. Do I really need to grok
it all? I keep scanning the source.</p>
<p>In a <code>def _build_with_temp_dir()</code> I spot the following code:</p>
<div class="pygments_murphy"><pre><span></span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span> <span class="o">=</span> <span class="p">[</span>
    <span class="o">*</span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">[:</span><span class="mi">1</span><span class="p">],</span>
    <span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">_global_args</span><span class="p">(</span><span class="n">config_settings</span><span class="p">),</span>
    <span class="o">*</span><span class="n">setup_command</span><span class="p">,</span>
    <span class="s2">&quot;--dist-dir&quot;</span><span class="p">,</span>
    <span class="n">tmp_dist_dir</span><span class="p">,</span>
    <span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">_arbitrary_args</span><span class="p">(</span><span class="n">config_settings</span><span class="p">),</span>
<span class="p">]</span>
</pre></div>

<p>Ahh, cool. It looks to be calling <code>self._global_args()</code> and
<code>self._arbitrary_args()</code> and adding the arguments those functions return
to <code>sys.argv</code> before evaluating <code>setup.py</code> in the current interpreter.</p>
<p>I look at the definition of <code>_arbitrary_args()</code> and I'm onto something:</p>
<div class="pygments_murphy"><pre><span></span><span class="k">def</span> <span class="nf">_arbitrary_args</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config_settings</span><span class="p">:</span> <span class="n">_ConfigSettings</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
  <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">  Users may expect to pass arbitrary lists of arguments to a command</span>
<span class="sd">  via &quot;--global-option&quot; (example provided in PEP 517 of a &quot;escape hatch&quot;).</span>
<span class="sd">  ...</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">args</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_config</span><span class="p">(</span><span class="s2">&quot;--global-option&quot;</span><span class="p">,</span> <span class="n">config_settings</span><span class="p">)</span>
  <span class="n">global_opts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_valid_global_options</span><span class="p">()</span>
  <span class="n">bad_args</span> <span class="o">=</span> <span class="p">[]</span>

  <span class="k">for</span> <span class="n">arg</span> <span class="ow">in</span> <span class="n">args</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">arg</span><span class="o">.</span><span class="n">strip</span><span class="p">(</span><span class="s2">&quot;-&quot;</span><span class="p">)</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">global_opts</span><span class="p">:</span>
          <span class="n">bad_args</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">arg</span><span class="p">)</span>
          <span class="k">yield</span> <span class="n">arg</span>

  <span class="k">yield from</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_config</span><span class="p">(</span><span class="s2">&quot;--build-option&quot;</span><span class="p">,</span> <span class="n">config_settings</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">bad_args</span><span class="p">:</span>
      <span class="n">SetuptoolsDeprecationWarning</span><span class="o">.</span><span class="n">emit</span><span class="p">(</span>
          <span class="s2">&quot;Incompatible `config_settings` passed to build backend.&quot;</span><span class="p">,</span>
          <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">          The arguments </span><span class="si">{bad_args!r}</span><span class="s2"> were given via `--global-option`.</span>
<span class="s2">          Please use `--build-option` instead,</span>
<span class="s2">          `--global-option` is reserved for flags like `--verbose` or `--quiet`.</span>
<span class="s2">          &quot;&quot;&quot;</span><span class="p">,</span>
          <span class="n">due_date</span><span class="o">=</span><span class="p">(</span><span class="mi">2023</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">26</span><span class="p">),</span>  <span class="c1"># Warning introduced in v64.0.1, 11/Aug/2022.</span>
      <span class="p">)</span>
</pre></div>

<p>It looks to peek inside <code>config_settings</code> and handle <code>--global-option</code>
and <code>--build-option</code> specially. But we clearly see <code>--global-option</code> is
deprecated in favor of <code>--build-option</code>.</p>
<p>So is the <code>--config-settings</code> key name <code>--build-option</code> and its value
the <code>setup.py</code> argument we want to insert?</p>
<p>I try that:</p>
<div class="pygments_murphy"><pre><span></span>$ venv/bin/python -m pip install --config-settings=--build-option=--rust-backend -e .
...
$ ls -al zstandard/*cpython-312*.so
-rwxr-xr-x  1 gps  staff  1002680 Oct 27 12:54 zstandard/_cffi.cpython-312-darwin.so
-rwxr-xr-x  1 gps  staff   919352 Oct 27 12:53 zstandard/backend_c.cpython-312-darwin.so
-rwxr-xr-x  1 gps  staff  2727920 Oct 27 12:54 zstandard/backend_rust.cpython-312-darwin.so
</pre></div>

<p>It worked!</p>
<h2>Disbelief Over <code>--config-settings=--build-option=</code></h2>
<p>But, um, <code>--config-settings=--build-option=--rust-backend</code>. We've triple encoded
command arguments here. This feels exceptionally weird. Is that really the
supported/preferred interface? Surely there's something simpler.</p>
<p><code>def _arbitrary_args()</code>'s docstring mentioned <em>escape hatch</em> in the context
of PEP-517. I open PEP-517 and search for that term, finding
<a href="https://peps.python.org/pep-0517/#config-settings">Config settings</a>. Sure
enough, it is describing the mechanism I just saw the source code to. And its
pip example is using <code>pip install</code>'s <code>--global-option</code> and <code>--build-option</code>
arguments. So this all seems to check out. (Although these pip arguments are
deprecated in favor of <code>-C/--config-settings</code>.)</p>
<p>Thinking I missed some obvious documentation, I search the setuptools
documentation for <a href="https://setuptools.pypa.io/en/latest/search.html?q=%22--build-option%22&amp;check_keywords=yes&amp;area=default#">--build-option</a>.
The only hits are in the <a href="https://setuptools.pypa.io/en/latest/history.html#v64-0-0">v64.0.0 changelog entry</a>.
So you are telling me this feature of passing arbitrary config settings into
<code>setup.py</code> via PEP-517 build frontends is only documented in the <em>changelog</em>?!</p>
<p>Ok, I know my <code>setup.py</code> is abusing <code>sys.argv</code>. I'm off the paved road for
passing settings into <code>setup.py</code>. What is the preferred <code>pyproject.toml</code>
era mechanism for passing settings into <code>setup.py</code>? These settings can't
be file based because they are dynamic. There must be a <code>config_settings</code>
mechanism to thread dynamic settings into <code>setup.py</code> that doesn't rely on
these magical <code>--build-option</code> and <code>--global-option</code> settings keys.</p>
<p>I stare and stare at the
<a href="https://github.com/pypa/setuptools/blob/2384d915088b960999ca74fb81ce70bffd17b082/setuptools/build_meta.py">build_meta.py source code</a>
looking for find an answer. But all I see is the <code>def _build_with_temp_dir()</code>
calling into <code>self._global_args()</code> and <code>self._arbitrary_args()</code> to append
arguments to <code>sys.argv</code>. Huh? Surely this isn't the only solution. Surely
there's a simpler way. The setuptools documentation said <em>Adding arguments
to setup is discouraged</em>, seemingly implying a better way of doing it. And
yet the only code I'm seeing in <code>build_meta.py</code> for passing custom
<code>config_settings</code> values in is literally via additional <code>setup.py</code> process
arguments. This can't be right.</p>
<p>I start unwinding my mental stack and browser tabs trying to come across
something I missed.</p>
<p>I again look at
<a href="https://setuptools.pypa.io/en/latest/build_meta.html#dynamic-build-dependencies-and-other-build-meta-tweaks">Dynamic build dependencies and other build_meta tweaks</a>
and see its code is defining a custom <code>[build-system]</code> backend that
does a <code>from setuptools.build_meta import *</code> and defines some custom
build backend interface APIs (which receive <code>config_settings</code>) and then
proxy into the original implementations. While the example is related to
build metadata, I'm thinking <em>do I need to implement my own setuptools
wrapping build backend that implements a custom
<a href="https://peps.python.org/pep-0517/#build-wheel">def build_wheel()</a> to
intercept <code>config_settings</code></em>? Surely this is avoidable complexity.</p>
<h2>Pip's Eager Deprecations</h2>
<p>I keep unwinding context and again notice pip's warning message telling
me <em>A possible replacement is to use <code>--config-settings</code>. Discussion can
be found at https://github.com/pypa/pip/issues/11859</em>.</p>
<p>I open <a href="https://github.com/pypa/pip/issues/11859">pip issue #11859</a>.
Oh, that's the same issue tracking the <code>--global-option</code> deprecation I
encountered earlier. I again scan the issue timeline. It is mostly
references from other GitHub projects. Telltale sign that this
deprecation is creating waves.</p>
<p>The issue is surprisingly light on comments for how many references it
has.</p>
<p>The comment with
<a href="https://github.com/pypa/pip/issues/11859#issuecomment-1664649395">the most emoji reactions</a>
says:</p>
<div class="pygments_murphy"><pre><span></span>Is there an example showing how to use --config-settings with setup.py
and/or newer alternatives? The setuptools documentation is awful and the
top search results are years/decades out-of-date and wildly contradictory.`
</pre></div>

<p>I don't know who you are, @alexchandel, but we're on the same wavelength.</p>
<p>Then the next comment says:</p>
<div class="pygments_murphy"><pre><span></span>Something like this seems to work to pass global options to setuptools.

pip -vv install   --config-setting=&quot;--global-option=--verbose&quot;  .

Passing --build-option in the same way does not work, as setuptools
attempts to pass these to the egg_info command where they are not supported.
</pre></div>

<p>So there it seemingly is, confirmation that my independently derived
solution of <code>--config-settings=--build-option=-...</code> is in fact the way to
go. But this commenter says to use <code>--global-option</code>, which appears to
be deprecated in modern setuptools. Oof.</p>
<p>The next comment links to <a href="https://github.com/pypa/setuptools/issues/3896">pypa/setuptools#3896</a>
where apparently there's been an ongoing conversation since April about how
setuptools should <em>design and document a stable mechanism to pass <code>config_settings</code>
to PEP517 backend</em>.</p>
<p>If I'm interpreting this correctly, it looks like distutils/setuptools - the
primary way to define Python packages for the better part of twenty years -
doesn't have a stable mechanism for passing configuration settings from
modern <code>pyproject.toml</code> <code>[build-system]</code> frontends. Meanwhile pip is deprecating
long-working mechanisms to pass options to <code>setup.py</code> and forcing people to
use a mechanism that setuptools doesn't explicitly document much less say is
stable. This is all taking place <a href="https://mail.python.org/pipermail/distutils-sig/2017-September/031548.html">six years</a>
after PEP-517 was accepted.</p>
<p>I'm kind of at a loss for words here. I understand pip's desire to delete some
legacy code and standardize on the new way of doing things. But it really looks
like they are breaking backwards compatibility for <code>setup.py</code> a bit too
eagerly. That's a <em>questionable</em> decision in my mind, so I
<a href="https://github.com/pypa/pip/issues/11859#issuecomment-1778620671">write a detailed comment on the pip issue</a>
explaining how the interface works and asking the pip folks to hold off on
deprecation until setuptools has a stable, documented solution. Time will
tell what happens.</p>
<h2>In Summary</h2>
<p>What an adventure that Python packaging yak shave was! I feel
like I just learned a whole lot of things that I shouldn't have needed to learn
in order to keep my Python package building without deprecation warnings.
Yes, I scope bloated myself to understanding how things worked because
that's my ethos. But even without that extra work, there's a lot here that I
feel I shouldn't have needed to do, like figure out the undocumented
<code>--config-settings=--build-option=</code> interface.</p>
<p>Despite having ported my <code>python setup.py</code> invocation to modern, PEP-517 build
frontends (<code>build</code> and <code>pip</code>) and gotten rid of various deprecation messages
and warnings, I'm still not sure the implications of that transition. I really
want to understand the trade-offs for adopting <code>pyproject.toml</code> and using the
modern build frontends for doing things. But I couldn't find any documentation
on this anywhere! I don't know basic things like whether my adoption of
<code>pyproject.toml</code> will break end-users stuck on older Python versions or what.
I still haven't ported my project metadata from <code>setup.py</code> to <code>pyproject.toml</code>
because I don't understand the implications. I feel like I'm flying blind and
am bound to make mistakes with undesirable impacts to end-users of my package.</p>
<p>But at least I was able to remove deprecation warnings from my packaging CI
with <em>just</em> several hours of work.</p>
<p>I recognize this post is light on constructive feedback and suggestions
for how to improve matters.</p>
<p>One reason is that I think a lot of the improvements are self-explanatory -
clearer warning messages, better documentation, not deprecating things
prematurely, etc. I prefer to just submit PRs instead of long blog posts. But
I just don't know what is appropriate in some cases: one of the themes of this
post is I just don't grok the state of Python packaging right now.</p>
<p>This post did initially contain a few thousand words expanding on what all I
thought was broken and how it should be fixed. But I stripped the content
because I didn't want my (likely controversial) opinions to distract from
the self-assessed user experience study documented in this post. This content
is probably better posted to a PyPA mailing list anyway, otherwise I'm just
another guy complaining on the Internet.</p>
<p>I've <a href="https://discuss.python.org/t/user-experience-with-porting-off-setup-py/37502">posted a link</a>
to this post to the
<a href="https://discuss.python.org/c/packaging/14">packaging category</a> on
<a href="https://discuss.python.org">discuss.python.org</a> so the PyPA (and other
subscribed parties) are aware of all the issues I stumbled over. Hopefully
people with more knowledge of the state of Python packaging see this post,
empathize with my struggles, and enact meaningful improvements so others
can port off <code>setup.py</code> with a fraction of the effort as it took me.</p>]]></content>
  </entry>
  <entry>
    <author>
      <name>Gregory Szorc</name>
      <uri>http://gregoryszorc.com/blog</uri>
    </author>
    <title type="html"><![CDATA[Achieving A Completely Open Source Implementation of Apple Code Signing and Notarization]]></title>
    <link rel="alternate" type="text/html" href="http://gregoryszorc.com/blog/2022/08/08/achieving-a-completely-open-source-implementation-of-apple-code-signing-and-notarization" />
    <id>http://gregoryszorc.com/blog/2022/08/08/achieving-a-completely-open-source-implementation-of-apple-code-signing-and-notarization</id>
    <updated>2022-08-08T08:08:08Z</updated>
    <published>2022-08-08T08:08:08Z</published>
    <category scheme="http://gregoryszorc.com/blog" term="Apple" />
    <category scheme="http://gregoryszorc.com/blog" term="Rust" />
    <summary type="html"><![CDATA[Achieving A Completely Open Source Implementation of Apple Code Signing and Notarization]]></summary>
    <content type="html" xml:base="http://gregoryszorc.com/blog/2022/08/08/achieving-a-completely-open-source-implementation-of-apple-code-signing-and-notarization"><![CDATA[<p>As I've previously blogged in
<a href="/blog/2021/04/14/pure-rust-implementation-of-apple-code-signing/">Pure Rust Implementation of Apple Code Signing</a>
(2021-04-14) and
<a href="/blog/2022/04/25/expanding-apple-ecosystem-access-with-open-source,-multi-platform-code-signing/">Expanding Apple Ecosystem Access with Open Source, Multi Platform Code signing</a>
(2022-04-25), I've been hacking on an open source implementation of Apple code
signing and notarization using the Rust programming language. This takes the form
of the <code>apple-codesign</code> crate / library and its <code>rcodesign</code> CLI executable.
(<a href="https://gregoryszorc.com/docs/apple-codesign/stable/">Documentation</a> /
<a href="https://github.com/indygreg/apple-platform-rs/tree/main/apple-codesign">GitHub project</a> /
<a href="https://crates.io/crates/apple-codesign">crates.io</a>).</p>
<p>As of that most recent post in April, I was pretty happy with the relative
stability of the implementation: we were able to sign, notarize, and staple
Mach-O binaries, directory bundles (<code>.app</code>, <code>.framework</code> bundles, etc), XAR
archives / flat packages / <code>.pkg</code> installers, and DMG disk images. Except for
the <a href="https://gregoryszorc.com/docs/apple-codesign/0.17.0/apple_codesign_quirks.html">known limitations</a>,
if Apple's official <code>codesign</code> and <code>notarytool</code> tools support it, so do we.
<strong>This allows people to sign, notarize, and release Apple software from non-Apple
operating systems like Linux and Windows.</strong> This opens up new avenues for
Apple platform access.</p>
<p>A major limitation in previous versions of the <code>apple-codesign</code> crate was our
reliance on Apple's <a href="https://help.apple.com/itc/transporteruserguide/">Transporter</a>
tool for notarization. Transporter is a Java application made available for macOS,
Linux, and Windows that speaks to Apple's servers and can upload assets to their
notarization service. I used this tool at the time because it seemed to
be officially supported by Apple and the path of least resistance to standing
up notarization. But Transporter was a bit wonky to use and an extra
dependency that you needed to install.</p>
<p>At WWDC 2022, Apple <a href="https://developer.apple.com/videos/play/wwdc2022/10109/">announced</a>
a new <a href="https://developer.apple.com/documentation/notaryapi">Notary API</a> as
part of the App Store Connect API. In what felt like a wink directly at me,
Apple themselves even calls out the possibility for leveraging this API to
notarize from Linux! I knew as soon as I saw this that it was only a matter
of time before I would be able to replace Transporter with a pure Rust client
for the new HTTP API. (I was already thinking about using the unpublished HTTP
API that <code>notarytool</code> uses. And from the limited reversing notes I have from
before WWDC it looks like the new official Notary API is very similar - possibly
identical to - what <code>notarytool</code> uses. So kudos to Apple for opening up this
access!)</p>
<p><strong>I'm very excited to announce that we now have a pure Rust implementation
of a client for Apple's Notary API in the <code>apple-codesign</code> crate. This means we
can now notarize Apple software from any machine where you can get the Rust
crate to compile. This means we no longer have a dependency on the 3rd party
Apple Transporter application. Notarization, like code signing, is 100% open
source Rust code.</strong></p>
<p>As excited as I am to announce this new feature, <strong>I'm even more excited that
it was largely implemented by a contributor, Robin Lambertz /
<a href="https://github.com/roblabla">@roblabla</a>!</strong> They
<a href="https://github.com/indygreg/PyOxidizer/issues/591">filed a GitHub feature request</a>
while WWDC 2022 was still ongoing and then <a href="https://github.com/indygreg/PyOxidizer/pull/593">submitted a PR</a>
a few days later. It took me a few months to get around to reviewing it
(I try to avoid computer screens during summers), but it was a fantastic
PR given the scope of the change. It never ceases to bring joy to me when
someone randomly contributes greatness to open source.</p>
<p>So, as of the just-released <a href="https://github.com/indygreg/PyOxidizer/releases/tag/apple-codesign%2F0.17.0">0.17 release</a>
of the <code>apple-codesign</code> Rust crate and its corresponding <code>rcodesign</code> CLI tool, you can now
<code>rcodesign notary-submit</code> to speak to Apple's Notary API using a pure Rust client. No
more requirements on 3rd party, proprietary software. All you need to sign and
notarize Apple applications is the self-contained <code>rcodesign</code> executable and a Linux,
Windows, macOS, BSD, etc machine to run it on.</p>
<p>I'm stoked to finally achieve this milestone! There are probably thousands of
companies and individuals who have wanted to release Apple software from
non-macOS operating systems. (The existence and popularity of tools like
<a href="https://fastlane.tools/">fastlane</a> seems to confirm this.) The historical
lack of an Apple code signing and notarization solution that worked outside
macOS has prevented this. Well, that barrier has officially fallen.</p>
<p>Release notes, documentation, and (self-signed) pre-built executables of the
<code>rcodesign</code> executable for major platforms are available on the
<a href="https://github.com/indygreg/PyOxidizer/releases/tag/apple-codesign%2F0.17.0">0.17 release page</a>.</p>]]></content>
  </entry>
  <entry>
    <author>
      <name>Gregory Szorc</name>
      <uri>http://gregoryszorc.com/blog</uri>
    </author>
    <title type="html"><![CDATA[Announcing the PyOxy Python Runner]]></title>
    <link rel="alternate" type="text/html" href="http://gregoryszorc.com/blog/2022/05/10/announcing-the-pyoxy-python-runner" />
    <id>http://gregoryszorc.com/blog/2022/05/10/announcing-the-pyoxy-python-runner</id>
    <updated>2022-05-10T08:00:00Z</updated>
    <published>2022-05-10T08:00:00Z</published>
    <category scheme="http://gregoryszorc.com/blog" term="Python" />
    <category scheme="http://gregoryszorc.com/blog" term="PyOxidizer" />
    <summary type="html"><![CDATA[Announcing the PyOxy Python Runner]]></summary>
    <content type="html" xml:base="http://gregoryszorc.com/blog/2022/05/10/announcing-the-pyoxy-python-runner"><![CDATA[<p>I'm pleased to announce the initial release of
<a href="https://pyoxidizer.readthedocs.io/en/latest/pyoxy.html">PyOxy</a>.
Binaries are <a href="https://github.com/indygreg/PyOxidizer/releases/tag/pyoxy%2F0.1.0">available on GitHub</a>.</p>
<p>(Yes, I used my <a href="/blog/2022/04/25/expanding-apple-ecosystem-access-with-open-source,-multi-platform-code-signing/">pure Rust Apple code signing implementation</a>
to remotely sign the macOS binaries from GitHub Actions using a YubiKey
plugged into my Windows desktop: that experience still feels magical
to me.)</p>
<p>PyOxy is all of the following:</p>
<ul>
<li>An executable program used for running Python interpreters.</li>
<li>A single file and highly portable (C)Python distribution.</li>
<li>An alternative <code>python</code> driver providing more control over the
  interpreter than what <code>python</code> itself provides.</li>
<li>A way to make some of PyOxidizer's technology more broadly available without
  using PyOxidizer.</li>
</ul>
<p>Read the following sections for more details.</p>
<h2><code>pyoxy</code> Acts Like <code>python</code></h2>
<p>The <code>pyoxy</code> executable has a <code>run-python</code> sub-command that will essentially
do what <code>python</code> would do:</p>
<pre><code>$ pyoxy run-python
Python 3.9.12 (main, May  3 2022, 03:29:54)
[Clang 14.0.3 ] on darwin
Type "help", "copyright", "credits" or "license" for more information.
&gt;&gt;&gt;
</code></pre>
<p>A Python REPL. That's familiar!</p>
<p>You can even pass <code>python</code> arguments to it:</p>
<pre><code>$ pyoxy run-python -- -c 'print("hello, world")'
hello, world
</code></pre>
<p>When a <code>pyoxy</code> executable is renamed to any filename beginning with <code>python</code>,
it implicitly behaves like <code>pyoxy run-python --</code>.</p>
<pre><code>$ mv pyoxy python3.9
$ ls -al python3.9
-rwxrwxr-x  1 gps gps 120868856 May 10  2022 python3.9

$ ./python3.9
Python 3.9.12 (main, May  3 2022, 03:29:54)
[Clang 14.0.3 ] on darwin
Type "help", "copyright", "credits" or "license" for more information.
&gt;&gt;&gt;
</code></pre>
<h2>Single File Python Distributions</h2>
<p>The official <code>pyoxy</code> executables are built with PyOxidizer and leverage the
Python distributions provided by my
<a href="https://github.com/indygreg/python-build-standalone/">python-build-standalone</a>
project. On Linux and macOS, a fully featured Python interpreter and its library
dependencies are statically linked into <code>pyoxy</code>. The <code>pyoxy</code> executable also embeds
a copy of the Python standard library and imports it from memory using the
<a href="https://pyoxidizer.readthedocs.io/en/latest/oxidized_importer.html">oxidized_importer</a>
Python extension module.</p>
<p>What this all means is that the official <code>pyoxy</code> executables can function as
single file CPython distributions! Just download a <code>pyoxy</code> executable, rename it
to <code>python</code>, <code>python3</code>, <code>python3.9</code>, etc and it should behave just like a normal
<code>python</code> would!</p>
<p>Your Python installation has never been so simple. And fast: <code>pyoxy</code> should be
a few milliseconds faster to initialize a Python interpreter mostly because of
<code>oxidized_importer</code> and it avoiding filesystem overhead to look for and load
<code>.py[c]</code> files.</p>
<h2>Low-Level Control Over the Python Interpreter with YAML</h2>
<p>The <code>pyoxy run-yaml</code> command is takes the path to a YAML file defining the
embedded Python interpreter configuration and then launches that Python
interpreter in-process:</p>
<pre><code>$ cat &gt; hello_world.yaml &lt;&lt;EOF
---
allocator_debug: true
interpreter_config:
  run_command: 'print("hello, world")'
...
EOF

$ pyoxy run-yaml hello_world.yaml
hello, world
</code></pre>
<p>Under the hood, PyOxy uses the
<a href="https://docs.rs/pyembed/0.20.0/pyembed/">pyembed</a> Rust crate to manage embedded
Python interpreters. The YAML document that PyOxy uses is simply deserialized
into a
<a href="https://docs.rs/pyembed/0.20.0/pyembed/struct.OxidizedPythonInterpreterConfig.html">pyembed::OxidizedPythonInterpreterConfig</a>
Rust struct, which <code>pyembed</code> uses to spawn a Python interpreter. This Rust struct
offers near complete control over how the embedded Python interpreter behaves: it
even allows you to tweak settings that are impossible to change from environment
variables or <code>python</code> command arguments! (Beware: this power means you can
easily cause the interpreter to crash if you feed it a bad configuration!)</p>
<h2>YAML Based Python Applications</h2>
<p><code>pyoxy run-yaml</code> ignores all file content before the YAML <code>---</code> start document
delimiter. This means that on UNIX-like platforms
you can create <em>executable YAML</em> files defining your Python application. e.g.</p>
<pre><code>$ mkdir -p myapp
$ cat &gt; myapp/__main__.py &lt;&lt; EOF
print("hello from myapp")
EOF

$ cat &gt; say_hello &lt;&lt;"EOF"
#!/bin/sh
"exec" "`dirname $0`/pyoxy" run-yaml "$0" -- "$@"
---
interpreter_config:
  run_module: 'myapp'
  module_search_paths: ["$ORIGIN"]
...
EOF

$ chmod +x say_hello

$ ./say_hello
hello from myapp
</code></pre>
<p>This means that to <em>distribute</em> a Python application, you can drop a copy
of <code>pyoxy</code> in a directory then define an executable YAML file masquerading
as a shell script and you can run Python code with as little as two files!</p>
<h2>The Future of PyOxy</h2>
<p>PyOxy is very young. I hacked it together on a weekend in September 2021.
I wanted to shore up some functionality before releasing it then. But I
got perpetually sidetracked and never did the work. I figured it would be
better to make a smaller splash with a lesser-baked product now than wait
even longer. Anyway...</p>
<p>As part of building
<a href="https://pyoxidizer.readthedocs.io/en/stable/pyoxidizer.html">PyOxidizer</a> I've
built some peripheral technology:</p>
<ul>
<li>Standalone and highly distributable Python builds via the
  <a href="https://github.com/indygreg/python-build-standalone">python-build-standalone</a>
  project.</li>
<li>The <a href="https://docs.rs/pyembed/0.20.0/pyembed/">pyembed</a> Rust crate for managing
  an embedded Python interpreter.</li>
<li>The <a href="https://pyoxidizer.readthedocs.io/en/stable/oxidized_importer.html">oxidized_importer</a>
  Python package/extension for importing modules from memory, among other
  things.</li>
<li>The <a href="https://pyoxidizer.readthedocs.io/en/latest/oxidized_importer_packed_resources.html">Python packed resources</a>
  data format for representing a collection of Python modules and resource
  files for efficient loading (by <code>oxidized_importer</code>).</li>
</ul>
<p>I conceived PyOxy as a vehicle to enable people to leverage PyOxidizer's
technology without imposing PyOxidizer onto them. I feel that PyOxidizer's
broader technology is generally useful and too valuable to be gated behind
using PyOxidizer.</p>
<p>PyOxy is only officially released for Linux and macOS for the moment.
It definitely builds on Windows. However, I want to improve the single file
executable experience before officially releasing PyOxy on Windows. This
requires an extensive overhaul to <code>oxidized_importer</code> and the way it
serializes Python resources to be loaded from memory.</p>
<p>I'd like to add a sub-command to produce a
<a href="https://pyoxidizer.readthedocs.io/en/latest/oxidized_importer_packed_resources.html">Python packed resources</a>
payload. With this, you could bundle/distribute a Python application as
<code>pyoxy</code> plus a file containing your application's packed resources alongside
YAML configuring the Python interpreter. Think of this as a more modern and
faster version of the venerable <code>zipapp</code> approach. This would enable PyOxy to
satisfy packaging scenarios provided by tools like Shiv, PEX, and XAR.
However, unlike Shiv and PEX, <code>pyoxy</code> also provides an embedded Python
interpreter, so applications are much more portable since there isn't
reliance on the host machine having a Python interpreter installed.</p>
<p>I'm really keen to see how others want to use <code>pyoxy</code>.</p>
<p>The YAML based control over the Python interpreter could be super useful for
testing, benchmarking, and general Python interpreter configuration
experimentation. It essentially opens the door to things previously only
possible if you wrote code interfacing with Python's C APIs.</p>
<p>I can also envision tools that hide the existence of Python wanting to
leverage the single file Python distribution property of <code>pyoxy</code>. For
example, tools like Ansible could copy <code>pyoxy</code> to a remote machine to provide
a well-defined Python execution environment without having to rely on what
packages are installed. Or <code>pyoxy</code> could be copied into a container or
other sandboxed/minimal environment to provide a Python interpreter.</p>
<p>And that's PyOxy. I hope you find it useful. Please file any bug reports
or feature requests in <a href="https://github.com/indygreg/PyOxidizer/issues">PyOxidizer's issue tracker</a>.</p>]]></content>
  </entry>
  <entry>
    <author>
      <name>Gregory Szorc</name>
      <uri>http://gregoryszorc.com/blog</uri>
    </author>
    <title type="html"><![CDATA[Expanding Apple Ecosystem Access with Open Source, Multi Platform Code Signing]]></title>
    <link rel="alternate" type="text/html" href="http://gregoryszorc.com/blog/2022/04/25/expanding-apple-ecosystem-access-with-open-source,-multi-platform-code-signing" />
    <id>http://gregoryszorc.com/blog/2022/04/25/expanding-apple-ecosystem-access-with-open-source,-multi-platform-code-signing</id>
    <updated>2022-04-25T08:00:00Z</updated>
    <published>2022-04-25T08:00:00Z</published>
    <category scheme="http://gregoryszorc.com/blog" term="Apple" />
    <category scheme="http://gregoryszorc.com/blog" term="Rust" />
    <summary type="html"><![CDATA[Expanding Apple Ecosystem Access with Open Source, Multi Platform Code Signing]]></summary>
    <content type="html" xml:base="http://gregoryszorc.com/blog/2022/04/25/expanding-apple-ecosystem-access-with-open-source,-multi-platform-code-signing"><![CDATA[<p>A little over one year ago, I
<a href="/blog/2021/04/14/pure-rust-implementation-of-apple-code-signing/">announced a project to implement Apple code signing in pure Rust</a>.
There have been quite a number of developments since that post and I thought
a blog post was in order. So here we are!</p>
<p>But first, some background on why we're here.</p>
<h2>Background</h2>
<p>(Skip this section if you just want to get to the technical bits.)</p>
<p>Apple runs some of the largest and most profitable software application
ecosystems in existence. Gaining access to these ecosystems has traditionally
required the use of macOS and membership in the Apple Developer Program.</p>
<p>For the most part this makes sense: if you want to develop applications for
Apple operating systems you will likely utilize Apple's operating systems
and Apple's official tooling for development and distribution. Sticking to
the paved road is a good default!</p>
<p>But many people want more... flexibility. Open source developers, for example,
often want to distribute cross-platform applications with minimal effort.
There are entire programming language ecosystems where the operating system
you are running on is abstracted away as an implementation detail for many
applications. <strong>By creating a de facto requirement that macOS, iOS, etc
development require the direct access to macOS and (often above market priced)
Apple hardware, the distribution requirements imposed by Apple's software
ecosystems are effectively exclusionary and prevent interested parties
from contributing to the ecosystem.</strong></p>
<p>One of the aspects of software distribution on Apple platforms that trips
a lot of people up is code signing and notarization. Essentially, you need
to:</p>
<ol>
<li>Embed a cryptographic signature in applications that effectively attests
   to its authenticity from an Apple Developer Program associated account.
   (This is signing.)</li>
<li>Upload your application to Apple so they can inspect it, verify it meets
   requirements, likely store a copy of it. Apple then issues their own
   cryptographic signature called a <em>notarization ticket</em> which then needs
   to be <em>stapled</em>/attached to the application being distributed so Apple
   operating systems can trust it. (This is notarization.)</li>
</ol>
<p>Historically, these steps required Apple proprietary software run exclusively
from macOS. This means that even if you are in a software ecosystem like Rust,
Go, or the web platform where you can cross-compile apps without direct access
to macOS (testing is obviously a different story), you would still need macOS
somewhere if you wanted to sign and notarize your application. And signing and
notarization is effectively required on macOS due to default security settings.
On mobile platforms like iOS, it is impossible to distribute applications that
aren't signed and notarized unless you are running a jailbreaked device.</p>
<p>A lot of people (myself included) have grumbled at these requirements.
Why should I be forced to involve an Apple machine as part of my software
release process if I don't need macOS to build my application? Why do I have
to go through a convoluted dance to sign and notarize my application at
release time - can't it be more streamlined?</p>
<p>When I looked at this space last year, I saw some obvious inefficiencies
and room to improve. So as I said then, I <em>foolishly</em> set out to reimplement
Apple code signing so developers would have more flexibility and opportunity
for distributing applications to Apple's ecosystems.</p>
<p><strong>The ultimate goal of this work is to expand Apple ecosystem access to more
developers.</strong> A year later, I believe I'm delivering a product capable of
doing this.</p>
<h2>One Year Later</h2>
<p><strong>Foremost, I'm excited to announce release of
<a href="https://github.com/indygreg/PyOxidizer/releases/tag/apple-codesign/0.14.0">rcodesign 0.14.0</a>.
This is the first time I'm publishing pre-built binaries (Linux, Windows, and macOS)
of <code>rcodesign</code>. This reflects my confidence in the relative maturity of the
software.</strong></p>
<p>In case you are wondering, yes, the macOS <code>rcodesign</code> executable is self-signed:
it was signed by a GitHub Actions Linux runner using a code signing certificate
exclusive to a YubiKey. That YubiKey was plugged into a Windows 11 desktop next to
my desk. The <code>rcodesign</code> executable was not copied between machines as part of the
signing operation. Read on to learn about the sorcery that made this possible.</p>
<p>A lot has changed in the <a href="https://github.com/indygreg/apple-platform-rs/tree/main/apple-codesign">apple-codesign</a>
project / Rust crate in the last year! Just look at the
<a href="https://github.com/indygreg/apple-platform-rs/blob/main/apple-codesign/CHANGELOG.rst">changelog</a>!</p>
<p>The project was renamed from <code>tugger-apple-codesign</code>.</p>
<p>(If you installed via <code>cargo install</code>, you'll need to
<code>cargo install --force apple-codesign</code> to force Cargo to overwrite the <code>rcodesign</code>
executable with one from a different crate.)</p>
<p>The <code>rcodesign</code> CLI executable is still there and more powerful than ever.
You can still sign Apple applications from Linux, Windows, macOS, and any other
platform you can get the Rust program to compile on.</p>
<p>There is now <a href="https://pyoxidizer.readthedocs.io/en/latest/apple_codesign.html">Sphinx documentation for the project</a>.
This is published on readthedocs.io alongside PyOxidizer's documentation (because
I'm using a monorepo). There's some general documentation in there, such as a
guide on how to
<a href="https://pyoxidizer.readthedocs.io/en/latest/apple_codesign_custom_assessment_policies.html">selectively bypass Gatekeeper</a>
by deploying your own alternative code signing PKI to parallel Apple's. (This
seems like something many companies would want but for whatever reason I'm
not aware of anyone doing this - possibly because very few people understand
how these systems work.)</p>
<p>There are bug fixes galore. When I look back at the state of <code>rcodesign</code>
when I first blogged about it, I think of how naive I was. There were a myriad
of applications that wouldn't pass notarization because of a long tail of bugs.
There are still known issues. But <strong>I believe many applications will
successfully sign and notarize now.</strong> I consider failures novel and worthy of
bug reports - so please <a href="https://pyoxidizer.readthedocs.io/en/latest/apple_codesign_debugging.html">report them</a>!</p>
<p>Read on to learn about some of the notable improvements in the past year (many
of them occurring in the last two months).</p>
<h2>Support for Signing Bundles, DMGs, and <code>.pkg</code> Installers</h2>
<p>When I announced this project last year, only Mach-O binaries and trivially
simple <code>.app</code> bundles were signable. And even then there were a ton of subtle
issues.</p>
<p><code>rcodesign sign</code> can now sign more complex bundles, including many nested
bundles. There are reports of iOS app bundles signing correctly! (However, we
don't yet have good end-user documentation for signing iOS apps. I will gladly
accept PRs to improve the documentation!)</p>
<p>The tool also gained support for signing <code>.dmg</code> disk image files and <code>.pkg</code>
flat package installers.</p>
<p>Known limitations with signing are now
<a href="https://pyoxidizer.readthedocs.io/en/latest/apple_codesign_quirks.html">documented</a>
in the Sphinx docs.</p>
<p><strong>I believe <code>rcodesign</code> now supports signing all the major file formats used
for Apple software distribution.</strong> If you find something that doesn't sign
and it isn't documented as a known issue with an existing GitHub issue tracking
it, please report it!</p>
<h2>Support for Notarization on Linux, Windows, and macOS</h2>
<p>Apple publishes a Java tool named
<a href="https://help.apple.com/itc/transporteruserguide/">Transporter</a> that enables you
to upload artifacts to Apple for notarization. They make this tool available for
Linux, Windows, and of course macOS.</p>
<p>While this tool isn't open source (as far as I know), usage of this tool enables
you to notarize from Linux and Windows while still using Apple's official
tooling for communicating with their servers.</p>
<p><code>rcodesign</code> now has support for invoking Transporter and uploading artifacts
to Apple for notarization. We now support notarizing bundles, <code>.dmg</code> disk
images, and <code>.pkg</code> flat installer packages. I've successfully notarized all
of these application types from Linux.</p>
<p>(I'm capable of implementing
an alternative uploader in pure Rust but without assurances that Apple won't
bring down the ban hammer for violating terms of use, this is a bridge I'm
not yet willing to cross. The requirement to use Transporter is literally the
only thing standing in the way of making <code>rcodesign</code> an all-in-one single
file executable tool for signing and notarizing Apple software and I <strong>really</strong>
wish I could deliver this user experience win without reprisal.)</p>
<p><strong>With support for both signing and notarizing all application types, it is
now possible to release Apple software without macOS involved in your release
process.</strong></p>
<h2>YubiKey Integration</h2>
<p>I try to use my YubiKeys as much as possible because a secret or private key
stored on a YubiKey is likely more secure than a secret or private key sitting
around on a filesystem somewhere. If you hack my machine, you can likely
gain access to my private keys. But you will need physical access to my
YubiKey and to compel or coerce me into unlocking it in order to gain access
to its private keys.</p>
<p><strong><code>rcodesign</code> now has support for using YubiKeys for signing operations.</strong></p>
<p>This does require an off-by-default <code>smartcard</code> Cargo feature. So if
building manually you'll need to e.g.
<code>cargo install --features smartcard apple-codesign</code>.</p>
<p>The YubiKey integration comes courtesy of the amazing
<a href="https://crates.io/crates/yubikey">yubikey</a> Rust crate. This crate will speak
directly to the smartcard APIs built into macOS and Windows. So if you have an
<code>rcodesign</code> build with YubiKey support enabled, YubiKeys should
<em>just work</em>. Try it by plugging in your YubiKey and running
<code>rcodesign smartcard-scan</code>.</p>
<p>YubiKey integration has its
<a href="https://pyoxidizer.readthedocs.io/en/latest/apple_codesign_smartcard.html">own documentation</a>.</p>
<p>I even implemented some commands to make it easy to manage the code signing
certificates on your YubiKey. For example, you can run
<code>rcodesign smartcard-generate-key --smartcard-slot 9c</code> to generate a new private
key directly on the device and then
<code>rcodesign generate-certificate-signing-request --smartcard-slot 9c --csr-pem-path csr.pem</code>
to export that certificate to a Certificate Signing Request (CSR), which you can
exchange for an Applie-issued signing certificate at developer.apple.com. <strong>This
means you can easily create code signing certificates whose private key was
generated directly on the hardware device and can never be exported.</strong>
Generating keys this way is widely considered to be more secure than storing
keys in software vaults, like Apple's Keychains.</p>
<h2>Remote Code Signing</h2>
<p>The feature I'm most excited about is what I'm calling
<a href="https://pyoxidizer.readthedocs.io/en/latest/apple_codesign_remote_signing.html">remote code signing</a>.</p>
<p>Remote code signing allows you to delegate the low-level cryptographic signature
operations in code signing to a separate machine.</p>
<p>It's probably easiest to just demonstrate what it can do.</p>
<p><strong>Earlier today I signed a macOS universal Mach-O executable from a GitHub-hosted
Linux GitHub Actions runner using a YubiKey physically attached to the
Windows 11 machine next to my desk at home. The signed application was not
copied between machines.</strong></p>
<p>Here's how I did it.</p>
<p>I have a GitHub Actions workflow that calls <code>rcodesign sign --remote-signer</code>.
I manually triggered that workflow and started watching the near real time
job output with my browser. Here's a screenshot of the job logs:</p>
<p><img alt="GitHub Actions initiating remote code signing" src="https://raw.githubusercontent.com/indygreg/PyOxidizer/058f718641ad47b39ccf54346f0f0ad6e91bd09b/apple-codesign/docs/apple_codesign_actions_sjs_join.png" /></p>
<p><code>rcodesign sign --remote-signer</code> prints out some instructions (including a
wall of base64 encoded data) for what to do next. Importantly, it requests that
someone else run <code>rcodesign remote-sign</code> to continue the signing process.</p>
<p>And here's a screenshot of me doing that from the Windows terminal:</p>
<p><img alt="Windows terminal output from running remote-sign command" src="https://raw.githubusercontent.com/indygreg/PyOxidizer/058f718641ad47b39ccf54346f0f0ad6e91bd09b/apple-codesign/docs/apple_codesign_actions_signer_output.png" /></p>
<p>This log shows us connecting and authenticating with the YubiKey along
with some status updates regarding speaking to a remote server.</p>
<p>Finally, here's a screenshot of the GitHub Actions job output after
I ran that command on my Windows machine:</p>
<p><img alt="GitHub Actions initiating machine output" src="https://raw.githubusercontent.com/indygreg/PyOxidizer/058f718641ad47b39ccf54346f0f0ad6e91bd09b/apple-codesign/docs/apple_codesign_actions_initiator_output.png" /></p>
<p><em>Remote signing</em> enabled me to sign a macOS application from a GitHub Actions
runner operated by GitHub while using a code signing certificate securely
stored on my YubiKey plugged into a Windows machine hundreds of kilometers away
from the GitHub Actions runner. Magic, right?</p>
<p>What's happening here is the 2 <code>rcodesign</code> processes are communicating
with each other via websockets bridged by a central relay server.
(I operate a
<a href="https://pyoxidizer.readthedocs.io/en/latest/apple_codesign_remote_signing_design.html#default-remote-code-signing-server">default server free of charge</a>.
The server is open source and a Terraform module is available if you want
to run your own server with hopefully just a few minutes of effort.)
When the initiating machine wants to create a signature, it sends a
message back to the <em>signer</em> requesting a cryptographic signature. The
signature is then sent back to the initiator, who incorporates it.</p>
<p><strong>I designed this feature with automated releases from CI systems (like
GitHub Actions) in mind. I wanted a way where I could streamline the
code signing and release process of applications without having to give
a low trust machine in CI ~unlimited access to my private signing key.
But the more I thought about it the more I realized there are likely
many other scenarios where this could be useful. Have you ever emailed
or Dropboxed an application for someone else to sign because you don't
have an Apple issued code signing certificate? Now you have an alternative
solution that doesn't require copying files around!</strong> As long as you
can see the log output from the initiating machine or have that output
communicated to you (say over a chat application or email), you can
remotely sign files on another machine!</p>
<h3>An Aside on the Security of Remote Signing</h3>
<p>At this point, I'm confident the more security conscious among you have
been grimacing for a few paragraphs now. Websockets through a central
server operated by a 3rd party?! Giving remote machines access to perform
code signing against arbitrary content?! Your fears and skepticism are
100% justified: I'd be thinking the same thing!</p>
<p>I fully recognize that a service that facilitates remote code signing makes
for a very lucrative attack target! If abused, it could be used to coerce
parties with valid code signing certificates to sign unwanted code, like
malware. There are many, many, many <em>wrong</em> ways to implement such a feature.
I pondered for hours about the threat modeling and how to make this feature
as secure as possible.</p>
<p><a href="https://pyoxidizer.readthedocs.io/en/latest/apple_codesign_remote_signing_design.html">Remote Code Signing Design and Security Considerations</a>
captures some of my high level design goals and security assessments.
And <a href="https://pyoxidizer.readthedocs.io/en/latest/apple_codesign_remote_signing_protocol.html">Remote Code Signing Protocol</a>
goes into detail about the communications protocol, including the
crypto (actual cryptography, not the fad) involved. The key takeaways are
the protocol and server are designed such that a malicious server or
man-in-the-middle can not forge signature requests. Signing sessions expire
after a few minutes and 3rd parties (or the server) can't inject malicious
messages that would result in unwanted signatures. There is an initial
handshake to derive a session ephemeral shared encryption key and from
there symmetric encryption keys are used so all meaningful messages between
peers are end-to-end encrypted. About the worst a malicious server could do
is conduct a denial of service. This is by design.</p>
<p>As I argue in <a href="https://pyoxidizer.readthedocs.io/en/latest/apple_codesign_remote_signing_design.html#security-analysis-in-the-bigger-picture">Security Analysis in the Bigger Picture</a>,
I believe that my implementation of <em>remote signing</em> is <strong>more</strong> secure than
many common practices because common practices today entail making copies
of private keys and giving low trust machines (like CI workers) access to
private keys. Or files are copied around without cryptographic chain-of-custody
to prove against tampering. Yes, <em>remote signing</em> introduces a vector for remote
access to <em>use</em> signing keys. But practiced as I intended, <em>remote signing</em> can
eliminate the need to copy private keys or grant ~unlimited access to them.
From a threat modeling perspective, I think the net restriction in key
access makes <em>remote signing</em> more secure than the private key management
practices by many today.</p>
<p><strong>All that being said, the giant asterisk here is I implemented my own
cryptosystem to achieve end-to-end message security. If there are bugs in
the design or implementation, that cryptosystem could come crashing down,
bringing defenses against message forgery with it.</strong> At that point, a
malicious server or privileged network actor could potentially coerce
someone into signing unwanted software. But this is likely the extent of
the damage: an offline attack against the signing key should not be
possible since signing requires presence and since the private key is
never transmitted over the wire. Even without the end-to-end encryption,
the system is <em>arguably</em> more secure than leaving your private key
lingering around as an easily exfiltrated CI secret (or similar).</p>
<p>(I apologize to every cryptographer I worked with at Mozilla who beat into me
the commandment that <em>thou shall not roll their own crypto</em>: I have sinned
and I feel remorseful.) </p>
<p>Cryptography is hard. And I'm sure I made plenty of subtle mistakes.
<a href="https://github.com/indygreg/PyOxidizer/issues/552">Issue #552</a> tracks
getting an audit of this protocol and code performed. And the aforementioned
<a href="https://pyoxidizer.readthedocs.io/en/latest/apple_codesign_remote_signing_protocol.html">protocol design docs</a>
call out some of the places where I question decisions I've made.</p>
<p><strong>If you would be interested in doing a security review on this feature,
please get in touch on issue #552 or
<a href="mailto:gregory.szorc@gmail.com">send me an email</a>. If there's one immediate
outcome I'd like from this blog post it would be for some white hat^Hknight
to show up and give me peace of mind about the cryptosystem implementation.</strong></p>
<p><strong>Until then, please assume the end-to-end encryption is completely flawed.</strong>
Consider asking someone with <em>security</em> or <em>cryptographer</em> in their job title
for their opinion on whether this feature is safe for you to use. Hopefully
we'll get a security review done soon and this caveat can go away!</p>
<p>If you do want to use this feature,
<a href="https://pyoxidizer.readthedocs.io/en/latest/apple_codesign_remote_signing.html">Remote Code Signing</a>
contains some usage documentation, including how to use it with GitHub
Actions. (I could also use some help productionizing a reusable GitHub Action
to make this more turnkey! Although I'm hesitant to do it before I know the
cryptosystem is sound.)</p>
<p>That was a long introduction to <em>remote code signing</em>. But I couldn't
in good faith present the feature without addressing the security aspect.
Hopefully I didn't scare you away! <strong>Traditional / local signing should
have no security concerns</strong> (beyond the willingness to run software written
by somebody you probably don't know, of course).</p>
<h2>Apple Keychain Support</h2>
<p>As of today's 0.14 release we now have early support for signing with code signing
certificates stored in Apple Keychains! If you created your Apple code signing
certificates in Keychain Access or Xcode, this is probably where you code
signing certificates live.</p>
<p>I held off implementing this for the longest time because I didn't perceive
there to be a benefit: if you are on macOS, just use Apple's official tooling.
But with <code>rcodesign</code> gaining support for remote code signing and some other
features that could make it a compelling replacement for Apple tooling on
all platforms, I figured we should provide the feature so we stop discouraging
people to export private keys from Keychains.</p>
<p>This integration is very young and there's still a lot that can be done,
such as automatically using an appropriate signing certificate based on
what you are signing. Please file feature request issues if there's
a must-have feature you are missing!</p>
<h2>Better Debugging of Failures</h2>
<p>Apple's code signing is complex. It is easy for there to be subtle differences
between Apple's tooling and <code>rcodesign</code>.</p>
<p><code>rcodesign</code> now has <code>print-signature-info</code> and <code>diff-signatures</code> commands to
dump and compare YAML metadata pertinent to code signing to make it easier to
compare behavior between code signing implementations and even multiple
signing operations.</p>
<p>The documentation around
<a href="https://pyoxidizer.readthedocs.io/en/latest/apple_codesign_debugging.html">debugging and reporting bugs</a>
now emphasizes using these tools to help identify bugs.</p>
<h2>A Request For Users and Feedback</h2>
<p>I now believe <code>rcodesign</code> to be generally usable. I've thrown a lot of
random software at it and I feel like most of the big bugs and major missing
features are behind us.</p>
<p>But I also feel it hasn't yet received wide enough attention to have confidence
in that assessment.</p>
<p><strong>If you want to help the development of this tool, the most important
actions you can take are to attempt signing / notarization operations with it
and report your results.</strong></p>
<p>Does <code>rcodesign</code> spark joy? Please leave a comment in the
<a href="https://github.com/indygreg/PyOxidizer/discussions/556">GitHub discussion for the latest release</a>!</p>
<p>Does <code>rcodesign</code> not work? I would very much appreciate a bug report!
Details on how to file good bugs are
<a href="https://pyoxidizer.readthedocs.io/en/latest/apple_codesign_debugging.html">in the docs</a>.</p>
<p>Have general feedback? UI is confusing? Documentation is insufficient?
Leave a comment in the aforementioned discussion. Or
<a href="https://github.com/indygreg/PyOxidizer/issues/new">create a GitHub issue</a> if
you think it is actionable. I can't fix what I don't know about!</p>
<p>Have private feedback? <a href="mailto:gregory.szorc@gmail.com">Send me an email</a>.</p>
<h2>Conclusion</h2>
<p>I could write thousands of words about all I learned from hacking on this
project.</p>
<p>I've learned way too much about too many standards and specifications in the
crypto space. RFCs 2986, 3161, 3280, 3281, 3447, 4210, 4519, 5280, 5480,
5652, 5869, 5915, 5958, and 8017 plus probably a few more. How cryptographic
primitives are stored and expressed: ASN.1, OIDs, BER, DER, PEM, SPKI,
PKCS#1, PKCS#8. You can show me the raw parse tree for an ASN.1 data structure
and I can probably tell you what RFC defines it. I'm not proud of this. But
I will say actually knowing what every field in an X.509 certificate does
or the many formats that cryptographic keys are expressed in seems empowering.
Before, I would just search for the <code>openssl</code> incantation to do something.
Now, I know which ASN.1 data structures are involved and how to manipulate
the fields within.</p>
<p>I've learned way too much around minutia around how Apple code signing
actually works. The mechanism is way too complex for something in the security
space. There was at least one high profile Gatekeeper bug in the past year
allowing improperly signed code to run. I suspect there will be more: the
surface area to exploit is just too large.</p>
<p><strong>I think I'm proud of building an open source implementation of Apple's code
signing. To my knowledge nobody else has done this outside of Apple. At least
not to the degree I have.</strong> Then factor in that I was able to do this without
access (or willingness) to look at Apple source code and much of the progress was
achieved by diffing and comparing results with Apple's tooling. Hours of
staring at diffoscope and comparing binary data structures. Hours of trying
to find the magical settings that enabled a SHA-1 or SHA-256 digest to agree.
It was tedious work for sure. I'll likely never see a financial return on
the time equivalent it took me to develop this software. But, I suppose I
can nerd brag that I was able to implement this!</p>
<p><strong>But the real reward for this work will be if it opens up avenues to more
(open source) projects distributing to the Apple ecosystems.</strong> This has
historically been challenging for multiple reasons and many open source
projects have avoided official / proper distribution channels to avoid the
pain (or in some cases because of philosophical disagreements with the premise
of having a walled software garden in the first place). I suspect things
will only get worse, as I feel it is inevitable Apple clamps down on signing
and notarization requirements on macOS due to the rising costs of malware
and ransomware. <strong>So having an alternative, open source, and multi-platform
implementation of Apple code signing seems like something important that
should exist in order to provide opportunities to otherwise excluded
developers. I would be humbled if my work empowers others. And this is
all the reward I need.</strong></p>]]></content>
  </entry>
  <entry>
    <author>
      <name>Gregory Szorc</name>
      <uri>http://gregoryszorc.com/blog</uri>
    </author>
    <title type="html"><![CDATA[Bulk Analyze Linux Packages with Linux Package Analyzer]]></title>
    <link rel="alternate" type="text/html" href="http://gregoryszorc.com/blog/2022/01/09/bulk-analyze-linux-packages-with-linux-package-analyzer" />
    <id>http://gregoryszorc.com/blog/2022/01/09/bulk-analyze-linux-packages-with-linux-package-analyzer</id>
    <updated>2022-01-09T21:10:00Z</updated>
    <published>2022-01-09T21:10:00Z</published>
    <category scheme="http://gregoryszorc.com/blog" term="packaging" />
    <category scheme="http://gregoryszorc.com/blog" term="Rust" />
    <summary type="html"><![CDATA[Bulk Analyze Linux Packages with Linux Package Analyzer]]></summary>
    <content type="html" xml:base="http://gregoryszorc.com/blog/2022/01/09/bulk-analyze-linux-packages-with-linux-package-analyzer"><![CDATA[<p>I've frequently wanted to ask random questions about Linux packages and
binaries:</p>
<ul>
<li>Which packages provide a file X?</li>
<li>Which binaries link against a library X?</li>
<li>Which libraries/packages define/export a symbol X?</li>
<li>Which binaries reference an undefined symbol X?</li>
<li>Which features of ELF are in common use?</li>
<li>Which x86 instructions are in a binary?</li>
<li>What are the most common ELF section names and their sizes?</li>
<li>What are the differences between package X in distributions A and B?</li>
<li>Which ELF binaries have the most relocations?</li>
<li>And many more.</li>
</ul>
<p>So, I built
<a href="https://github.com/indygreg/PyOxidizer/tree/main/linux-package-analyzer">Linux Package Analyzer</a>
to facilitate answering questions like this.</p>
<p>Linux Package Analyzer is a Rust crate providing the <code>lpa</code> CLI tool. <code>lpa</code> currently
supports importing Debian and RPM package repositories (the most popular Linux
packaging formats) into a local SQLite database so subsequent analysis can be
efficiently performed offline. In essence:</p>
<ol>
<li>Run <code>lpa import-debian-repository</code> or <code>lpa import-rpm-repository</code> and point
   the tool at the base URL of a Linux package repository.</li>
<li>Package indices are fetched.</li>
<li>Discovered <code>.deb</code> and <code>.rpm</code> files are downloaded.</li>
<li>Installed files within each package archive are inspected.</li>
<li>Binary/ELF files have their executable sections disassembled.</li>
<li>Results are stored in a local SQLite database for subsequent analysis.</li>
</ol>
<p>The LPA-built database currently stores the following:</p>
<ul>
<li>Imported packages (name, version, source URL).</li>
<li>Installed files within each package (path, size).</li>
<li>ELF file information (parsed fields from header, number of relocations,
  important metadata from the <code>.dynamic</code> section, etc).</li>
<li>ELF sections (number, type, flags, address, file offset, etc).</li>
<li>Dynamic libraries required by ELF files.</li>
<li>ELF symbols (name, demangled name, type constant, binding, visibility,
  version string, etc).</li>
<li>For x86 architectures, counts of unique instructions in each ELF file and
  counts of instructions referencing specific registers.</li>
</ul>
<p>Using a command like <code>lpa import-debian-repository --components main,multiverse,restricted,universe
--architectures amd64 http://us.archive.ubuntu.com/ubuntu impish</code>, I can import
the (currently) ~96 GB of package data from 63,720 packages defining Ubuntu 21.10
to a local ~12 GB SQLite database and answer tons of random questions. Interesting
insights yielded so far include:</p>
<ul>
<li>The entirety of the package ecosystem for amd64 consists of
  63,720 packages providing 6,704,222 files (168,730 of them ELF binaries)
  comprising 355,700,362,973 bytes in total.</li>
<li>Within the 168,730 ELF binaries are 5,286,210 total sections having
  606,175 distinct names. There are also 116,688,943 symbols
  in symbol tables (debugging info is not included and local symbols not
  imported or exported are often not present in symbol tables) across
  19,085,540 distinct symbol names. The sum of all the unique symbol names
  is 1,263,441,355 bytes and 4,574,688,289 bytes if you count occurrences
  across all symbol tables (this might be an over count due to how ELF string
  tables work).</li>
<li>The longest demangled ELF symbol is 271,800 characters and is defined in
  the file <code>usr/lib/x86_64-linux-gnu/libmshr.so.2019.2.0.dev0</code> provided by
  the <code>libmshr2019.2</code> package.</li>
<li>The longest non-mangled ELF symbol is 5,321 characters and is defined in
  multiple files/packages, as it is part of a library provided by GCC.</li>
<li>Only 145 packages have files with
  <a href="https://sourceware.org/glibc/wiki/GNU_IFUNC">indirect functions (IFUNCs)</a>.
  If you discard duplicates (mainly from GCC and glibc), you are left with
  ~11 packages. This does not appear to be a popular ELF feature!</li>
<li>With 54,764 references in symbol tables, <code>strlen</code> appears to be the most
  (recognized) widely used libc symbol. It even bests <code>memcpy</code> (52,726) and
  <code>free</code> (42,603).</li>
<li><code>MOV</code> is the most frequent x86 instruction, followed by <code>CALL</code>. (I could
  write an entire blog post about observations about x86 instruction use.)</li>
</ul>
<p>There's a trove of data in the SQLite database and the <code>lpa</code> commands only
expose a fraction of it. I reckon a lot of interesting tweets, blog posts,
research papers, and more could be derived from the data that <code>lpa</code> assembles.</p>
<p><code>lpa</code> does all of its work in-process using pure Rust. The Debian and RPM
repository interaction is handled via the
<a href="https://crates.io/crates/debian-packaging">debian-packaging</a> and
<a href="https://crates.io/crates/rpm-repository">rpm-repository</a> crates (which I
wrote). ELF file parsing is handled by the (amazing)
<a href="https://crates.io/crates/object">object</a> crate. And x86 disassembling via
the <a href="https://crates.io/crates/iced-x86">iced-x86</a> crate. Many tools similar
to <code>lpa</code> call out to other processes to interface with <code>.deb</code>/.<code>rpm</code> packages,
parse ELF files, disassemble x86, etc. Doing this in pure Rust makes life so
much simpler as all the functionality is self-contained and I don't have to
worry about run-time dependencies for random tools. This means that <code>lpa</code>
should <em>just work</em> from Windows, macOS, and other non-Linux environments.</p>
<p>Linux Package Analyzer is very much in its infancy. And I don't really
have a grand vision for it. (I built it and some of the packaging code it
is built on) in support of some even grander projects I have cooking.) Please
file bugs, feature requests, and pull requests
<a href="https://github.com/indygreg/PyOxidizer">in GitHub</a>. The project is currently
part of the PyOxidizer repo (because I like monorepos). But I may pull it and
other os/packaging/toolchain code into a new monorepo since target audiences
are different.</p>
<p>I hope others find this tool useful!</p>]]></content>
  </entry>
  <entry>
    <author>
      <name>Gregory Szorc</name>
      <uri>http://gregoryszorc.com/blog</uri>
    </author>
    <title type="html"><![CDATA[Rust Implementation of Debian Packaging Primitives]]></title>
    <link rel="alternate" type="text/html" href="http://gregoryszorc.com/blog/2022/01/03/rust-implementation-of-debian-packaging-primitives" />
    <id>http://gregoryszorc.com/blog/2022/01/03/rust-implementation-of-debian-packaging-primitives</id>
    <updated>2022-01-03T16:00:00Z</updated>
    <published>2022-01-03T16:00:00Z</published>
    <category scheme="http://gregoryszorc.com/blog" term="packaging" />
    <category scheme="http://gregoryszorc.com/blog" term="Rust" />
    <summary type="html"><![CDATA[Rust Implementation of Debian Packaging Primitives]]></summary>
    <content type="html" xml:base="http://gregoryszorc.com/blog/2022/01/03/rust-implementation-of-debian-packaging-primitives"><![CDATA[<p>Does your Linux distribution use tools with <code>apt</code> in their name to manage
system packages? If so, your system packages are using Debian packaging.</p>
<p>Most tools interfacing with Debian packages (<code>.deb</code> files) and repositories
use functionality provided by the <a href="https://salsa.debian.org/apt-team/apt">apt</a>
repository. This repository provides libraries like <code>libapt</code> as well as
tools like <code>apt-get</code> and <code>apt</code>. Most of the functionality is implemented in
C++.</p>
<p>I wanted to raise awareness that I've begun implementing Debian packaging
primitives in pure Rust. The <code>debian-packaging</code> crate is
<a href="https://crates.io/crates/debian-packaging">published on crates.io</a>. For
now, it is developed inside the
<a href="https://github.com/indygreg/PyOxidizer">PyOxidizer repository</a> (because I
like monorepos).</p>
<p>So far, a handful of useful functionality is implemented:</p>
<ul>
<li>Parsing and serializing <a href="https://www.debian.org/doc/debian-policy/ch-controlfields.html">control files</a></li>
<li>Reading repository indices files and parsing their content.</li>
<li>Reading HTTP hosted repositories.</li>
<li>Publishing repositories to the filesystem and S3.</li>
<li>Writing changelog files.</li>
<li>Reading and writing <code>.deb</code> files.</li>
<li>Copying repositories.</li>
<li>Creating repositories.</li>
<li>PGP signing and verification operations.</li>
<li>Parsing and sorting version strings.</li>
<li>Dependency syntax parsing.</li>
<li>Dependency resolution.</li>
</ul>
<p>Hopefully the <a href="https://docs.rs/debian-packaging/0.9.0/debian_packaging/">documentation</a>
contains all you would want to know for how to use the crate.</p>
<p>The crate is designed to be used as a library so any Rust program can
(hopefully) easily tap the power of the Debian packaging ecosystem.</p>
<p>As with most software, there are likely several bugs and many features not yet
implemented. But I have bulk downloaded the entirety of some distribution's
repositories without running into obvious parse/reading failures. So I'm
reasonably confident that important parts of the code (like control file parsing,
repository indices file handling, and <code>.deb</code> file reading) work as advertised.</p>
<p>Hopefully someone out there finds this work useful!</p>]]></content>
  </entry>
  <entry>
    <author>
      <name>Gregory Szorc</name>
      <uri>http://gregoryszorc.com/blog</uri>
    </author>
    <title type="html"><![CDATA[Why You Shouldn't Use Git LFS]]></title>
    <link rel="alternate" type="text/html" href="http://gregoryszorc.com/blog/2021/05/12/why-you-shouldn't-use-git-lfs" />
    <id>http://gregoryszorc.com/blog/2021/05/12/why-you-shouldn't-use-git-lfs</id>
    <updated>2021-05-12T10:30:00Z</updated>
    <published>2021-05-12T10:30:00Z</published>
    <category scheme="http://gregoryszorc.com/blog" term="Mercurial" />
    <category scheme="http://gregoryszorc.com/blog" term="Git" />
    <summary type="html"><![CDATA[Why You Shouldn't Use Git LFS]]></summary>
    <content type="html" xml:base="http://gregoryszorc.com/blog/2021/05/12/why-you-shouldn't-use-git-lfs"><![CDATA[<p>I have long held the opinion that you should avoid Git LFS if possible.
Since people keeping asking me why, I figured I'd capture my thoughts
in a blog post so I have something to refer them to.</p>
<p>Here are my reasons for not using Git LFS.</p>
<h2>Git LFS is a Stop Gap Solution</h2>
<p>Git LFS was developed outside the official Git project to fulfill a
very real market need that Git didn't/doesn't handle large files very
well.</p>
<p>I believe it is inevitable that Git will gain better support for
handling of large files, as this seems like a critical feature for
a popular version control tool.</p>
<p>If you make this long bet, LFS is only an interim solution and its
value proposition disappears after Git has better native support
for large files.</p>
<p>LFS as a stop gap solution would be tolerable except for the fact
that...</p>
<h2>Git LFS is a One Way Door</h2>
<p>The adoption or removal of Git LFS in a repository is an irreversible
decision that requires rewriting history and losing your original 
commit SHAs.</p>
<p>In some contexts, rewriting history is tolerable. In many others, it
is an extremely expensive proposition. My experience maintaining
version control in professional contexts aligns with the opinion
that rewriting history is expensive and should only be considered a
measure of last resort. Maybe if tools made it easier to rewrite
history without the negative consequences (e.g. GitHub would redirect
references to old SHA1 in URLs and API calls) I would change my opinion
here. Until that day, the drawbacks of losing history are just too
high to stomach for many.</p>
<p>The reason adoption or removal of LFS is irreversible is due to the
way Git LFS works. What LFS does is change the blob content that a
Git commit/tree references. Instead of the content itself, it stores
a pointer to the content. At checkout and commit time, LFS blobs/records
are treated specially via a mechanism in Git that allows content to be
rewritten as it moves between Git's core storage and its materialized
representation. (The same filtering mechanism is responsible for
normalizing line endings in text files. Although that feature is built
into the core Git product and doesn't work exactly the same way. But
the principles are the same.)</p>
<p>Since the LFS pointer is part of the Merkle tree that a Git commit
derives from, you can't add or remove LFS from a repo without rewriting
existing Git commit SHAs.</p>
<p>I want to explicitly call out that even if a rewrite is acceptable in
the short term, things may change in the future. If you adopt LFS
today, you are committing to a) running an LFS server forever b)
incurring a history rewrite in the future in order to remove LFS from
your repo, or c) ceasing to provide an LFS server and locking out people
from using older Git commits. I don't think any of these are great
options: I would prefer if there were a way to offboard from LFS in
the future with minimal disruption. This is theoretically possible, but
it requires the Git core product to recognize LFS blobs/records natively.
There's no guarantee this will happen. So adoption of Git LFS is a one
way door that can't be easily reversed.</p>
<h2>LFS is More Complexity</h2>
<p>LFS is more complex for Git end users.</p>
<p>Git users have to install, configure, and sometimes know about the
existence of Git LFS. Version control should <em>just work</em>. Large file
handling should <em>just work</em>. End-users shouldn't have to care that
large files are handled slightly differently from small files.</p>
<p>The usability of Git LFS is generally pretty good. However, there's an
upper limit on that usability as long as LFS exists outside the core
Git product. And LFS will likely never be integrated into the core Git
product because the Git maintainers know that LFS is only a stop gap
solution. They would rather solve large files storage <em>correctly</em> than
~forever carry the legacy baggage of having to support LFS in the core
product.</p>
<p>LFS is more complexity for Git server operators as well. Instead of
a self-contained Git repository and server to support, you now have
to support a likely separate HTTP server to facilitate LFS access.
This isn't the hardest thing in the world, especially since we're
talking about key-value blob storage, which is arguably a solved problem.
But it's another piece of infrastructure to support and secure and it
increases the surface area of complexity instead of minimizing it.
As a server operator, I would much prefer if the large file storage
were integrated into the core Git product and I simply needed to provide
some settings for it to <em>just work</em>.</p>
<h2>Mercurial Does LFS Slightly Better</h2>
<p>Since I'm a maintainer of the Mercurial version control tool, I thought
I'd throw out how Mercurial handles large file storage better than
Git. Mercurial's large file handling isn't great, but I believe it
is strictly better with regards to the trade-offs of adopting large
file storage.</p>
<p>In Mercurial, use of LFS is a dynamic feature that server/repo operators
can choose to enable or disable whenever they want. When the Mercurial
server sends file content to a client, presence of external/LFS storage
is a <em>flag</em> set on that file revision. Essentially, the flag says <em>the
data you are receiving is an LFS record, not the file content itself</em> and
the client knows how to resolve that record into content.</p>
<p>Conceptually, this is little different from Git LFS records in terms of
content resolution. However, the big difference is this flag is part
of the repository interchange data, not the core repository data as it
is with Git. Since this flag isn't part of the Merkle tree used to
derive the commit SHA, adding, removing, or altering the content of the
LFS records doesn't require rewriting commit SHAs. The tracked content
SHA - the data now stored in LFS - is still tracked as part of the Merkle
tree, so the integrity of the commit / repository can still be verified.</p>
<p>In Mercurial, the choice of whether to use LFS and what to use LFS for
is made by the server operator and settings can change over time. For
example, you could start with no use of LFS and then one day decide to
use LFS for all file revisions larger than 10 MB. Then a year later
you lower that to all revisions larger than 1 MB. Then a year after
that Mercurial gains better <em>native</em> support for large files and
you decide to stop using LFS altogether.</p>
<p>Also in Mercurial, it is possible for clients to push a large file
<em>inline</em> as part of the push operation. When the server sees that
large file, it can be like <em>this is a large file: I'm going to add
it to the blob store and advertise it as LFS</em>. Because the large
file record isn't part of the Merkle tree, you can have nice things
like this.</p>
<p>I suspect it is only a matter of time before Git's wire protocol learns
the ability to dynamically advertise <em>remote servers</em> for content
retrieval and this feature will be leveraged for better large file
handling. Until that day, I suppose we're stuck with having to
rewrite history with LFS and/or funnel large blobs through Git natively,
with all the pain that entails.</p>
<h2>Conclusion</h2>
<p>This post summarized reasons to avoid Git LFS. Are there justifiable
scenarios for using LFS? Absolutely! If you insist on using Git and
insist on tracking many <em>large</em> files in version control, you
should definitely consider LFS. (Although, if you are a heavy user
of large files in version control, I would consider Plastic SCM instead,
as they seem to have the most mature solution for large files handling.)</p>
<p>The main point of this post is to highlight some drawbacks with
using Git LFS because Git LFS is most definitely not a magic bullet. If
you can stomach the short and long term effects of Git LFS adoption, by
all means, use Git LFS. But please make an informed decision either way.</p>]]></content>
  </entry>
  <entry>
    <author>
      <name>Gregory Szorc</name>
      <uri>http://gregoryszorc.com/blog</uri>
    </author>
    <title type="html"><![CDATA[Pure Rust Implementation of Apple Code Signing]]></title>
    <link rel="alternate" type="text/html" href="http://gregoryszorc.com/blog/2021/04/14/pure-rust-implementation-of-apple-code-signing" />
    <id>http://gregoryszorc.com/blog/2021/04/14/pure-rust-implementation-of-apple-code-signing</id>
    <updated>2021-04-14T13:45:00Z</updated>
    <published>2021-04-14T13:45:00Z</published>
    <category scheme="http://gregoryszorc.com/blog" term="PyOxidizer" />
    <category scheme="http://gregoryszorc.com/blog" term="Apple" />
    <category scheme="http://gregoryszorc.com/blog" term="Rust" />
    <summary type="html"><![CDATA[Pure Rust Implementation of Apple Code Signing]]></summary>
    <content type="html" xml:base="http://gregoryszorc.com/blog/2021/04/14/pure-rust-implementation-of-apple-code-signing"><![CDATA[<p>A few weeks ago I (foolishly?) set out to implement Apple code signing
(what Apple's <code>codesign</code> tool does) in pure Rust.</p>
<p>I wanted to quickly announce on this blog the existence of the project and
the news that as of a few minutes ago, the <code>tugger-apple-codesign</code> crate
implementing the code signing functionality is now
<a href="https://crates.io/crates/tugger-apple-codesign">published on crates.io</a>!</p>
<p>So, you can now sign Apple binaries and bundles on non-Apple hardware by
doing something like this:</p>
<pre><code>$ cargo install tugger-apple-codesign
$ rcodesign sign /path/to/input /path/to/output
</code></pre>
<p>Current features include:</p>
<ul>
<li>Robust support for parsing embedded signatures and most related data
  structures. <code>rcodesign extract</code> can be used to extract various signature
  data in raw or human readable form.</li>
<li>Parse and verify RFC 5652 Cryptographic Message Syntax (CMS) signature
  data.</li>
<li>Sign binaries. If a code signing certificate key pair is provided,
  a CMS signature will be created. This includes support for Time-Stamp Protocol
  (TSP) / RFC 3161 tokens. If no key pair is provided, you get an ad-hoc
  signature.</li>
<li>Signing bundles. Nested bundles and binaries will automatically be signed.
  Non-code resources will be digested and a <code>CodeResources</code> XML file will be
  produced.</li>
</ul>
<p>The most notable missing features are:</p>
<ul>
<li>No support for obtaining signing keys from keychains. If you want to sign
  with a cryptographic key pair, you'll need to point the tool at a PEM encoded
  key pair and CA chain.</li>
<li>No support for parsing the Code Signing Requirements language. We can parse the
  binary encoding produced by <code>csreq -b</code> and convert it back to this DSL. But we
  don't parse the human friendly language.</li>
<li>No support for notarization.</li>
</ul>
<p>All of these could likely be implemented. However, I am not actively working on
any of these features. If you would like to contribute support, make noise in
the <a href="https://github.com/indygreg/apple-platform-rs/issues">GitHub issue tracker</a>.</p>
<p>The Rust API, CLI, and documentation are still a bit rough around the edges. I
haven't performed thorough QA on aspects of the functionality. However, the
tool is able to produce signed binaries that Apple's canonical <code>codesign</code> tool
says are well-formed. So I'm reasonably confident some of the functionality
works as intended. If you find bugs or missing features, please
<a href="https://github.com/indygreg/apple-platform-rs/issues">report them on GitHub</a>. Or even
better: submit pull requests!</p>
<p>As part of this project, I also created and published the
<a href="https://crates.io/crates/cryptographic-message-syntax">cryptographic-message-syntax</a>
crate, which is a pure Rust partial implementation of RFC 5652, which defines
the cryptographic message signing mechanism. This RFC is a bit dated and seems
to have been superseded by RPKI. So you may want to look elsewhere before
inventing new signing mechanisms that use this format.</p>
<p>Finally, it appears the Windows code signing mechanism (Authenticode) also uses
RFC 5652 (or a variant thereof) for cryptographic signatures. So by implementing
Apple code signatures, I believe I've done most of the legwork to implement
Windows/PE signing! I'll probably implement Windows signing in a new crate whenever
I hook up automatic code signing to PyOxidizer, which was the impetus for this work
(I want to make it possible to build distributable Apple programs without Apple
hardware, using as many open source Rust components as possible).</p>]]></content>
  </entry>
  <entry>
    <author>
      <name>Gregory Szorc</name>
      <uri>http://gregoryszorc.com/blog</uri>
    </author>
    <title type="html"><![CDATA[Rust is for Professionals]]></title>
    <link rel="alternate" type="text/html" href="http://gregoryszorc.com/blog/2021/04/13/rust-is-for-professionals" />
    <id>http://gregoryszorc.com/blog/2021/04/13/rust-is-for-professionals</id>
    <updated>2021-04-13T08:20:00Z</updated>
    <published>2021-04-13T08:20:00Z</published>
    <category scheme="http://gregoryszorc.com/blog" term="Programming" />
    <category scheme="http://gregoryszorc.com/blog" term="Rust" />
    <summary type="html"><![CDATA[Rust is for Professionals]]></summary>
    <content type="html" xml:base="http://gregoryszorc.com/blog/2021/04/13/rust-is-for-professionals"><![CDATA[<p>A professional programmer delivers value through the authoring and maintaining
of software that solves problems. (There are other important ways for
professional programmers to deliver value but this post is about
programming.)</p>
<p>Programmers rely on various tools to author software. Arguably the most
important and consequential choice of tool is the programming language.</p>
<p>In this post, I will articulate why I believe Rust is a highly compelling
choice of a programming language for software professionals. I will state
my case that Rust disposes software to a lower defect rate, reduces total
development and deployment costs, and is exceptionally satisfying to use.
In short, I hope to convince you to learn and deploy Rust.</p>
<h2>My Background and Disclaimers</h2>
<p>Before I go too far, I'm targeting this post towards <em>professional programmers</em> -
people who program (or support programming through roles like management) as
their primary line of work or who spend sufficient time programming outside of
work. I consider myself a <em>professional programmer</em> both because I am a full-time
engineer in the software industry and because I contribute to some significant
open source projects outside of my day job.</p>
<p>The statement <em>Rust is for Professionals</em> does not imply any logical variant
thereof. e.g. I am not implying <em>Rust is not for non-professionals</em>. Rather,
the subject/thesis merely defines the audience I want to speak to: people who
spend a lot of time authoring, maintaining, and supporting software and are
invested in its longer-term outcomes.</p>
<p>I think opinion pieces about programming languages benefit from knowing
the author's experience with programming. I first started hacking on code in
the late 1990's. I've been a full-time software developer since 2007 after
graduating with a degree in Computer Engineering (after an aborted attempt
at Biomedical Engineering - hence my affinities for hardware and biological
sciences). I've programmed in the following languages: C, C++ (only until
C++11), C#, Erlang, Go, JavaScript, Java, Lua, Perl, PHP, Python, Ruby,
Rust, shell, SQL, and Verilog. Notably missing from this list is a Lisp and
a Haskell/Scala type language. Of these languages, I've spent the most time
with C, C#, JavaScript, Perl, PHP, Python, and Rust.</p>
<p>I'm not that strong in computer science or language theory: many colleagues
can talk circles around me when it comes to describing computer science
and programming language concepts like algorithms, type theory, and common terms
used to describe languages. (I have failed many technical interviews because of
my limitations here.) In contrast, I perceive my technical strengths as applying
an engineering rigor and practicality to problem solving. I care vastly more
about how/why things work the way they do and the practical consequences of
decisions/choices we make when it comes to software. I find that I tend to
think about 2nd and 3rd order effects and broader or longer-term consequences
more often than others. Some would call this <em>systems engineering</em>.</p>
<p>I've programmed all kinds of different software. Backend web services,
desktop applications, web sites, Firefox browser internals, the Mercurial
version control tool, build systems, system/machine management. Notably missing
are mobile programming (e.g. iOS/Android) and serious embedded systems (I've
hacked around with Raspberry Pis and Arduinos, but those seem very friendly
compared to other embedded devices). My strongest affinity is probably towards
<em>systems software</em> and general purpose tools: I enjoy building software that
other people use to build things. <em>Infrastructure</em> if you will.</p>
<p>Finally, I am expressing my personal opinion in this post. I do not speak for
any employer, present or former. While I would love to see more Rust at my
current employer, this post is not an attempt to influence what happens behind
my employer's walls: there a better ways to conduct successful
<a href="https://en.wikipedia.org/wiki/Nemawashi">nemawashi / 根回し</a> than a public
blog post. I am not affiliated with the Rust Project in any capacity
beyond a very infrequent code contributor and issue filer: I view myself as a
normal Rust user. I did work at Mozilla - the company who bankrolled most of
Rust's initial development. I even briefly worked in the same small Vancouver
office as Graydon Hoare, Rust's primary credited inventor! While I was keen for
Rust to succeed because it was affiliated with my then employer, I was most
definitely not a Rust evangelist or fan boy while at Mozilla. I have little
to personally benefit from this post: I'm writing it because I enjoy writing
and I believe the message is important.</p>
<p>With that out of the way, let's talk about Rust!</p>
<h2>Rust Makes Me Irrationally Giddy</h2>
<p>When I look back at my professional self when I was in my 20s, I feel like I was
young and dumb and overly exuberant about computers, technology, new software, and
the like. An older, more grizzled professional, I now accept the reality that it is
a miracle computers and software work as well as they do as often as they do. Point
at any common task on a computer and an iceberg of complexity and nuance lingers
under the surface. Our industry is abound in the repetition of proven sub-optimal
ideas. You see practices cargo culted across the decades (like the
<a href="http://exple.tive.org/blarg/2019/10/23/80x25/">80 character terminal/line width</a> and
<a href="http://exple.tive.org/blarg/2020/11/26/punching-holes/">null-terminated strings</a>,
which can both be traced back to Hollerith punchcards from the late 19th century).
You witness cycles of pendulum swings, the same fads and trends, just with different
labels (microservices are the new SOA, YAML is the new XML, etc). I can definitely
relate to people in this industry who want to drop everything and move to a farm or
something (but I grew up in Indiana and had cows living down the street, so I
know this lifestyle isn't for me).</p>
<p><strong>Rust is the first programming language I've encountered in years that makes
me excited. And not just normal excited: irrationally excited. Like the kind
of excitement you have for something when you are naive about its limitations
and don't know any better (like many blockchain/cryptocurrency advocates). I
feel like the discovery of Rust is transporting me back to my younger self,
before I discovered the ugly realities of how computers and software work, and
is giving me hope that better tools, better ways of building software could
actually exist. To channel my inner Marie Kondo: Rust sparks joy.</strong></p>
<p>When I started learning Rust in earnest in 2018, I thought this was a fluke. <em>It
is just the butterflies you get when you think you fall in love,</em> I told myself.
<em>Give it time: your irrational excitement will fade.</em> But after using Rust for
~2.5 years now, my positive feelings about it have only grown <em>stronger</em>. There's
a reason Rust has
<a href="https://insights.stackoverflow.com/survey/2020#technology-most-loved-dreaded-and-wanted-languages-loved">claimed the top spot in Stack Overflow's most loved languages survey</a>
for 5 years and running. And not by the skin of its teeth: Rust is blowing
the competition out of the water. 19% over TypeScript and Python. 23% over Kotlin
and Go. If this were a Forrester report for a company-offered product, Rust
would be the <em>clear market leader</em> and marketers and salespeople would be using
this result to sign up new customers in droves and print money hand over fist.</p>
<p>Let me tell you why Rust excites me.</p>
<h2>Rust is Different (In a Good Way)</h2>
<p>After you've learned enough programming languages, you start to see common
patterns. Manual versus garbage collected memory management. Control flow
primitives like <code>if</code>, <code>else</code>, <code>do</code>, <code>while</code>, <code>for</code>, <code>unless</code>. Nullable types.
Variable declaration syntax. The list goes on.</p>
<p>To me, Rust introduced a number of new concepts, like <code>match</code> for control
flow, enums as algebraic types, the borrow checker, the <code>Option</code> and <code>Result</code>
types/enums and more. There were also behaviors of Rust that were different
from languages I knew: variables are immutable by default, <code>Result</code> types
must be checked they aren't an error to avoid a compiler warning, refusing
to compile if there are detectable memory access issues, and tons more.</p>
<p>Many of the new concepts weren't novel to Rust. But considering I've had exposure
to many popular programming languages, the fact many were new to me means these
aren't common features in <em>mainstream</em> languages. <strong>Learning Rust felt like fresh
air to me: here was a language designed to be general purpose and make inroads into
industry adoption while also willing to buck many of the trends of conventional
language design from the last several decades.</strong></p>
<p>When going against conventional practice, it is very easy to unintentionally
alienate yourself from potential users. Design a programming language too unlike
anything in common use and you are going to have a difficult time attracting
users. This is a problem with many <em>academic</em>/<em>opinionated</em> programming
languages (or so I hear). Rust does venture away from the tried and popular.
And that does contribute to a steeper learning curve. However, there is enough
familiarity in Rust's core language to give you a foothold when learning Rust.
(And Rust's <a href="https://www.rust-lang.org/learn">official learning resources</a> are
terrific.)</p>
<p>I feel like Rust's language designers set out to take a first principles
approach to the language using modern ideas and ignoring old, disproven ones,
realized they needed to ground the language in familiarity to achieve market
penetration, and produced reasonable compromises to yield something that was
new and novel but familiar enough to not completely alienate its large
potential user base.</p>
<p>If you don't like being exposed to new ideas and ways of working, Rust's
approach is probably a negative to you. But if you are like me and enjoy
continuously expanding your knowledge and testing new ideas, Rust's
novelty and willingness to <em>be different</em> is a much welcomed attribute.</p>
<h2>Rust: Toolbox Included</h2>
<p>It used to be that programming languages were just compilers or interpreters.
In recent years, we've seen more and more programming languages bundled
with other tools, such as build/packaging tools, code formatters, linters,
documentation generators, language servers, centralized package repositories,
and more.</p>
<p>I'm not sure what spurred this trend (maybe it was Go?), but I think it is
a good move. Programming languages are ecosystems and the compiler/interpreter
is just one part of a complex system. If you care about end-user experience
and adoption (especially if you are a new language), you want an as turnkey
on-boarding experience as possible. I think that's easier to pull off when
you offer a cohesive, multi-tool strategy to attract and retain users.</p>
<p>We refer to programming languages with a comprehensive standard library as
<em>batteries included</em>. I'm going to refer to programming languages with
additional included tools beyond the compiler/interpreter as <em>toolbox
included</em>.</p>
<p><strong>Rust, is very much a <em>toolbox included</em> language.</strong> (Unless you are installing
it via your Linux distribution: in that case Linux packagers have likely
unbundled all the tools into separate packages, making the experience a bit
more end-user hostile, as Linux packagers tend to do for reasons that
merit their own blog post. If you want to experience Rust the way its
maintainers intended - the <em>Director's Cut</em> if you will - install Rust via
<a href="https://rustup.rs/">rustup</a>.)</p>
<p>In addition to the Rust compiler (<code>rustc</code>) and the Rust standard library,
the following components are all officially developed and offered as part
of the Rust programming language <a href="https://github.com/rust-lang">on GitHub</a>:</p>
<ul>
<li>Cargo - Rust's package manager and build system.</li>
<li>Clippy - A Rust linter.</li>
<li><a href="https://doc.rust-lang.org/rustdoc/what-is-rustdoc.html">rustdoc</a> -
  Documentation generator for Rust projects.</li>
<li>rustfmt - A Rust code formatter.</li>
<li>rls - A Rust <a href="https://langserver.org/">Language Server Protocol</a> implementation.</li>
<li><a href="https://crates.io/">crates.io</a> - Rust's official, public package registry.</li>
<li>rustup - Previously mentioned Rust installer.</li>
<li><a href="https://github.com/rust-lang/vscode-rust">vscode-rust</a> - Visual Studio Code
  extension adding support for Rust. (JetBrains has their own high quality
  extension for their IDEs, which they develop themselves.)</li>
<li><a href="https://github.com/rust-lang/book">The Rust Programming Language</a> Book</li>
<li>And many more.</li>
</ul>
<p>As an end-user, having all these tools and resources at my fingertips,
maintained by the official Rust project is an absolute joy.</p>
<p>For the local tools, <code>rustup</code> ensures they are upgraded as a group, so I don't
have to worry about managing them. I periodically run <code>rustup update</code> to
ensure my Rust <em>toolbox</em> is up-to-date and that's all I have to do.</p>
<p>Contrast with say Node.js, <a href="https://xkcd.com/1987/">Python</a>, and Ruby, where
the package manager is on a separate release cadence from the core language
and I have to think about managing multiple tools. (Rust will likely have
to cross this bridge once there are multiple implementations of Rust or
multiple popular package managers. But until then, things are very simple.)</p>
<p>Further contrast with languages like JavaScript/Node.js, Python, and Ruby,
where tools like a code formatter, linter, and documentation generator
aren't always developed under the core project umbrella. As an end-user,
you have to know to seek out these additional value-add tools. Furthermore,
you have to know which ones to use and how to configure them. The
fragmentation also tends to yield varying levels of quality and end-user
experience, to the detriment of end-users. The Rust toolbox, by contrast,
feels simple and polished.</p>
<p><strong>Rust's <em>toolbox included</em> approach enables me to follow unified practices
(arguably best practices) while expending minimal effort.</strong> As a result,
the following tend to be very similar across nearly every Rust project you'll
run into:</p>
<ul>
<li>Code formatting. (Nearly everyone uses <code>rustfmt</code>.)</li>
<li>Adherence to common coding and style conventions. (Nearly everyone uses
  <code>clippy</code>.)</li>
<li>Project documentation. (Nearly everyone uses <code>rustdoc</code>.)</li>
</ul>
<p>Cargo could warrant its own dedicated section. But I'll briefly touch on it
here.</p>
<p>Cargo is Rust's official package manager and build system. With <code>cargo</code>, you
can:</p>
<ul>
<li>Create new Rust projects with a common project layout.</li>
<li>Build projects.</li>
<li>Run project tests.</li>
<li>Update project dependencies.</li>
<li>Generate project documentation (via <code>rustdoc</code>).</li>
<li>Install other Rust projects from source.</li>
<li>Publish packages to Rust package registries.</li>
</ul>
<p>As a build system, Cargo is generally a breeze to work with. Configuration
files are TOML. Adding dependencies is often a 1 line addition to a
<code>Cargo.toml</code> file. Dependencies often <em>just work</em> on the first try. It's
not like say C/C++, where taking on a new dependency can easily consume
a day or two to get it integrated in your build system and compatible with
your source code base. <strong>I can't emphasize enough how much joy it brings
to be able to leverage an <em>it just works</em> build tool for systems-level
programming: I'm finding myself doing things in Rust like parsing ELF, PE,
and Mach-O binaries because it is so easy to integrate low-level functionality
like this into any Rust program.</strong> Cargo is <em>boring</em>. And when it comes to
build systems, that's a massive compliment!</p>
<p>No other language I've used has as comprehensive and powerful of a
<em>toolbox</em> as Rust does. This <em>toolbox</em> is highly leveraged by the Rust
community, resulting is remarkable consistency across projects. This
consistency makes it easier to understand, use, and contribute back to
other Rust projects. Contrast this with say C/C++, where large code bases
often employ multiple tools in the same space on different parts of the
same code base, leading to cognitive dissonance and overhead.</p>
<p>As a professional programmer, Rust's powerful and friendly <em>toolbox</em>
enables me to build Rust software more easily than with other languages.
I spend less time wrangling tools and more time coding. That translates
to less overhead delivering value through software. Other languages
would be wise to emulate aspects of Rust's model.</p>
<h2>Rust is Humane</h2>
<p>Of all the programming languages I've used, Rust seems to empathize
with its users the most.</p>
<p>There's a few facets to this.</p>
<p>A lot of care seems to have gone into the end-user experience of the
Rust <em>toolbox</em>.</p>
<p>The Rust compiler often gives extremely actionable error and warning
messages. If something is wrong, it tells me why it is wrong, often
pointing out exactly where in source code the problem resides, drawing
carets to the source code where things went wrong. In many cases,
the compiler will emit a suggested fix, which I can incorporate
automatically by pressing a few keys in my IDE. Contrast this with
C/C++ and even Go, which tend to have either too-terse-to-be-actionable
or too-verbose-to-make-sense-of feedback. By comparison, output from
other compilers often comes across as condescending, as if they are
saying <em>git gud, idiot</em>. Rust's compiler output tends to come across
as <em>I'm sorry you had a problem: how can I help?</em> I feel like the
compiler actually cares about my [valuable] time and satisfaction.
It wants to keep me in
<a href="https://en.wikipedia.org/wiki/Flow_(psychology)">flow</a>.</p>
<p>Then there's <a href="https://github.com/rust-lang/rust-clippy">Clippy</a>, a
Rust linter maintained as part of the Rust project.</p>
<p>One thing I love about Clippy is - like the compiler - many of the lints
contain suggestions, which I can incorporate automatically through my
IDE. So many other linters just tell you what is wrong and don't seem
to go the extra mile to be respectful of my time by offering to fix it
for me.</p>
<p><strong>Another aspect of Clippy I love is it is like having an invisible Rust
mentor continuously providing constructive feedback to help me level-up my
Rust.</strong> I don't know how many times I've written Rust code similarly to how I
would write code in other languages and Clippy suggests a more <em>Rustic</em>
solution. Most of the time I'm like <em>oh, I didn't know about that: that's
a much better pattern/solution than what I wrote!</em></p>
<p>Do I agree with Clippy all the time? Nope. But I do find its signal to
noise ratio is exceptionally high compared to other linters I've used.
And Clippy is trivial to configure and override, so disagreements are
easy to manage. Like the Rust compiler, I feel that Clippy is respectful
of my time and has the long term maintainability and correctness of my
software at heart.</p>
<p>Then there's the Rust Community - the people behind the core Rust projects.
<strong>The Rust Community is one of the most professional and welcoming I've
seen.</strong> Their <a href="https://www.rust-lang.org/policies/code-of-conduct">Code of Conduct</a>
is sufficiently comprehensive and actionable. They have their vigorous
debates like any other community. But the conversation is civil. Bad
apples are discarded when they crop up.</p>
<p>At a talk I made about PyOxidizer at a Rust meetup a few years back,
I made a comment in passing about a negative comment I encountered on
a Rust sub-Reddit. After the talk, a moderator of that sub who was in
the audience (unbeknownst to me) approached for more information so they
could investigate, which they did.</p>
<p>I once tweeted about a somewhat confusing, not-very-actionable compiler
error I encountered. A few minutes later, some compiler developers were
conversing in replies. A few hours later, a pull request was created and
a much better error message was merged in short order. I'm not a special
one-off here either: I've stumbled across Stack Overflow questions and
other forums where Rust core developers see that someone is encountering
a confusing issue, question the process that got them to that point, and
then make refinements to minimize it from happening in the future. The
practice is very similar to what empathetic product managers and user
experience designers do.</p>
<p>Not many other communities (or companies for that matter) seem to
demonstrate such a high level of compassion and empathy for their
users. To be honest, I'm not sure how Rust manages to pull it off,
as this tends to be very expensive in terms of people time and it can
be very easy to not prioritize. One thing is for certain: <strong>the Rust Community
is loaded with empathetic people who care about the well-being of users
of their products. And it shows from the interaction in forums to
the software tools they produce. To everyone who has contributed in the
Rust Community: thank you for all that you have done and for setting an
example for the rest of us to live up to.</strong></p>
<h2>Rust is Surprisingly High Level</h2>
<p>One of the reasons I avoided learning Rust for years is that I perceived
it was too low level and therefore tedious. Rust was being advertised as
a <em>systems programming language</em> and you would hear stories of <em>fighting
the borrow checker</em>. I assumed I'd need to be thinking a lot about memory
and ownership. I assumed the cost to author and maintain Rust code would be
high. I thought Rust would be <em>a safer C/C++</em>, with many of the software
development lifecycle caveats that apply. And for the software I was
writing at a time, the value proposition of Rust seemed weak. I thought
a combination of C and say Python was <em>good enough</em>. When I started
writing <a href="https://github.com/indygreg/PyOxidizer">PyOxidizer</a>, I initially
thought only the run-time code calling into the Python interpreter C APIs
would be written in Rust and the rest would be Python.</p>
<p>How wrong I was!</p>
<p><strong>When I actually started coding Rust, I was shocked at how high-level it
felt.</strong> Now, depending on the space of your software, Rust code can be
very low-level and tedious (not unlike C/C++). However, <strong>for the
vast majority of code I author, Rust feels more like Python than C</strong>. And
even the lower-level code feels much higher level than C or even C++.</p>
<p>In my mind, <strong>the expressiveness of Rust comes very close to higher-level,
dynamic languages (like JavaScript, Python, and Ruby) while maintaining
the raw speed of C/C++ all without sacrificing low-level control for
cases when you need it. And it does all of this while maintaining strong
safety guarantees</strong> (unlike say Go, which has the
<a href="https://www.infoq.com/presentations/Null-References-The-Billion-Dollar-Mistake-Tony-Hoare/">billion dollar mistake</a>:
null references).</p>
<p>I had a mental Venn diagram of the properties of programming languages
(gc versus non-gc, static versus dynamic typing, compiled versus interpreted,
etc) and which traits (like execution speed, development time, etc) would
be possible and Rust invalidated large parts of that model!</p>
<p><strong>You often don't need to think about memory management in Rust</strong>: once you
understand the rules the borrow checker enforces, memory is largely something
that exists but is managed for you by the language, just like in garbage
collected languages. Of course there are scenarios where you should
absolutely be thinking about memory and should have a grasp on what Rust
is doing under the hood. But in my experience, most code can be blissfully
ignorant of what is actually happening at the memory level. (However,
awareness of value <em>ownership</em> when programming Rust does add overhead, so
it's not like the cognitive load required for reasoning about memory
disappears completely.)</p>
<p>Rust has both a stack and a heap. But when programming you often don't
need to distinguish these locations. You can do things in Rust like
return a reference to a stack allocated value and pass this reference around
to other functions. This would be a CVE factory in C/C++. But because of
Rust's borrow checker, this is safe (and a common practice) in Rust.
It also predisposes the code towards better performance! Often in C/C++
you will allocate on the heap because you need to return a reference to
memory and returning a reference to a stack allocated value is extremely
dangerous. This heap allocation incurs run-time overhead. So Rust allowing
you to do the fast thing safely is a nice mini win.</p>
<p>In many statically typed languages, I feel like my programming speed
is substantially reduced by having to repeatedly spell out or think
about type names. In C, it feels like I'm always writing type names
so I can perform casting. Newer versions of C++ and Java have improved
matters significantly (e.g. the <code>auto</code> keyword). However, I haven't
programmed them enough recently to know how they compare to Rust on this
front. All I know is that I'm writing type names a lot less frequently
in Rust than I thought I would be and that my programming output isn't
limited by my typing speed as much as it historically was in C/C++.</p>
<p>Despite being compiled down to assembly and exposing extremely
low-level control, Rust often feels like a higher-level language. Equivalent
functionality in Rust is often more concise and/or readable than in C/C++,
while performing similarly, all while having substantially stronger safety
guarantees. <strong>As a professional programmer, the value proposition is
blinding: Rust enables me to do more with less, achieve a lower
defect rate, and not sacrifice on performance.</strong></p>
<h2>Correctness, Quality, Execution Speed, and Development Velocity: Pick 4</h2>
<p>The operation of computers and operating systems is exceptionally complex.</p>
<p>All programming languages justifiably attempt to abstract away aspects of
this complexity to make it easier to deliver value through software. For
example:</p>
<ul>
<li>Assembly is hard: here's a higher level language that compiles down to
  assembly or is implemented in a language that does.</li>
<li>Managing memory manually is hard: use garbage collection.</li>
<li>Concurrency is hard: only allow 1 thread to run at a time (JavaScript, Python,
  etc).</li>
<li>Text encoding is hard: <em>strings</em> are Unicode/UTF-8.</li>
<li>Operating systems have different interfaces: here's a pile of abstractions
  in the standard library for things like I/O, networking, filesystem paths,
  etc.</li>
<li>Strong, static typing isn't very flexible and can impose high change costs:
  use dynamic typing.</li>
<li>And tons more.</li>
</ul>
<p>These abstractions often have undesirable consequences/trade-offs:</p>
<ul>
<li>Garbage collection adds run-time overhead (10% is a number that's commonly
  cited).</li>
<li>Garbage collection adds random slowdowns/pauses, making it difficult to
  achieve consistency in long-tail latency optimization (i.e. ensuring
  consistency in P99.9, P99.99, and beyond percentiles).</li>
<li>Interpreted languages tend to be slower than compiled languages unless you
  invest lots of time into a JIT.</li>
<li>Limiting execution to a single thread limits the ability to harness
  the full power of modern CPUs, which tend to have several cores.</li>
<li>Primitives like environment variables, process arguments, and filenames
  aren't guaranteed to be UTF-8 and coercing them to UTF-8 can be lossy.</li>
<li>Dynamic typing doesn't catch as many bugs at <em>compile time</em> and you have
  to be more diligent about guarding against invariants.</li>
<li>And tons more.</li>
</ul>
<p><strong>In other words, there are trade-offs with nearly every decision in programming
language and [standard] library design. There are usually no obviously
correct and undesirable consequence-free decisions.</strong></p>
<p><strong>And we further have to consider the fallibility of people and the inevitability
that mistakes will be made, that bugs and regressions will be introduced and will
need addressing. As an industry, we generally accept that mistakes occur and
bugs are an unavoidable aspect of software development. If new features and
enhancements are value, bugs and defects are anti-value. Like financial debt,
existence of bugs and sub-optimal code can be tolerated to varying extents. But
this is a highly nuanced topic and different people, companies, and projects will
have different perspectives on it. We can all agree that bugs are an inevitable
fact of software.</strong></p>
<p>We also need to confront the reality that as an industry we have
very little empirical data that says much of significance about topics
like <a href="https://danluu.com/empirical-pl/">static versus dynamic typing</a>.
Although we do know some things. As Alex Gaynor informs us in
<a href="https://alexgaynor.net/2020/may/27/science-on-memory-unsafety-and-security/">What science can tell us about C and C++'s security</a>,
the result of ~2/3 of security vulnerabilities being caused by memory
unsafety seems to reproduce against a sufficiently diverse set of
projects and companies. That result and the implications of it are
worth paying attention to!</p>
<p>With that being said, let's dive into my take on the matter.</p>
<p><strong>Of all the programming languages I've used, I feel that Rust has the
strongest disposition towards authoring and maintaining correct, high-quality
software. It does this by offering a myriad of features that are designed
to prevent (or at least minimize) defects. In addition, I believe Rust
shifts the detection of defects to earlier in the software development
lifecycle, greatly reducing the cost to mitigate defects and therefore
develop software.</strong></p>
<p>(As an aside, every time the topic of Rust's safety and correctness comes
up, random people on the Internet rush to their keyboards to say things
along the lines of <em>C/C++ and other languages can be made to be just as
safe as Rust: it's the bad programmers who are using C/C++ wrong.</em> To
those people: please stop. Your belief implies the infallibility of people
and machines and that mistakes won't be made. If things like memory unsafety
bugs in C/C++ could be prevented, industry titans like Apple, Google, and
Microsoft would have found a way. These companies are likely taking many
more measures to prevent security vulnerabilities than you are and
yet the ~2/3 of security vulnerabilities being caused by memory unsafety
(read: humans and machines failing to reason about run-time behavior)
result still occurs. <strong>To the wiser among us, I urge you to call out
perpetrators of this <em>good programmers don't create bugs</em> myth when you see
it</strong>, just like you would/should if you encounter racist, sexist, or other
non-inclusive behaviors. The reason is that belief in this myth can lead to
physical or emotional harm, just like non-inclusive -isms. Security bugs,
for example, can lead to disclosure of private or sensitive data, which can
result in real world harm. Think a stalker or abusive former partner
learning where you now live. Or a memory unsafety error in a medical device
leading to device malfunction, injuring or killing a patient. Because this
is a sensitive topic, I want to be clear that I'm not trying to compare the
relative harms incurred by racism, sexism, other -isms, or the <em>mythical
perfect programmer</em>. Rather, all I'm saying is each of these surpass
the minimum threshold of harm incurred that justifies calling out and
stopping the harmful behavior. I believe that as professionals we have an
ethical and professional obligation to actively squash the <em>mythical
perfect programmer</em> fallacy when we encounter it. Debates on the merits
and limits of tools to prevent/find defects is fine: belief in the <em>perfect
programmer</em> is not. Sorry for the mini rant: I just get upset by people who
think software exists in a vacuum and doesn't have real-world implications
for people's safety.)</p>
<p>In the sections below, I'll outline some of Rust's features and behaviors
that support my assertion that Rust is biased towards correct and higher
quality outcomes and lowers total development cost.</p>
<h3>The Borrow Checker</h3>
<p>To the uninitiated, the borrow checker is perhaps Rust's most novel contribution
to programming. It is a compile time mechanism that enforces various rules about
how Rust code must behave. Think of these as <em>laws</em> that Rust code must obey.
But these are more like societal laws, not scientific laws (which are
irrefutable), as Rust's laws can be broken, often leading to negative
consequences, just like societal laws.</p>
<p>Rust's <a href="https://doc.rust-lang.org/book/ch04-01-what-is-ownership.html">ownership</a>
rules are as follows:</p>
<ul>
<li>Each value in Rust has a variable that's called its owner.</li>
<li>There can only be one owner at a time.</li>
<li>When the owner goes out of scope, the value will be dropped / released.</li>
</ul>
<p>Then there are rules about
<a href="https://doc.rust-lang.org/book/ch04-02-references-and-borrowing.html">references</a>
(think <em>intelligent pointers</em>) to owned values:</p>
<ul>
<li>At any given time, you can have either one mutable reference or any number of
  immutable references.</li>
<li>References must always be valid.</li>
</ul>
<p>Put together, these rules say:</p>
<ul>
<li>There is only a single canonical owner of any given value at any
  given time. The owner automatically releases/frees the value when it is
  no longer needed (just like a garbage collected language does when the
  reference count goes to 0).</li>
<li>If there are references to an owned value, that reference must be valid
  (the owned value hasn't been dropped/released) and you can only have
  either multiple readers or a single writer (not e.g. a reader and a
  writer).</li>
</ul>
<p>The implications of these rules on the behavior of Rust code are significant:</p>
<ul>
<li>Use after free isn't something you have to worry about because references
  can't point to dropped/released values.</li>
<li>Buffer underruns, overflows, and other illegal memory access can't
  exist because references must be valid and point to an owned value /
  memory range.</li>
<li>Memory level data races are prevented because the <em>single writer or multiple
  readers</em> rule prevents concurrent reading and writing. (An assertion here
  is any guards - like locks and mutexes - have appropriate barriers/fences
  in place to ensure correct behavior in multi-threaded contexts. The ones
  in the standard library should.)</li>
</ul>
<p>I used to think that these rules <em>limited</em> the behavior of Rust code. That
statement is true. However, as I've thought about it more, I've refined
my take to be that <strong>ownership and reference rules reinforce properties that
well-behaved software exhibits.</strong></p>
<p>If a C/C++ program had illegal memory access, you would say it is <em>buggy</em> and
the behavior is not <em>correct</em>. If a Java program attempted to mutate a value
on thread A without a lock or other synchronization primitive and thread B
raced to read it, leading to data inconsistency, you would also call that a
<em>bug</em> and <em>incorrect</em> behavior. If a JavaScript/Python/Ruby function were
changed such that it started mutating a value that should be constant, you
would call that a <em>bug</em> and <em>incorrect</em> behavior.</p>
<p><strong>While Rust's ownership and reference rules do limit what software can
do, the functionality they are limiting is often <em>unsafe</em> or <em>buggy</em>, so
losing this functionality is often desirable from a quality and correctness
standpoint. Put another way, Rust's borrow checker eliminates entire
classes of [common] bugs by preventing patterns that lead to incorrect,
buggy behavior.</strong></p>
<p>This. Is. Huge.</p>
<p><strong>Rust's borrow checker catches bugs for which other languages have no
automated mechanism or no low cost, low latency mechanism for detecting.</strong>
There are ways to achieve aspects of what the borrow checker does in other
languages. But they tend to require contorting your coding style to
accomplish and/or employing high cost tools (often running asynchronously
to the compiler) such as {address, memory, thread} sanitizers or fuzzing.
With Rust, you get this bug detection built into the language and compiler:
no additional tools needed. (I'm not saying you shouldn't run additional
tools like sanitizers or fuzz testing against Rust: just that you get a
significant benefit of these tools for a drastically lower cost since they
are built in to the core language.)</p>
<p>Rust's ownership and reference rules help ensure your software is more
well-behaved and bug-free. But, sometimes those rules are too strict.
Fortunately, Rust isn't dogmatic about enforcing them. There are legitimate
cases where you can't work in the confines of these rules.</p>
<p>Say you want to share a cache between multiple threads. Caches need to be
both readable and writable by multiple threads. This violates
the reference rules and maybe the single owner ownership rule, depending on
how things are implemented. Fortunately, there are primitives in the
<a href="https://doc.rust-lang.org/std/sync/index.html">std::sync</a> module like
<a href="https://doc.rust-lang.org/std/sync/struct.RwLock.html">RwLock</a>
and <a href="https://doc.rust-lang.org/std/sync/struct.Arc.html">Arc</a> (atomically
reference counted) you can use here. <code>Arc</code> (and its non-threadsafe <code>Rc</code>
counterpart) give you reference counting, just like a garbage collected
language. Primitives like <code>RwLock</code> allow you to wrap an inner value
and temporarily <em>acquire</em> an appropriate reference to it, mutable or
non-mutable. There's a bit of slight of hand here, but the tricks
employed enable you to satisfy the ownership and reference rules and
use common programming techniques and patterns while still having the
<em>safety</em> and <em>correctness</em> protections the borrow checker enforces.</p>
<h3>Data Races: What Data Races?</h3>
<p>Multi-threaded and concurrent programming is hard. Really hard. Like it
is exceptionally easy to introduce hard-to-diagnose-and-debug bugs hard.</p>
<p>There are many reasons for this. We can all probably relate to the fact
that reasoning about multi-threaded code is just hard: instead of 1 call
stack to reason about there are N. Further complicating matters are that
many of us don't have a firm grasp on how memory works at a very low level.
Do you know all the ins and outs on how CPU caches work on the architecture
you are targeting? Me neither! (But
<a href="https://software.rajivprab.com/2018/04/29/myths-programmers-believe-about-cpu-caches/">this</a>
is a very good place to start excavating a rabbit hole.)</p>
<p>If you are like me, you've spent many years of your professional career
not having to care about multi-threading or concurrent programming because you
spend so much time in languages with single threads, are only implementing code
that runs in single threaded contexts, or you've recognized the reality that
implementing this code safely and correctly is hard and you've intentionally
avoided the space or chosen software architectures (like queue-based message
passing) to minimize risks. Or maybe if you are say a Java programmer you
sprinkle <code>synchronized</code> everywhere out of precaution or in response to race
conditions / bugs once they are found. (Everyone's personal experience is
different, of course.)</p>
<p>Long story short, <strong>the aforementioned ownership and reference rules enforced
by the borrow checker eliminate data races.</strong> This was a major <em>oh wow</em>
moment for me when I learned Rust: I had heard about memory safety but didn't
realize the same forces behind it were also responsible for making concurrency
safe!</p>
<p>This property is referred to as <em>fearless concurrency</em>. I encourage you
to read Aaron Turon's
<a href="https://blog.rust-lang.org/2015/04/10/Fearless-Concurrency.html">Fearless Concurrency</a>
as well as the
<a href="https://doc.rust-lang.org/book/ch16-00-concurrency.html">Fearless Concurrency</a>
chapter in the Rust Book as well.</p>
<h3>Operating Systems Abstractions Ground in Reality</h3>
<p>Rust is the only programming language I've used that attempts to expose
operating system primitives like environment variables, command arguments,
and filesystem paths and doesn't completely mess it up. Truth be told,
this is kind of a niche topic. But as I help maintain a version control
tool which needs to care about preserving content identically across
systems, this topic is near and dear to my heart.</p>
<p>In POSIX land, primitives like environment variables, command arguments,
and filesystem paths are <code>char*</code>, or a bag of null-terminated bytes.</p>
<p>On Windows, these primitives are <code>wchar_t*</code>, or wide bytes.</p>
<p>On both POSIX and Windows, the encoding of the raw bytes can be... complicated.</p>
<p>Nearly every programming language / standard library in existence attempts
to normalize these values to its native string type, which is typically
Unicode or UTF-8. That's doable and correct a lot of the time. Until it
isn't.</p>
<p>Rust, by contrast, has standard library APIs like
<a href="https://doc.rust-lang.org/stable/std/env/fn.vars.html">std::env::vars()</a>
that will coerce operating system values to Rust's UTF-8 backed <code>String</code>
type. But Rust also exposes the
<a href="https://doc.rust-lang.org/stable/std/ffi/struct.OsString.html">OsString</a>
type, which represents operating system native strings. And there are
function variants like
<a href="https://doc.rust-lang.org/stable/std/env/fn.vars_os.html">std::env::vars_os()</a>
to access the raw values instead of the UTF-8 normalized ones.</p>
<p>Rust <a href="https://doc.rust-lang.org/stable/std/path/index.html">paths</a> internally
are stored as <code>OsString</code>, as that as the value passed to the C API
to perform filesystem I/O. However, you can coerce paths to <code>String</code>
easily enough or define paths in terms of <code>String</code> without jumping through
hoops.</p>
<p>The point I'm trying to get across is that Rust's abstractions are ground
in the reality of how computers work. <strong>Given the choice, Rust will rarely
sacrifice the ability to do something correctly.</strong> In cases like operating
system interop, Rust gives you the choice of convenience or correctness,
rather than forcing inconvenience or incorrectness on you, like nearly
every other language.</p>
<h3>Encoding and Enforcing Invariants in the Type System</h3>
<p>Rust <a href="https://doc.rust-lang.org/std/keyword.enum.html">enums</a> are
<a href="https://en.wikipedia.org/wiki/Algebraic_data_type">algebraic data types</a>.
Rust enum variants can have values associated with them and Rust enums,
like structs (Rust's main way to define a type), can have
functions/methods hung off of them. Rust enums are effectively
fully-featured, specialized types, where value instances must be a
certain variant of that enum. <strong>This makes Rust enums much more powerful
than in other languages where enums simply map to integer values and/or
can't have associated functions. This power unlocks a lot of possibility
and harnessed the right way can drastically improve correctness of code
and lead to fewer defects.</strong></p>
<p>Programming inevitably needs to deal with invariants, the various
possibilities that can occur. Programmers will reach for control flow
operators to handle these: <em>if x do this</em>, <em>else if y do that</em>, <em>switch</em>
statements, and the like. Handling every possible invariant can be complex,
especially as software evolves over time and the ground beneath you is
constantly shifting.</p>
<p><strong>As you become more familiar with Rust, you'll find yourself encoding
and enforcing invariants in the type system more and more. And enums
are likely the main way you accomplish this.</strong></p>
<p>Let's start with a contrived example. In C/C++, if you had a function
that accepted either an <code>Apple</code> or an <code>Orange</code> value, you might do
something like: <code>void eat(Apple* apple, Orange* orange)</code>. Then you'd
have inline logic like <code>if apple != null</code>. In a dynamically typed
language, you could pass a single argument, but you'd perform inline
type comparison. e.g. with Python you'd write <code>if isinstance(fruit, Apple)</code>.</p>
<p>With Rust, you'd declare and use an enum. e.g.</p>
<div class="pygments_murphy"><pre><span></span><span class="k">struct</span> <span class="nc">Apple</span><span class="w"> </span><span class="p">{}</span><span class="w"></span>
<span class="k">struct</span> <span class="nc">Orange</span><span class="w"> </span><span class="p">{}</span><span class="w"></span>

<span class="k">enum</span> <span class="nc">Fruit</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="n">Apple</span><span class="p">(</span><span class="n">Apple</span><span class="p">),</span><span class="w"></span>
<span class="w">    </span><span class="n">Orange</span><span class="p">(</span><span class="n">Orange</span><span class="p">),</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>

<span class="k">impl</span><span class="w"> </span><span class="n">Fruit</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="k">fn</span> <span class="nf">eat</span><span class="p">(</span><span class="o">&amp;</span><span class="bp">self</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">        </span><span class="k">match</span><span class="w"> </span><span class="bp">self</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">            </span><span class="n">Self</span>::<span class="n">Apple</span><span class="p">(</span><span class="n">apple</span><span class="p">)</span><span class="w"> </span><span class="o">=&gt;</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="p">...</span><span class="w"> </span><span class="p">},</span><span class="w"></span>
<span class="w">            </span><span class="n">Self</span>::<span class="n">Orange</span><span class="p">(</span><span class="n">orange</span><span class="p">)</span><span class="w"> </span><span class="o">=&gt;</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="p">...</span><span class="w"> </span><span class="p">},</span><span class="w"></span>
<span class="w">        </span><span class="p">}</span><span class="w"></span>
<span class="w">    </span><span class="p">}</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>

<span class="kd">let</span><span class="w"> </span><span class="n">apple</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Fruit</span>::<span class="n">Apple</span><span class="p">(</span><span class="n">Apple</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="p">});</span><span class="w"></span>
<span class="n">apple</span><span class="p">.</span><span class="n">eat</span><span class="p">();</span><span class="w"></span>
</pre></div>

<p>This (again contrived) example shows how we Rust enum variants can hold
inner values, how we can define methods on Rust enums (so they behave like
regular types), and introduces the <code>match</code> control flow operator.</p>
<p>Quickly, <a href="https://doc.rust-lang.org/book/ch06-02-match.html">match</a> is a
super powerful operator. It will compare its argument against provided
patterns and evaluate the arm that matches. Patterns <strong>must</strong> be comprehensive
or the compiler will error. In the case of enums, if you add a variant - say
<code>Banana</code> for our <code>Fruit</code> example - and fail to add that variant to existing
<code>match</code> expressions, you will get compiler errors!</p>
<p>As you become more proficient with Rust, you'll find yourself moving
lots of (often redundant) control flow expressions and conditional dispatch
(<em>if X do this, if Y do that</em>) into enum variants and encoding the dispatched
actions into that enum/type directly. Conceptually, this is logically
little different from having a base type or interface or by having a single
<em>wrapper</em> class holds various possible values. But the guarantees are stronger
because each distinct possibility is strongly defined as an enum variant.
And when combined with the <code>match</code> control flow operator, you can have
the Rust compiler verify that all variants are accounted for every time
you take conditional action based on the variant.</p>
<p>The 2 most common enums in Rust are <code>Option</code> and <code>Result</code>. The following
sections will explain how they work and further demonstrate how invariants
can be encoded and enforced in Rust's type system.</p>
<h3><code>Option</code>: A Better Way to Handle Nullability</h3>
<p>Many programming languages have the concept of
<a href="https://en.wikipedia.org/wiki/Nullable_type">nullable types</a>: the ability
for a value to be null or some null-like value. You will often find this
expressed in languages as <code>null</code>, <code>nil</code>, <code>None</code>, or some variant thereof.</p>
<p>When programming in these languages, nullable values <strong>must</strong> be accounted
or it could lead to errors. Languages like C/C++ and Go will attempt to
to resolve the address behind <code>null</code>/<code>nil</code>, leading to at least a program
crash and possibly a security vulnerability. Languages like Java and Python
will raise exceptions (<code>NullPointerException</code> in Java - frequently abbreviated
<code>NPE</code> because it is so common - and likely <code>TypeError</code> in Python).</p>
<p>The prevalence of failure to account for nullable values is a major reason
why null references were coined by their inventor as
<a href="https://www.infoq.com/presentations/Null-References-The-Billion-Dollar-Mistake-Tony-Hoare/">the billion dollar mistake</a>.
(I suspect the real world value is much greater than $1B.)</p>
<p>Having an easy-to-ignore nullable invariant lingering in type systems seems
like a massive foot gun to me. And indeed every programmer with sufficient
experience has likely introduced a bug due to failure to account for null. I
sure have!</p>
<p><strong>Rust doesn't have a null value. Therefore no null references and no
<em>billion dollar mistake</em>. Instead, Rust's standard library has
<a href="https://doc.rust-lang.org/std/option/index.html">Option<T></a>, an enum
representing nullable types / values. And <code>Option</code> is vastly superior to
null values.</strong></p>
<p><code>Option&lt;T&gt;</code> is an enum with 2 variants, <code>Some(T)</code> or <code>None</code>: an instance of some
type or <em>nothing</em>. What makes <code>Option</code> different from languages with null
references is you have to explicitly ask for the inner value: there is no
automatic dereference. <strong>Rust forces you to confront the reality that a
value is nullable and by doing so can drastically reduce a very common bug
class.</strong> I say <em>drastically reduce</em> instead of <em>eliminate</em> because it is
still possible to shoot yourself in the foot. For example, you can call
<a href="https://doc.rust-lang.org/std/option/enum.Option.html#method.unwrap">Option.unwrap()</a>
to obtain the inner value, triggering a panic if the <code>None</code> variant is
present. Despite the potential for programming errors, this solution is
strictly better than null references because <code>Option</code> forces you to confront
the reality of nullability and use of the <em>dangerous</em> access mechanisms is
relatively easy to audit for. (Clippy has some lints to encourage best
practices here.)</p>
<p>The existence of <code>Option&lt;T&gt;</code> means that if you are operating on a non-<code>Option</code>
value, that value is guaranteed to exist and not be null. If you are operating
on <code>Option</code>, the fact it is optional is explicitly encoded in the type and you
know you need to account for it. <strong>If the value passed into a function was once
always defined and a later refactor changed it to be optional (or vice versa), that
semantic change is reflected in the type system and Rust forces you to confront
the implications when that change is made,</strong> not after it was deployed to
production and you started seeing segfaults, NPEs, and the like.</p>
<p><strong>After using Rust's <code>Option&lt;T&gt;</code> to express nullability, you will look at
every other language with null references and bemoan how <em>primitive</em> and
<em>unsafe</em> it feels by comparison. You will yearn for Rust's safer approach
biasing towards correctness and higher quality software.</strong> <code>Option&lt;T&gt;</code> is
massive feature for the professional programmer who cares about these
traits.</p>
<h3><code>Result</code>: A Better Way to Handle Errors</h3>
<p>Different programming languages have different ways of handling errors.
Returning integers or booleans to express success or failure is common.
As is throwing and trapping/catching exceptions.</p>
<p>Like nullability, history has shown us that programmers often fail to
handle error invariants, with bugs of varying severity ensuing. Even Linux
filesystems
<a href="http://research.cs.wisc.edu/wind/Publications/iron-sosp05.pdf">fail</a> to
<a href="http://usenix.org/legacy/event/fast08/tech/full_papers/gunawi/gunawi_html/index.html">handle</a>
errors!</p>
<p><strong>I argue that the traditional programming patterns we use to handle errors bias
towards buggy outcomes, especially with the <em>return an integer/error value</em>
approach.</strong> It is easy to forget to check the return value of a function. In
C/C++, maybe a function once returned nothing (<code>void</code>) and was later refactored
to return an integer error code. You have to know to audit for existing callers
when making these changes or updating dependencies. Furthermore, handling errors
requires effort. That <code>if err != 0</code> or <code>if err != nil</code> pattern gets mighty
annoying to type all of the time! Plus, you have to know what value to compare
against: <em>success</em> can often be 0, -1, or 1 or any other arbitrary value.
<strong>Getting error handling correct 100% of the time is <em>hard</em>. You will fail and
this will lead to bugs.</strong></p>
<p><a href="https://doc.rust-lang.org/std/result/index.html">Result<T, E></a> is Rust's
primary/preferred mechanism for propagating errors and it is different
from traditional approaches.</p>
<p>Like <code>Option&lt;T&gt;</code>, <code>Result&lt;T, E&gt;</code> is an enum with 2 variants: <code>Ok(T)</code> and
<code>Err(E)</code>. That is, a value is either <em>success</em>, wrapping an inner value of
type <code>T</code> or <em>error</em>, wrapping an inner value of type <code>E</code> describing that
error.</p>
<p>Like <code>Option&lt;T&gt;</code>, <code>Result&lt;T, E&gt;</code> forces you to confront the existence of
invariants. Before operating on the value returned by a function, you need
to explicitly access it and that forces you to confront that an error could
have occurred. In addition, the <code>Result</code> type is
<a href="https://doc.rust-lang.org/std/result/index.html#results-must-be-used">annotated</a>
and the compiler will emit a warning when you don't check it. Scenarios like
changing an infallible function returning a type <code>T</code> to fallible returning a
<code>Result&lt;T, E&gt;</code> will fail to compile (due to typing violations) or make compiler
warning noise if there are call sites that fail to account for that change.</p>
<p>In addition to making it more likely that errors are acted upon correctly,
Rust also contains a
<a href="https://doc.rust-lang.org/std/result/index.html#the-question-mark-operator-">? operator</a>
for simplifying handling of errors.</p>
<p>As I said above, typing patterns like <code>if err != 0</code> or <code>if err != nil</code> can become
extremely tedious. Your brain knows what it needs to type to handle errors
but it takes precious seconds to do so, slowing you down. You may have code where
the majority of the lines are the same error handling boilerplate over and over,
increasing verbosity and arguably decreasing readability.</p>
<p>Rust's <code>?</code> operator will <code>return</code> an <code>Err(E)</code> variant or evaluate to the
inner value from the <code>Ok(T)</code> variant. So you can often add an <code>?</code>
operator after a function call returning a <code>Result&lt;T, E&gt;</code> to automatically
propagate an error. Typing a single character is vastly easier and simpler
than writing explicit control flow for error handling!</p>
<p>The benefits of <code>?</code> are blatantly apparent when you have functions calling
into multiple fallible functions. Long functions with multiple <code>if err != 0</code>
blocks followed by the next logical operation often reduce to a 1-liner. e.g.
<code>bar(foo()?)?</code> or <code>foo.do_x()?.do_y()?</code>. When I said earlier that Rust feels
like a higher level language, the <code>?</code> operator is a significant contributor to
that.</p>
<p>There are some downsides to <code>Result&lt;T, E&gt;</code> in terms of programming overhead
and consistency between Rust programs. I'll cover these later in the post.</p>
<p><strong><code>Result&lt;T, E&gt;</code> biases Rust code towards correctness by forcing programmers
to confront the reality that an error could exist and should be handled.
Once you program in Rust, you will look at error handling mechanisms like
returning an error integer or nullable value, realize how brittle and/or
tedious they are, and yearn for something better.</strong></p>
<h3>The <code>unsafe</code> Escape Hatch</h3>
<p>If some of Rust's limitations are too much for you, Rust has an
<em>in case of emergency break glass</em> feature called <code>unsafe</code>. This is kind of
like <em>C mode</em> where you can do things like access and manipulate raw memory
through pointers. You can <em>cast</em> a value to a pointer and back to a new Rust
reference/value, effectively short circuiting the borrow checker for that
particular reference/value.</p>
<p><strong>A common misconception is <code>unsafe</code> disables the borrow checker and/or loosens
type checking. This is incorrect: many of those features are still running
in <code>unsafe</code> code.</strong> However, because Rust can't fully reason about what's
happening (e.g. it doesn't know who owns a raw memory address and when
it will be freed), it can't properly enforce all of its rules that guarantee
safety, leading to, well, <em>unsafety</em>. (See
<a href="https://doc.rust-lang.org/book/ch19-01-unsafe-rust.html">Unsafe Rust</a> for
more on this topic.)</p>
<p><code>unsafe</code> is a necessary evil. In many Rust programs, you won't have to
ever use it. But if you do have to use it, its presence will draw review
scrutiny like moths to light. So unlike say C/C++ where practically every
memory access is a potential security bug and it is effectively impossible
in many scenarios to comprehensively audit for memory safety (if there were
there would be no memory safety bugs), using <code>unsafe</code> safely is often viable
because scrutiny can be concentrated on its relatively few occurrences.
And more experienced Rust programmers know how to encapsulate <code>unsafe</code> into
<em>safe</em> wrappers, limiting how much code needs to be audited when code
around <code>unsafe</code> changes.</p>
<p>What I've personally been enlightened by is the myriad of operations
that Rust considers <em>unsafe</em>. As you learn more and more Rust, you'll
encounter random functions sprinkled across the standard library that
are <code>unsafe</code> and you'll wonder why. The docs usually tell you and that's
how you learn something new (and maybe horrifying) about how computers
actually work.</p>
<h3>Fearless Refactoring</h3>
<p>A significant portion of the software development lifecycle is evolving
existing code. Fixing bugs. Extending existing code with new
functionality. Refactoring code to fix bugs or prepare for new features.
Using code in new, unplanned ways.</p>
<p>In many code bases, the amount of people time spent evolving the code
dwarfs the time for creating actual greenfield code/features.
(Unfortunately, quantifying when you are doing <em>evolution</em> versus
greenfield coding is quite difficult, so both facets often get lumped
together into simply <em>software development time</em>. But in my mind they are
discrete - although highly interdependent - units of work and the evolution
time tends to dwarf the greenfield time on established projects.) <strong>So it
follows that long-term evolution/maintainability of code bases is more
important than initial code creation time.</strong></p>
<p><strong>There is a sufficient body of industry research demonstrating that the
cost to fix defects rises exponentially as you progress through the
software development lifecycle</strong> (do a search for say <em>software development
lifecycle cost of fixing a bug</em>).</p>
<p>Furthermore, human memory functions not unlike multi-tier caches and your
ability to recall information will diminish over time. (You probably know
what you were doing 5 minutes ago, might remember what you were doing at
this time yesterday, and probably have no clue what you were doing on this
date 20 years ago.)</p>
<p><strong>In terms of coding, the best way to address a defect is to not introduce
it in the first place. If you can't do that, your goal is to detect and
correct it as early in the development process as possible, as close as
possible to when the source code creating that defect came into existence.</strong>
Practically, in order of descending desirability:</p>
<ol>
<li>Don't introduce defect (this is impossible because humans are fallible).</li>
<li>Detect and correct defect as soon as the bad key press occurs (within
   reason: you don't want the programmer to lose too much flow) (milliseconds
   later).</li>
<li>At next build / test time (seconds or minutes later).</li>
<li>When code is shared with others (maybe you push a branch and CI tells
   you something is wrong) (minutes to days later).</li>
<li>During code review (minutes to days later).</li>
<li>When code is integrated (e.g. merged) (minutes to days later).</li>
<li>When code is deployed (minutes to days or even months later).</li>
<li>When a bug is reported long after the code has been deployed (weeks
   to years later).</li>
</ol>
<p>The earlier a defect is caught, the better the chances that the author
(or other involved parties) have relevant code <em>paged in</em> and can fix it
with less effort and with lower chances of introducing additional defects.
For me, authoring new code is relatively easy compared to refactoring old
code. That's because I have new code fully paged into my brain and I know
it like the back of my hand. I know where the sharp edges are and how
you'll get cut if you make certain changes. However, if several months
pass without revisiting the code, most of that heightened awareness
evaporates. If I need to change or review that code, my ability to do
that with a high degree of confidence and efficiency is drastically
eroded.</p>
<p>Generally speaking, the earlier a defect is caught, the less damage it can
do. Ideally, a defect is caught and fixed at local development time, before
you burden a reviewer with finding it and certainly before it causes harm or
anti-value after being deployed!</p>
<p>In addition, compressing the software development lifecycle allows you
to ship enhancements sooner, which enables you to deliver value sooner.
This is what we're trying to do as professional programmers after all!</p>
<p><strong>Because the cost to fix a defect rises exponentially as it moves through
the software development lifecycle, it follows that you want defect
detection to occur logarithmically to offset that cost.</strong> That means you
want as many defects as possible to be caught as early as possible.</p>
<p><strong>Compared to other programming languages I've used, Rust is exceptional
at detecting defects earlier in the development lifecycle and as a result
can drastically lower overall development costs.</strong> Here are the main factors
contributing to this belief:</p>
<ul>
<li>The type system is relatively strong and prevents many classes of bugs.</li>
<li>The borrow checker and the rules it enforces prevent <em>safety</em> issues
  at <em>compile</em> time. Some of these violations can be detected by other
  languages' compilers. However, in many cases sufficient auditing
  (like {address, memory, thread} sanitizers) is run much less frequently,
  often only in CI tests, which can be hours or days later.</li>
<li>Confidence that the above 2 function as advertised.</li>
<li>Invariants can be encoded and enforced in the type system through features
  like enums being algebraic data types.</li>
<li>Variables are immutable by default and must be explicitly annotated
  as mutable. This forces you to think about where and how data mutation
  occurs, enabling you to spot issues sooner.</li>
<li><code>Option&lt;T&gt;</code> significantly curtails the <em>billion dollar mistake</em>.</li>
<li><code>Result&lt;T, E&gt;</code> forces you to reckon about handling errors.</li>
</ul>
<p>The Rust compiler is just exceptional at detecting common defects.</p>
<p>Did your code refactor introduce a use-after-free or dangling reference?
Don't worry: the borrow checker will detect that. CVE prevented.</p>
<p>Did you introduce a race condition by performing a mutation somewhere
that was previously immutable? The borrow checker will detect that. You
potentially just saved hours of time debugging a hard-to-reproduce bug.</p>
<p>Did you add an enum variant but forget to add that variant to a
<code>match</code> expression? If you avoided using the <em>match all</em> <code>_</code>
expression, the compiler will tell you match arms aren't exhaustive
and give you an error.</p>
<p>Did a value that was previously always defined become nullable? Changing
the type from <code>T</code> to <code>Option&lt;T&gt;</code> will yield compiler errors due to type
mismatch.</p>
<p>Did an <code>Option&lt;T&gt;</code> that was previously always <code>Some(T)</code> suddenly
become <code>None</code>? Hopefully following Rust best practices mean your code
will just work. In the worst case you get a panic (with a stack trace).
But that's on par with say a Java NPE and is strictly better than a
null dereference that you get with languages like C/C++.</p>
<p>Did you change or add a function returning <code>Result&lt;T, E&gt;</code> but forget
to check if that <code>Result</code> is an <code>Ok(T)</code> or <code>Err(E)</code>, the compiler
will tell you.</p>
<p>I could go on. Rust is full of little examples like these where the
core language and standard library nudge you towards working code and
help detect defects earlier during development, saving vast amounts
of time and money later.</p>
<p><strong>The Rust compiler is so good at rooting out problems that many Rust
programmers have adopted the expression, <em>if it compiles it works</em>. This
statement is obviously falsifiable. But compared to every other programming
language I've used, I'm shocked by how often it is true.</strong></p>
<p>For other programming languages, a working compile is the beginning of your
verification or debugging journey. For Rust, it often feels like the hard
part is over and you are almost done. With other languages, you often
have an indefinite number of iterations to fix <em>language defects</em> (like
null dereferences or dynamic typing errors) beyond the compile step. You
need to address these in addition to any <em>logical/intent defects</em> in your
code. And fixing logical/intent defects could introduce more post-compile
defects. As a programmer, you just don't know when the process will be
done. With Rust, the compiler errors tell you exactly what the <em>language
defects</em> are. So by the time you appease the compiler, you are left with
just your <em>logical/intent</em> defects. I greatly prefer the Rust workflow
which separates these because I'm getting clearer feedback on my progress:
I know that once I've addressed all the <em>language defects</em> the compiler
complains about that is <em>just</em> a matter of fixing <em>logical/intent</em>
defects. I know I'm a giant step closer to victory.</p>
<p><a href="https://hbr.org/2011/05/the-power-of-small-wins">The Progress Principle</a>
is a psychological observation that people tend to prefer a series of
more smaller wins over fewer larger wins. And (unexpected) setbacks can
more than offset the benefits of wins. (The book is an easy read and
I've found its insights applicable to software development workflows.)
Whether Rust's language designers realized it or not, Rust's development
workflow plays into our psychological dispositions as described by <em>The
Progress Principle</em>: defects (setbacks) tend to occur earlier (at compile
time), not at unexpected later times (during code review, CI testing,
deploy, etc) and our progress towards a working solution is composed of
small wins, such as fixing compiler errors and knowing when you transition
from <em>language defects</em> into <em>logical/intent</em> defects. For me, this makes
iterating on Rust more <em>fulfilling</em> and <em>enjoyable</em> than other languages.</p>
<h2>Rust Makes You a Better Overall Programmer</h2>
<p>Whether you realize it or not, every programmer has a personal, generalized
model of how to program, how to reason about code, best practices, and
what not. When we program, we specialize that model to the language
and environment/project we're programming for. The mental model that
each of us has its shaped by our experience: which languages we know,
which concepts we've been exposed, mistakes we've made, people we've
worked with and the practices they've instilled.</p>
<p><strong>If for no other reason, you should learn Rust to expand your generalized
model of how to program so that you can apply Rust's principles outside
of Rust.</strong></p>
<p>Before I learned Rust, I had a mental model of the <em>lifetimes</em> of various
values/variables/memory and how they would be used. If I were coding C, I
would attempt to document these in function comments. e.g. if returning a
pointer, the comment would say how long the memory behind that pointer lives
or who is responsible for freeing it. So when I encountered Rust's ownership
and reference rules when learning Rust, they substantially overlapped with
my personal mental model of how you should reason about memory in order to avoid
bugs. I distinctly remember reading the Rust Book and thinking <em>wow, this
seems to be a formalization of some of the concepts and best practices living
in my head!</em></p>
<p>After using Rust for several months, I realized that my prior mental
model around reasoning about <em>safe</em> program behavior was woefully
incomplete and that Rust's was far superior.</p>
<p><strong>Rust's <em>different</em> ways of doing things will inevitably force you to
think about type design, data access patterns, control flow, etc more
than most other programming languages. In most other languages, it is much
easier to just write runnable code and defer the complexity around
ensuring the code is <em>safe</em>/<em>correct</em> and free from certain classes of
bugs, like memory access violations and race conditions. Rust's ways of
doing things forces you to confront many of these problems up-front,
before anything runs.</strong></p>
<p>Rust's stricter model and way about authoring software eventually percolates
into your personal generalized model of how to program in <em>any programming
language</em>. <strong>As you internalize patterns needed to program Rust proficiently,
you will subconsciously cherry-pick aspects of Rust and apply them when
programming in other languages, making you a better programmer in those
languages.</strong></p>
<p>For example, when you program C/C++, you will realize the minefield of
memory safety issues that linger in those languages. Many of those mines
never explode. But knowing Rust and the patterns needed to appease the
borrow checker and write <em>safe</em> code, you have a better sense of where the
mines are located, the patterns that lead to them exploding, and you can
take preemptive steps or apply extra scrutiny to avoid tripping
them. (If you are like me, you'll reach the conclusion that C/C++ is
intrinsically unsafe and is beyond saving, vowing to avoid it as much as
possible because it is just too dangerous to use safely/responsibly.)</p>
<p>Similarly, when programming in any language, you'll probably think more about
variable mutability and non-mutability, even if those languages don't have
the concept of mutability on variables. You'll be more attune to certain
patterns for mutating data: where mutation occurs, who has a mutable
reference, when there are both mutable and non-mutable references in
existence. Again, your knowledge from Rust will subconsciously raise your
awareness for classes of bugs, making you a better programmer.</p>
<p>The same thing applies to multi-threaded programming and race conditions.
After internalizing Rust's model of how to achieve multi-threading safely,
you will probably not look at multi-threading in other languages the same
way again. If you are like me, you will be horrified by how the lack of
Rust's enforced ownership/reference rules predisposes code to so many
horrible and hard-to-debug bugs. Again, you will probably find yourself
changing your approach to multi-threading to minimize risk.</p>
<p>Fun fact: while at Mozilla I heard multiple anecdotes of [very
intelligent] Firefox developers thinking they had found a bug in Rust's
borrow checker because they thought it was impossible for a flagged error
to occur. However, after sufficient investigation the result was always
(maybe with an exception or two because Mozilla adopted Rust very early)
that the Rust compiler was correct and the developer's assertions about
how code could behave was incorrect. In these cases, the Rust compiler
likely prevented hard-to-debug bugs or even exploitable security
vulnerabilities. I remember one developer exclaiming that if the
bug had shipped, it would have taken <em>weeks</em> to debug and would likely
have gone unfixed for <em>years</em> unless its severity warranted staffing.</p>
<p><strong>I strongly feel that I am a better programmer overall after learning
Rust because I find myself applying the [best] practices that Rust enforces
on me when programming in other languages. For this reason, even if you
don't plan to use Rust in any serious capacity, I encourage people to learn
Rust because exposure to its ideas will likely transform the ways you think
about programming for the better.</strong></p>
<h2>Rust Downsides and Dispelling Some Rust Myths</h2>
<p>This post has been rather positive about Rust so far. Rust, like
everything, is far from perfect and it has its downsides. Professionals
know the limitations of their tools and you should know some of the
issues you'll run into when using Rust.</p>
<p>In addition, Rust is still a relatively young and unpopular programming
language. Since relatively few people know Rust, there are a handful of
myths and inaccuracies circling about the language. I'll also dispel some
of those here.</p>
<h3>Steeper Learning Curve</h3>
<p>A common criticism levied against Rust is it is harder to learn than
other programming languages. I think this is a valid concern. My
experience is Rust took longer to learn and level-up than other
languages I've learned recently, notably Go, Kotlin, and Ruby.</p>
<p>I think the primary reason for this is the borrow checker and the
rules it enforces. Many programmers have never encountered forced
following of ownership and reference rules before and this concept is
completely foreign at first. I liken it to <em>a new way to program</em>.
If you only have experience with dynamically typed languages that
will allow you to compile a ham sandwich, there's a good chance you'll
be frustrated by Rust. Rust will likely challenge your conceptions of
how programming should work and may frustrate you in the process.</p>
<p>In addition to the borrow checker itself, there are a myriad of types
and patterns you'll encounter and eventually need to understand to
<em>appease</em> the borrow checker.</p>
<p>Beyond the borrow checker, Rust's standard library is comprehensive and
offers a lot of types and traits. It will take a while to be exposed
to many of them and know when/how to use each.</p>
<p>You will likely be adding 3rd party crates as dependencies to
your project for common functionality not (yet) in the standard library.
These expand the scope of concepts you need to learn.</p>
<p>I hope I'm not scaring anybody away: you can go pretty far in Rust without
encountering or understanding most of the standard library. That being
said, every new type, trait, concept, and crate you learn unlocks new
possibilities and avenues for delivering value through programming. So
there is an incentive to take the time to learn them sooner than later.</p>
<p>I learned Rust mostly independently for a personal project. While
learning resources such as <a href="https://www.rust-lang.org/learn">Learn Rust</a>,
the <a href="https://cheats.rs/">Rust Language Cheat Sheet</a>, and even Clippy
are fantastic, in hindsight I probably would have become more proficient
sooner had I contributed to an existing Rust project and/or had ongoing
technical collaboration with more experienced Rust developers. This is
probably no different than any other programming language. But because of
Rust's steeper learning curve, I think the benefits of peer exposure are
more significant. That being said, I've heard anecdotes of teams with no
Rust experience learning Rust together with successful results. So there's
no formal recipe for success here.</p>
<p>Finally, despite the steeper learning curve, I'd say the return on
investment pays off pretty quickly. As I've argued elsewhere in this
post, the Rust compiler and type system helps prevent many classes of
bugs. So while it may take longer to initially learn and compose idiomatic
Rust code, it won't take long for Rust to offset the time that you would
have spent chasing bugs, performance optimizations, and the like.</p>
<h3>Rust Moves Too Fast</h3>
<p>Rust releases a new version every 6 weeks. By contrast, many other
programming languages release ~yearly. This faster release cadence has
been a common complaint about Rust.</p>
<p>Quickly, I think people conflate release cadence with churn and hardship
from that release cadence. <strong>Generally speaking, release cadence isn't the
thing you care about: it's how disrupted you are from the releases.</strong> If
your old release continues to work just as well as the new release,
release cadence doesn't really matter (many major websites deploy/release
dozens of times per day and you don't care because you can't tell: you only
care when the UI or behavior changes). <strong>So the thing most of us care about
is how frequently Rust releases cause disruption.</strong> And disruption is often
caused by backwards incompatibility and the introduction of new features,
which when adopted, force upgrades.</p>
<p>A few years ago, I think the concern that <em>Rust moves too fast</em> was
valid: there were significant features in seemingly every release and
crates were eager to jump on the new features, forcing you to upgrade
if you wanted to keep your dependency tree up to date. I feel like I
caught the tail end of this relative chaos in 2018-2019.</p>
<p>But in the last 18-24 months, things seem to have quieted down. Many
of the major language features that people were eager to jump on have
landed. The only ongoing churn I'm aware of in Rust is in the async
ecosystem, and that seems to be stabilizing. New Rust releases are
generally pretty quiet in terms of <em>must use</em> features. The last
<em>milestone</em> release in my mind was 1.45 in July 2020, which stabilized
procedural macros. The community was pretty quick to jump on that
feature/release. My Rust projects have targeted 1.45+ for a while now
with minimal issues.</p>
<p>9 months with no major disruptions is on par with the release cadence
of other programming languages.</p>
<p><strong>In my opinion, the concern that <em>Rust moves too fast</em>, while once valid,
no longer generally applies.</strong> Pockets of truth for segments of users caring
about niche and lesser-used features, yes. But nothing that applies to the
entire Rust ecosystem.</p>
<h3>Compiling Is Too Slow</h3>
<p>A lot of people have commented that Rust builds take too long. It is
true: compiling Rust tends to take longer than C/C++, Go, Java, and
other languages requiring an ahead-of-time compile step.</p>
<p>While a lot has been done to make the Rust compiler faster (it feels
substantially faster than it was a few years ago), it still isn't
as fast as other languages.</p>
<p>Not to dismiss the problem, but in a lot of cases, the speed of Rust
compilation is <em>fast enough</em>. Incremental builds for small libraries
or programs will take a few hundred milliseconds to a second or two.
I suspect most of the people complaining about build times today are
developing very large Rust programs (tens of thousands of lines of
code and/or hundreds of dependencies).</p>
<p>A contributing problem to build times is dependency count. The simplicity
of Cargo makes it very easy to accumulate dependencies in Rust and
each additional crate will slow your build down. PyOxidizer has
~400 dependencies at this point in time, for example (I've been
throwing the kitchen sink at it in terms of features).</p>
<p>There are a few things under your control to mitigate this problem.</p>
<p>First, install <a href="https://github.com/mozilla/sccache">sccache</a>, a
transparent compiler cache. By default it caches to the local
filesystem. But you can also point it at Redis, Memcached, or blob
stores in AWS, Azure, or GCP. Firefox's CI uses an S3 backed cache and
the hit rate (for both Rust and C/C++) is 90-99% on nearly every build.
For PyOxidizer - a medium sized Rust project - sccache reduces full
build times from ~53s wall and ~572s CPU to ~32s wall to 225s CPU on
my 16 core Ryzen 5950X. The wall time savings on a lower CPU core count
machine are even more significant.</p>
<p>Speaking of CPU core counts, the second thing you can do is give
yourself access to more CPU cores. Laptops tend to have at most 4
CPU cores. Consider buying desktops or moving builds to remote
machines, often with dozens of CPU cores. This requires spending
money. But when you factor in people time saved and the cost of
that time and the value of someone's happiness/satisfaction, it
can often be justified.</p>
<p>I'm not trying to dismiss the problems that slow builds can impose,
but if you want to justify their cost, you can argue that the Rust
compiler does more at compilation time than other languages and that
this overhead brings benefits, such as preventing bugs earlier in
the software development lifecycle.
<a href="https://en.wikipedia.org/wiki/There_ain%27t_no_such_thing_as_a_free_lunch">There's no such thing as a free lunch</a>
and Rust's relatively slower builds are a <em>tax</em> you pay for the correctness
the compiler guarantees. To me, that's a justifiable trade-off.</p>
<h3>Rust is Too Young or Isn't Production Ready</h3>
<p>The <em>isn't production ready</em> concern is likely disproven by the
existence of Rust in production in critical roles at a sufficient
number of reputable companies. At this point, there are very few
technical reasons to say Rust isn't production ready. Non-technical reasons
such as lack of organizational knowledge or a limited talent pool for
hiring from, yes. But little on the technical front.</p>
<p>The <em>too young</em> part is ultimately a judgement call for how comfortable
you are with new technologies.</p>
<p>I'm generally pretty conservative/skeptical about adopting new technology.
If you are in this industry long enough you eventually get humbled by your
exuberance.</p>
<p>I was probably in the <em>Rust is too young</em> boat as late as 2017, maybe
2018. While I was cheering on Rust as a Mozillian, I was skeptical it
was going to take off. Birthing successfully languages is hard. The
language still seemed to move too fast and have too many missing
features. Things seemed to stabilize around the 2018 edition. That's
also when you started commonly hearing of companies adopting Rust. Lots
of startups at first. Then big companies started joining in.</p>
<p>Today, companies you have heard of like Amazon, Cloudflare,
Discord, Dropbox, Facebook, Google, and Microsoft are adopting Rust to
varying degrees. There are 58,750 published crates on
<a href="https://crates.io/">crates.io</a>.</p>
<p>I won't drop names, but I've heard of Rust <em>spreading like wildfire</em> at
some companies you've heard of. The stories are pretty similar: random
person or team wants to try Rust. Something small and isolated with a
minimal blast radius in case of disaster is tried first. Rust is an
overwhelming success. As more and more people are exposed to Rust, they
<em>see the light</em>, cries for Rust become louder, and it becomes even more
widely adopted.</p>
<h3>The <em>I'm Writing Fewer Bugs</em> Trap</h3>
<p>When I program in Rust, I strongly feel that my base rate of defect introduction
is substantially less than other programming languages. I have confidence that
the Rust compiler coupled with practices like encoding and enforcing invariants
in the type system leads to fewer defects. In some cases I feel like the surface
area for bugs is limited to <em>logical defects</em>, which are mis-expressions of the
human programmer's intent. And since no automated tool can reliably scan for
<em>human intent</em>, there's no way to prevent <em>logical bugs</em>, and that surface area
is the best we can ever expect from automated scanning.</p>
<p>Knowing what tests to write and how much effort to invest in test writing
is a difficult skill to level up and is full of trade-offs. With Rust, I find
myself writing fewer tests than in other languages because I have confidence
that the compiler will detect issues that would otherwise require explicit
testing.</p>
<p>I <em>feel</em> that my beliefs and practices are rooted in reality and justifiable. Yet I
recognize the danger in placing too much faith in my tools, in Rust.</p>
<p>In theory, Rust alleviates the need for running additional verification
tools, like {address, memory, thread} sanitizers because the safe subset
of Rust prevents the issues these tools detect. Many defects caught by
fuzzing are also similarly prevented by the design of Rust (but not all:
fuzzing is generally a good idea).</p>
<p><strong>What I'm trying to say is that it is really easy to fall into a trap where
you are over-confident about the abilities of Rust to prevent defects and
you find yourself letting your guard down and not maintaining testing and
other verification best practices.</strong></p>
<p>I'm still evolving my beliefs in this area. But my general opinion is that you
should still run things like {address, memory, thread} sanitizers and fuzzing
because <code>unsafe</code> likely exists <em>somewhere</em> in the compiled code, as likely does
C or assembly code. And because a chain is only as strong as its weakest link,
it only takes <em>any</em> bug to undermine the <em>safety</em> of the entire system.
So while these additional verification tools likely won't find as many issues
as they would in <em>unsafe</em> languages, I still think it is a good idea to continue
to run them against Rust, especially for <em>high value</em> code bases.</p>
<h3>Error Handling</h3>
<p><code>Result&lt;T, E&gt;</code> isn't a panacea. Because errors are full on types rather
than simple primitives like integers, you need to spend effort reasoning
and coding about how different error types interact. And often you need
to write a bit of boilerplate code to facilitate that interaction. This
can cancel out a lot of the efficiency benefits of Rust's <code>?</code> operator
for handling errors.</p>
<p>There are a handful of 3rd party Rust crates specializing in error
handling that you'll likely to encounter. These include
<a href="https://crates.io/crates/anyhow">anyhow</a>,
<a href="https://crates.io/crates/error-chain">error-chain</a>,
<a href="https://docs.rs/failure/0.1.8/failure/">failure</a>, and
<a href="https://crates.io/crates/thiserror">thiserror</a>.</p>
<p>Rust's error handling landscape can at times feel fragmented and make
you yearn for something more defined/opinionated in the standard library.
The Rust Community recognizes that this is an area that can be improved
and has <a href="https://blog.rust-lang.org/inside-rust/2020/09/18/error-handling-wg-announcement.html">formed</a>
an error handling project group to improve this space. So hopefully we see
some quality of life improvements to error handling in time.</p>
<h2>Conclusion</h2>
<p>I am irrationally effusive about Rust. When I see this level of excitement
in others, I am extremely skeptical. I was skeptical myself when my former
colleagues at Mozilla were talking up Rust years ago. But having used Rust
for 2.5 years now and authored tens of thousands of lines of Rust code, the
initial relationship euphoria has worn off and I am most definitely in love.</p>
<p><strong>Cynically, Rust has ruined in programming in other languages for me. Less
cynically, Rust has spoiled me.</strong></p>
<p>When I look at other languages without the rules enforced by Rust's borrow
checker, all I see are sharp edges waiting to materialize into bugs.</p>
<p>When I look at other languages with <em>weaker</em> type systems, I think about
all the time I spend having to defend against invariants and how much
cognitive load and programming/review effort I need to incur to maintain
the baseline of quality that I get with Rust.</p>
<p>When I look at programming languages like Python, Ruby, and TypeScript
where you can bolt a type system onto a language that doesn't have it, I
think <em>why would I want to do that when I can use an even better type
system while likely achieving much better performance with Rust?</em> (It's
tempting to reach for a metaphor involving lipstick and pigs.)</p>
<p>When I look at other languages, I generally see the same pile of decades
old ideas packaged in different boxes. Some of these ideas are good and
probably timeless (e.g. functions and variables). Some are demonstrably bad
and should be largely excised from common use (e.g. null references - the
<em>billion dollar mistake</em>).</p>
<p>When I interface with Rust's tooling, I feel like it is respectful of my
time and has my best interests (producing working software) at heart. I
feel the maintainers of the tooling care about me.</p>
<p>When I program in Rust, I feel that I'm producing fewer defects overall.
The compiler is catching defects that would otherwise be caught later
in the software development lifecycle, leading to increased software
development costs.</p>
<p>When I interact with Rust's community of people, respect and empathy
abounds.</p>
<p>Does Rust have its problems and limitations? Of course it does: nothing is
perfect! But in my opinion, its trade-offs are often strictly better than
those found in other programming languages I've used.</p>
<p>At the end of the day, Rust is a programming language and therefore a tool.
Adept professionals know not to get too attached to your tools: ultimately
it is the value you deliver, not how you deliver it. (Of course the choice
of tools can significantly impact the quality and timeline of value
delivery!) Will my thoughts on Rust and preferred languages change over time
as the landscape shifts: of course they will! But for the time being, <strong>Rust
brings so much to the table that its <em>competition</em> lacks that I'm overly
excited about Rust and its ability to advance the state of
software/programming and therefore the industry.</strong></p>
<p>In closing, my current CTO uses the phrase <em>commitment to craft</em> as a desired
mindset for their technical organization. That phrase translates to various
themes: higher quality / lower defect rate, build with the long-term in mind,
implement efficient solutions, etc. <strong>Like an artist reaches for a preferred
paintbrush or a chef for a preferred knife because their preferred tool enables
them to better express their craft, I feel that Rust often enables me to better
express the potential of my professional craft more than other programming languages.
I strongly feel that Rust predisposes software to higher quality outcomes - both
in terms of defect rate and run-time efficiency - while also reducing total
development and execution costs over the entire software development lifecycle.
That makes Rust my first choice language - my go-to tool - for many new
projects at this point in time. If you likewise value <em>commitment to craft</em>, I
urge you to explore Rust so that you too can better harness the potential
of our programming craft.</strong></p>
<p>But don't take my word on it, read what
<a href="https://kerkour.com/blog/rust-in-production-2021/">42 companies using Rust in production</a>
have to say.</p>]]></content>
  </entry>
</feed>

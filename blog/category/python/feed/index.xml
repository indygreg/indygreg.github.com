<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0"
     xmlns:content="http://purl.org/rss/1.0/modules/content/"
     xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
     xmlns:atom="http://www.w3.org/2005/Atom"
     xmlns:dc="http://purl.org/dc/elements/1.1/"
     xmlns:wfw="http://wellformedweb.org/CommentAPI/"
     >
  <channel>
    <title>Gregory Szorc's Digital Home</title>
    <link>http://gregoryszorc.com/blog</link>
    <description>Rambling on</description>
    <pubDate>Tue, 07 Mar 2017 20:08:45 GMT</pubDate>
    <generator>Blogofile</generator>
    <sy:updatePeriod>hourly</sy:updatePeriod>
    <sy:updateFrequency>1</sy:updateFrequency>
    <item>
      <title>Better Compression with Zstandard</title>
      <link>http://gregoryszorc.com/blog/2017/03/07/better-compression-with-zstandard</link>
      <pubDate>Tue, 07 Mar 2017 09:55:00 PST</pubDate>
      <category><![CDATA[Python]]></category>
      <category><![CDATA[Mercurial]]></category>
      <category><![CDATA[Mozilla]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2017/03/07/better-compression-with-zstandard</guid>
      <description>Better Compression with Zstandard</description>
      <content:encoded><![CDATA[<p>I think I first heard about the <a href="http://facebook.github.io/zstd/">Zstandard</a>
compression algorithm at a Mercurial developer sprint in 2015.
At one end of a large table a few people were uttering expletives out
of sheer excitement. At developer gatherings, that's the universal signal
for <em>something is awesome</em>. Long story short, a Facebook engineer shared
a link to the
<a href="http://fastcompression.blogspot.com/">RealTime Data Compression blog</a>
operated by Yann Collet (then known as the author of LZ4 - a compression
algorithm known for its insane speeds) and people were completely
nerding out over the excellent articles and the data within showing the
beginnings of a new general purpose lossless compression algorithm named
Zstandard. It promised better-than-deflate/zlib compression ratios <strong>and</strong>
performance on both compression and decompression. This being a Mercurial
meeting, many of us were intrigued because zlib is used by Mercurial
for various functionality (including on-disk storage and compression over
the wire protocol) and zlib operations frequently appear as performance hot
spots.</p>
<p>Before I continue, if you are interested in low-level performance and
software optimization, I highly recommend perusing the
<a href="http://fastcompression.blogspot.com/">RealTime Data Compression blog</a>.
There are some absolute nuggets of info in there.</p>
<p>Anyway, over the months, the news about Zstandard (zstd) kept getting
better and more promising. As the 1.0 release neared, the Facebook
engineers I interact with (Yann Collet - Zstandard's author - is now
employed by Facebook) were absolutely ecstatic about Zstandard and its
potential. I was toying around with pre-release versions and was
absolutely blown away by the performance and features. I believed
the hype.</p>
<p>Zstandard 1.0 was
<a href="https://code.facebook.com/posts/1658392934479273/smaller-and-faster-data-compression-with-zstandard">released on August 31, 2016</a>.
A few days later, I started the
<a href="https://github.com/indygreg/python-zstandard">python-zstandard</a> project to
provide a fully-featured and Pythonic interface to the underlying zstd C
API while not sacrificing safety or performance. The ulterior motive was
to leverage those bindings in Mercurial so Zstandard could be a first class
citizen in Mercurial, possibly replacing zlib as the default compression
algorithm for all operations.</p>
<p>Fast forward six months and I've achieved many of those goals.
python-zstandard has a nearly complete interface to the zstd C API.
It even exposes some primitives not in the C API, such as batch
compression operations that leverage multiple threads and use minimal
memory allocations to facilitate insanely fast execution. (Expect a
dedicated post on python-zstandard from me soon.)</p>
<p>Mercurial 4.1 ships with the python-zstandard bindings. Two Mercurial
4.1 peers talking to each other will exchange Zstandard compressed
data instead of zlib. For a Firefox repository clone, transfer size is
reduced from ~1184 MB (zlib level 6) to ~1052 MB (zstd level 3) in the
default Mercurial configuration while using ~60% of the CPU that zlib
required on the compressor end. When cloning from hg.mozilla.org, the
pre-generated zstd <em>clone bundle</em> hosted on a CDN using maximum
compression is ~707 MB - ~60% the size of zlib! And, work is ongoing
for Mercurial to support Zstandard for on-disk storage, which should
bring considerable performance wins over zlib for local operations.</p>
<p>I've learned a lot working on python-zstandard and integrating Zstandard
into Mercurial. My primary takeaway is <strong>Zstandard is awesome</strong>.</p>
<p>In this post, I'm going to extol the virtues of Zstandard and provide
reasons why I think you should use it.</p>
<h2>Why Zstandard</h2>
<p>The main objective of lossless compression is to spend one resource
(CPU) so that you may reduce another (I/O). This trade-off is usually
made because data - either at rest in storage or in motion over a
network or even through a machine via software and memory - is a
limiting factor for performance. So if compression is needed for your
use case to mitigate I/O being the limiting resource and you can swap
in a different compression algorithm that magically reduces both CPU
and I/O requirements, that's pretty exciting. At scale, better
and more efficient compression can translate to substantial cost
savings in infrastructure. It can also lead to improved application
performance, translating to better end-user engagement, sales,
productivity, etc. This is why companies like Facebook (Zstandard),
Google (brotli, snappy, zopfli), and
<a href="https://www.crunchbase.com/organization/pied-piper">Pied Piper</a>
(middle-out) invest in compression.</p>
<p>Today, the most widely used compression algorithm in the world is
likely <a href="https://en.wikipedia.org/wiki/DEFLATE">DEFLATE</a>. And, software
most often interacts with DEFLATE via what is likely the most widely
used software library in the world, <a href="http://www.zlib.net/">zlib</a>.</p>
<p>Being at least 27 years old, DEFLATE is getting a bit long in the
tooth. Computers are completely different today than they were in 1990.
The Pentium microprocessor debuted in 1993. If memory serves (pun
intended), it used PC66 DRAM, which had a transfer rate of 533 MB/s.
For comparison, a modern NVMe M.2 SSD (like the Samsung 960 PRO)
can read at 3000+ MB/s and write at 2000+ MB/s. In other words,
persistent storage today is faster than the RAM from the era when
DEFLATE was invented. And of course CPU and network speeds have
increased as well. We also have completely different instruction
sets on CPUs for well-designed algorithms and software to take
advantage of. What I'm trying to say is the market is ripe for
DEFLATE and zlib to be dethroned by algorithms and software that
take into account the realities of modern computers.</p>
<p>(For the remainder of this post I'll use <em>zlib</em> as a stand-in for
<em>DEFLATE</em> because it is simpler.)</p>
<p>Zstandard initially piqued my attention by promising better-than-zlib
compression and performance in both the compression and decompression
directions. That's impressive. But it isn't unique. Brotli achieves
the same, for example. But what kept my attention was Zstandard's rich
feature set, tuning abilities, and therefore versatility.</p>
<p>In the sections below, I'll describe some of the benefits of Zstandard
in more detail.</p>
<p>Before I do, I need to throw in an obligatory disclaimer about data
and numbers that I use. Benchmarking is hard. Benchmarks should not
be trusted. There are so many variables that can influence performance
and benchmarks. (A recent example that surprised me is the
<a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1323106">CPU frequency/power ramping properties of Xeon versus non-Xeon Intel CPUs</a>.
tl;dr a Xeon won't hit max CPU frequency if only a core or two
is busy, meaning that any single or low-threaded benchmark is
likely misleading on Xeons unless you change power settings to
mitigate its conservative power ramping defaults. And if you change
power settings, does that reflect real-life usage?)</p>
<p>Reporting useful and accurate performance numbers for compression is
hard because there are so many variables to care about. For example:</p>
<ul>
<li>Every corpus is different. Text, JSON, C++, photos, numerical data,
  etc all exhibit different properties when fed into compression and
  could cause compression ratios or speeds to vary significantly.</li>
<li>Few large inputs versus many smaller inputs (some algorithms work
  better on large inputs; some libraries have high per-operation
  overhead).</li>
<li>Memory allocation and use strategy. Performance can vary
  significantly depending on how a compression library allocates,
  manages, and uses memory. This can be an implementation specific
  detail as opposed to a core property of the compression algorithm.</li>
</ul>
<p>Since Mercurial is the driver for my work in Zstandard, the data and
numbers I report in this post are mostly Mercurial data. Specifically,
I'll be referring to data in the
<a href="https://hg.mozilla.org/mozilla-unified">mozilla-unified Firefox repository</a>.
This repository contains over 300,000 commits spanning almost 10 years.
The data within is a good mix of text (mostly C++, JavaScript, Python,
HTML, and CSS source code and other free-form text) and binary (like
PNGs). The Mercurial layer adds some binary structures to e.g. represent
metadata for deltas, diffs, and patching. There are two Mercurial-specific
pieces of data I will use. One is a Mercurial <em>bundle</em>. This is essentially
a representation of all data in a repository. It stores a mix of raw,
fulltext data and deltas on that data. For the mozilla-unified repo, an
uncompressed bundle (produced via <code>hg bundle -t none-v2 -a</code>) is ~4457 MB.
The other piece of data is <em>revlog chunks</em>. This is a mix of fulltext
and delta data for a specific item tracked in version control. I
frequently use the <em>changelog</em> corpus, which is the fulltext data
describing changesets or commits to Firefox. The numbers quoted and
used for charts in this post
<a href="https://docs.google.com/spreadsheets/d/1PHhGxsQR3zDE-2Eeu5Hn2t-UKw4oGV-7LQTMGEdBh9U/edit?usp=sharing">are available in a Google Sheet</a>.</p>
<p>All performance data was obtained on an i7-6700K running Ubuntu 16.10
(Linux 4.8.0) with a mostly stock config. Benchmarks were performed in
memory to mitigate storage I/O or filesystem interference. Memory used
is DDR4-2133 with a cycle time of 35 clocks.</p>
<p>While I'm pretty positive about Zstandard, it isn't perfect. There are
corpora for which Zstandard performs worse than other algorithms, even
ones I compare it directly to in this post. So, your mileage may vary.
Please enlighten me with your counterexamples by leaving a comment.</p>
<p>With that (rather large) disclaimer out of the way, let's talk about
what makes Zstandard awesome.</p>
<h2>Flexibility for Speed Versus Size Trade-offs</h2>
<p>Compression algorithms typically contain parameters to control how
much work to do. You can choose to spend more CPU to (hopefully)
achieve better compression or you can spend less CPU to sacrifice
compression. (OK, fine, there are other factors like memory usage at
play too. I'm simplifying.) This is commonly exposed to
end-users as a compression <em>level</em>. (In reality there are often
multiple parameters that can be tuned. But I'll just use <em>level</em>
as a stand-in to represent the concept.)</p>
<p>But even with adjustable compression levels, the performance of many
compression algorithms and libraries tend to fall within a relatively
narrow window. In other words, many compression algorithms focus on
niche markets. For example, LZ4 is super fast but doesn't yield great
compression ratios. LZMA yields terrific compression ratios but is
extremely slow.</p>
<p>This can be visualized in the following chart showing results when
compressing a mozilla-unified Mercurial bundle:</p>
<p><img alt="bundle compression with common algorithms" src="/images/compression-bundle-common.png" /></p>
<p>This chart plots the logarithmic compression speed in megabytes per
second against achieved compression ratio. The further right a data
point is, the better the compression and the smaller the output.
The higher up a point is, the faster compression is.</p>
<p>The ideal compression algorithm lives in the top right, which means
it compresses well and is fast. But the powers of mathematics push
compression algorithms away from the top right.</p>
<p>On to the observations.</p>
<p>LZ4 is highly vertical, which means its compression ratios are
limited in variance but it is extremely flexible in speed. So for
this data, you might as well stick to a lower compression level
because higher values don't buy you much.</p>
<p>Bzip2 is the opposite: a horizontal line. That means it is consistently
the same speed while yielding different compression ratios. In other
words, you might as well crank bzip2 up to maximum compression because
it doesn't have a significant adverse impact on speed.</p>
<p>LZMA and zlib are more interesting because they exhibit more variance
in both the compression ratio and speed dimensions. But let's be frank,
they are still pretty narrow. LZMA looks pretty good from a shape
perspective, but its top speed is just too slow - only ~26 MB/s!</p>
<p>This small window of flexibility means that you often have to choose
a compression algorithm based on the speed versus size trade-off you are
willing to make at that time. That choice often gets baked into
software. And as time passes and your software or data gains popularity,
changing the software to swap in or support a new compression algorithm
becomes harder because of the cost and disruption it will cause. That's
technical debt.</p>
<p>What we really want is a single compression algorithm that occupies
lots of <em>space</em> in both dimensions of our chart - a curve that has
high variance in both compression speed and ratio. Such an algorithm
would allow you to make an easy decision choosing a compression
algorithm without locking you into a narrow behavior profile. It would
allow you make a completely different size versus speed trade-off in
the future by only adjusting a config knob or two in your application -
no swapping of compression algorithms needed!</p>
<p>As you can guess, Zstandard fulfills this role. This can clearly be seen
in the following chart (which also adds brotli for comparison).</p>
<p><img alt="bundle compression with modern algorithms" src="/images/compression-bundle-modern.png" /></p>
<p>The advantages of Zstandard (and brotli) are obvious. Zstandard's
compression speeds go from ~338 MB/s at level 1 to ~2.6 MB/s at
level 22 while covering compression ratios from 3.72 to 6.05. On one
end, <strong>zstd level 1 is ~3.4x faster than zlib level 1 while achieving
better compression than zlib level 9!</strong> That fastest speed is only 2x
slower than LZ4 level 1. On the other end of the spectrum, zstd
level 22 runs ~1 MB/s slower than LZMA at level 9 and produces a
file that is only 2.3% larger.</p>
<p>It's worth noting that zstd's C API exposes several knobs for tweaking
the compression algorithm. Each compression level maps to a pre-defined
set of values for these knobs. It is possible to set these values beyond
the ranges exposed by the default compression levels 1 through 22. I've
done some basic experimentation with this and have made compression even
faster (while sacrificing ratio, of course). This covers the gap between
Zstandard and brotli on this end of the tuning curve.</p>
<p>The wide span of compression speeds and ratios is a game changer
for compression. Unless you have special requirements such as
lightning fast operations (which LZ4 can provide) or special
corpora that Zstandard can't handle well, Zstandard is a very safe and
flexible choice for general purpose compression.</p>
<h2>Multi-threaded Compression</h2>
<p>Zstd 1.1.3 contains a multi-threaded compression API that allows a
compression operation to leverage multiple threads. The output from
this API is compatible with the Zstandard frame format and doesn't require
any special handling on the decompression side. <strong>In other words, a
compressor can switch to the multi-threaded API and decompressors won't
care.</strong></p>
<p>This is a big deal for a few reasons. First, today's advancements in
computer processors tend to yield more capacity from more cores not
from faster clocks and better cycle efficiency (although many cases
do benefit greatly from modern instruction sets like AVX and therefore
better cycle efficiency). Second, so many compression libraries are
only single-threaded and require consumers to invent their own framing
formats or storage models to facilitate multi-threading. (See
<a href="http://blosc.org/">Blosc</a> for such a library.) Lack of a
multi-threaded API in the compression library means trusting another
piece of software or writing your own multi-threaded code.</p>
<p>The following chart adds a plot of Zstandard multi-threaded compression
with 4 threads.</p>
<p><img alt="multi-threaded compression" src="/images/compression-bundle-multithreaded.png" /></p>
<p>The existing curve for Zstandard basically shifted straight up. Nice!</p>
<p>The ~338 MB/s speed for single-threaded compression on zstd level 1
increases to ~1,376 MB/s with 4 threads. That's ~4.06x faster. And,
it is ~2.26x faster than the previous fastest entry, LZ4 at level 1!
The output size only increased by ~4 MB or ~0.3% over single-threaded
compression.</p>
<p>The scaling properties for multi-threaded compression on this input
are terrific: all 4 cores are saturated and the output size barely
changed.</p>
<p>Because Zstandard's multi-threaded compression API produces data compatible
with any Zstandard decompressor, it can logically be considered an extension
of <em>compression levels</em>. This means that the already extremely flexible
speed vs ratio curve becomes even wider in the speed axis. Zstandard
was already a justifiable choice with its extreme versatility. But when
you throw in native multi-threaded compression API support, the
flexibility for tuning compression performance is just absurd. With
enough cores, you are likely to run into I/O limits long before you
exhaust the CPU, at which point you can crank up the compression
level and sacrifice as much CPU as you are willing to burn. That's
a good position to be in.</p>
<h2>Decompression Speed</h2>
<p>Compression speed and ratios only tell half the story about a compression
algorithm. Except for archiving scenarios where you write once and
read rarely, you probably care about decompression performance.</p>
<p>Popular compression algorithms like zlib and bzip2 have less than stellar
decompression speeds. On my i7-6700K, zlib decompression can deliver many
decompressed data sets at the output end at 200+ MB/s. However, on the
input/compressed end, it frequently fails to reach 100 MB/s or even
80 MB/s. This is significant because if your application is reading data
over a 1 Gbps network or from a local disk (modern SSDs can read at several
hundred MB/s or more), then your application has a CPU bottleneck at
decoding the data - and that's before you actually do anything useful
with the data in the application layer! (Remember: the idea behind
compression is to spend CPU to mitigate an I/O bottleneck. So if
compression makes you CPU bound, you've undermined the point of
compression!) And if my Skylake CPU running at 4.0 GHz is CPU -
not I/O - bound, A Xeon in a data center will be even slower and
even more CPU bound (Xeons tend to run at much lower clock speeds -
the laws of thermodynamics require that in order to run more cores in
the package). In short, <strong>if you are using zlib for high throughput
scenarios, there's a good chance it is a bottleneck and slowing down
your application</strong>.</p>
<p>We again measure the speed of algorithms using a Firefox Mercurial
bundle. The following charts plot decompression speed versus ratio
for this file. The first chart measures decompression speed on the
input end of the decompressor. The second measures speed at the
output end.</p>
<p><img alt="decompression input" src="/images/decompression-bundle-input.png" /></p>
<p><img alt="decompression output" src="/images/decompression-bundle-output.png" /></p>
<p>Zstandard matches its great compression speed with great decompression
speed. Zstandard can deliver decompressed output at 1000+ MB/s while
consuming input at 200-275MB/s. Furthermore, decompression speed is
mostly independent of the compression level. (Although higher
compression levels require more memory in the decompressor.) So, <strong>if
you want to throw more CPU at re-compression later so data at rest takes
less space, you can do that without sacrificing read performance.</strong>
I haven't done the math, but there is probably a break-even point
where having dedicated machines re-compress terabytes or petabytes
of data at rest offsets the costs of those machine through reduced
storage costs.</p>
<p>While Zstandard is not as fast decompressing as LZ4 (which can consume
compressed input at 500+ MB/s), its performance is often ~4x faster
than zlib. On many CPUs, this puts it well above 1 Gbps, which is
often desirable to avoid a bottleneck at the network layer.</p>
<p>It's also worth noting that while Zstandard and brotli were comparable
on the compression half of this data, Zstandard has a clear advantage
doing decompression.</p>
<p>Finally, you don't appear to pay a price for multi-threaded Zstandard
compression on the decompression side (<code>zstdmt</code> in the chart).</p>
<h2>Dictionary Support</h2>
<p>The examples so far in this post have used a single 4,457 MB piece of
input data to measure behavior. Large data can behave completely
differently from small data. This is because so much of what
compression algorithms do is find patterns that came before so incoming
data can be <em>referenced</em> to old data instead of uniquely stored. And if
data is small, there isn't much of it that came before to reference!</p>
<p>This is often why many small, independent chunks of input compress
poorly compared to a single large chunk. This can be demonstrated by
comparing the widely-used <em>zip</em> and <em>tar</em> archive formats. On the
surface, both do the same thing: they are a container of files. But
they employ compression at different phases. A <em>zip</em> file will zlib
compress each entry independently. However, a <em>tar</em> file doesn't use
compression internally. Instead, the tar file itself is fed into a
compression algorithm and compressed as a whole.</p>
<p>We can observe the difference on real world data. Firefox
ships with a file named <code>omni.ja</code>. Despite the weird extension, this
is a <em>zip</em> file. The file contains most of the assets for non-compiled
code used by Firefox. This includes the JavaScript, HTML, CSS, and
images that power many parts of the Firefox frontend. The file weighs
in at 9,783,749 bytes for the 64-bit Windows Firefox Nightly from
2017-03-06. (Or 9,965,793 bytes when using <code>zip -9</code> - the code for
generating <code>omni.ja</code> is smarter than <code>zip</code> and creates smaller
files.) But a zlib level 9 compressed <code>tar.gz</code> file of that directory
is 8,627,155 bytes. That 1,156KB / 13% size difference is significant
when you are talking about delivering bits to end users! (In this
case, the content within the archive needs to be individually
addressable to facilitate fast access to any item without having
to decompress the entire archive: this matters for performance.)</p>
<p>A more extreme example of the differences between <em>zip</em> and <em>tar</em>
is the files in the Firefox source checkout. On revision
a08ec245fa24 of the Firefox Mercurial repository, a <em>zip</em> file of
all files in version control is 430,446,549 bytes versus
322,916,403 bytes for a <em>tar.gz</em> file (1,177,430,383 bytes uncompressed
spanning 180,912 files). Using Zstandard, compressing each file
discretely at compression level 3 yields 391,387,299 bytes of
compressed data versus 294,926,418 as a single stream (without the
<em>tar</em> container). Same compression algorithm. Different application
method. Drastically different results. That's the impact of input
size on compression performance.</p>
<p>While the compression ratio and speed of a single large stream is
often better than multiple smaller chunks, there are still
use cases that either don't have enough data or prefer independent
access to each piece of input (like Firefox's <code>omni.ja</code> file). So
a robust compression algorithm should handle small inputs as well
as it does large inputs.</p>
<p>Zstandard helps offset the inherent inefficiencies of small inputs
by supporting <em>dictionary compression</em>. A <em>dictionary</em> is
essentially data used to seed the compressor's state. If the
compressor sees data that exists in the dictionary, it references
the dictionary instead of storing new data in the compressed output
stream. This results in smaller output sizes and better compression
ratios. One drawback to this is the dictionary has to be used to
decompress data, which means you need to figure out how to
distribute the dictionary and ensure it remains in sync with all
data producers and consumers. This isn't always trivial.</p>
<p>Dictionary compression only works if there is enough repeated data
and patterns in the inputs that can be extracted to yield a
useful dictionary. Examples of this include markup languages, source
code, or pieces of similar data (such as JSON payloads from HTTP API
requests or telemetry data), which often have many repeated keywords
and patterns.</p>
<p>Dictionaries are typically produced by <em>training</em> them on existing
data. Essentially, you feed a bunch of samples into an algorithm
that spits out a meaningful and useful dictionary. The more coherency
in the data that will be compressed, the better the dictionary and
the better the compression ratios.</p>
<p>Dictionaries can have a significant effect on compression ratios and
speed.</p>
<p>Let's go back to Firefox's <code>omni.ja</code> file. Compressing each file
discretely at zstd level 12 yields 9,177,410 bytes of data. But if
we produce a 131,072 byte dictionary by training it on all files
within <code>omni.ja</code>, the total size of each file compressed discretely
is 7,942,886 bytes. Including the dictionary, the total size is
8,073,958 bytes, 1,103,452 bytes smaller than non-dictionary
compression! (The zlib-based <code>omni.ja</code> is 9,783,749 bytes.) So
Zstandard plus dictionary compression would likely yield a
meaningful ~1.5 MB size reduction to the <code>omni.ja</code> file. This would
make the Firefox distribution smaller and <em>may</em> improve startup
time (since many files inside <code>omni.ja</code> are accessed at
startup), which would make a number of people very happy. (Of
course, Firefox doesn't yet contain the zstd C library. And adding
it just for this use case may not make sense. But Firefox does ship
with the brotli library and brotli supports dictionary compression
and has similar performance characteristics as Zstandard, so, uh,
someone may want to look into transitioning <code>omni.jar</code> to
<em>not zlib</em>.)</p>
<p>But the benefits of dictionary compression don't end at compression
ratios: operations with dictionaries can be faster as well!</p>
<p>The following chart shows performance when compressing Mercurial
<em>changeset</em> data (describes a Mercurial commit) for the Firefox
repository. There are 382,530 discrete inputs spanning 221,429,458
bytes (mean: 579 bytes, median: 306 bytes). (Note: measurements were
conducted in Python and therefore may introduce some overhead.)</p>
<p><img alt="dictionary compression performance" src="/images/decompression-dictionary-changeset.png" /></p>
<p>Aside from zstd level 3 dictionary compression, Zstandard is faster
than zlib level 6 across the board (I suspect this one-off is an
oddity with the zstd compression parameters at this level and this
corpus because zstd level 4 is faster than level 3, which is weird).</p>
<p>It's also worth noting that non-dictionary zstandard compression
has similar compression ratios to zlib. Again, this demonstrates
the intrinsic difficulties of compressing small inputs.</p>
<p>But the real takeaway from this data are the speed differences with
dictionary compression enabled. Dictionary decompression is
2.2-2.4x <em>faster</em> than non-dictionary decompression. Already
respectable ~240 MB/s decompression speed (measured at the output
end) becomes ~530 MB/s. Zlib level 6 was ~140 MB/s, so swapping
in dictionary compression makes things ~3.8x faster. It takes ~1.5s
of CPU time to zlib decompress this corpus. So if Mercurial can
be taught to use Zstandard dictionary compression for changelog data,
certain operations on this corpus will complete ~1.1s faster. That's
significant.</p>
<p>It's worth stating that Zstandard isn't the only compression algorithm
or library to support dictionary compression. Brotli and zlib do as
well, for example. But, Zstandard's support for dictionary compression
seems to be more polished than other libraries I've seen. It has multiple
APIs for training dictionaries from sample data. (Brotli has none nor
does brotli's documentation say how to generate dictionaries as far as
I can tell.)</p>
<p>Dictionary compression is definitely an advanced feature, applicable
only to certain use cases (lots of small, similar data). But there's
no denying that if you can take advantage of dictionary compression,
you may be rewarded with significant performance wins.</p>
<h2>A Versatile C API</h2>
<p>I spend a lot of my time these days in higher-level programming
languages like Python and JavaScript. By the time you interact with
compression in high-level languages, the low-level compression APIs
provided by the compression library are most likely hidden from you
and bundled in a nice, friendly abstraction, suitable for a
higher-level language. And more often than not, many features of
that low-level API are not exposed for you to call. So, you don't
get an appreciation for how good (or bad) or feature rich (or
lacking) the low-level API is.</p>
<p>As part of writing
<a href="https://github.com/indygreg/python-zstandard">python-zstandard</a>, I've
spent a lot of time interfacing with the zstd C API. And, as part
of evaluating other compression libraries for use in Mercurial, I've
been looking at C APIs for other libraries and the Python bindings to
them. A takeaway from this is an appreciation for the quality of
zstd's C API.</p>
<p>Many compression library APIs are either too simple or too complex.
Zstandard's is in the Goldilocks zone. Aside from a few minor missing
features, its C API was more than adequate in its 1.0 release.</p>
<p>What I really appreciate about the zstd C API is that it provides
high, medium, and low-level APIs. From the highest level, you throw
it pointers to input and output buffers and it does an operation.
From the medium level, you use a reusable <em>context</em> holding state
and other parameters and it does an operation. From the low-level,
you are calling multiple functions and shuffling bytes around,
maintaining your own state and potentially bypassing the Zstandard
<em>framing</em> format in the process. The different levels give you
almost total control over everything. This is critical for performance
optimization and when writing bindings for higher-level languages that
may have different expectations on the behavior of software. The
performance I've achieved in python-zstandard just isn't (easily)
possible with other compression libraries because of their lacking
API design.</p>
<p>Oftentimes when interacting with a C library I think <em>if only there
were a function to let me do X my life would be much easier</em>. I
rarely have this experience with Zstandard. The C API is well thought out,
has almost all the features I want/need, and is pretty easy to use.
While most won't notice this difference, it should be a significant
advantage for Zstandard in the long run, as more bindings are
written and more people have a high-quality experience with it
because the C API allows them to.</p>
<h2>Zstandard Isn't Perfect</h2>
<p>I've been pretty positive about Zstandard so far in this post.
In fear of sounding like a fanboy who is so blinded by admiration
that he can't see faults and because nothing is perfect, I need to
point out some negatives about Zstandard. (Aside: put little faith
in the words uttered by someone who can't find a fault in something
they praise.)</p>
<p>First, the <a href="https://github.com/facebook/zstd/blob/3bee41a70eaf343fbcae3637b3f6edbe52f35ed8/doc/zstd_compression_format.md#zstandard-frames">framing format</a>
is a bit heavyweight in some scenarios. The frame header is at <em>least</em>
6 bytes. For input of 256-65791 bytes, recording the original source
size and its checksum will result in a 12 byte frame. Zlib, by contrast,
is only 6 bytes for this scenario. When storing tens of thousands of
compressed records (this is a use case in Mercurial), the frame overhead
can matter and this can make it difficult for compressed Zstandard
data to be as small as zlib for very small inputs. (It's worth noting
that zlib doesn't store the decompressed size in its header. There are
pros and cons to this, which I'll discuss in my eventual post about
python-zstandard and how it achieves optimal performance.) If the frame
overhead matters to you, the zstd C API does expose a <em>block</em> API that
operates at a level below the framing format, allowing you to roll your
own framing protocol. I also
<a href="https://github.com/facebook/zstd/issues/591">filed a GitHub issue</a> to
make the 4 byte magic number optional, which would go a long way to
cutting down on frame overhead.</p>
<p>Second, the C API is not yet fully stabilized. There are a number of
functions marked as <em>experimental</em> that aren't exported from the shared
library and are only available via static linking. There's a ton of
useful functionality in there, including low-level compression parameter
adjustment, digested dictionaries (for reusing computed dictionaries
across multiple <em>contexts</em>), and the multi-threaded compression API.
python-zstandard makes heavy use of these <em>experimental</em> APIs. This
requires bundling zstd with python-zstandard and statically linking
with this known version because functionality could change at any time.
This is a bit annoying, especially for distro packagers.</p>
<p>Third, the low-level compression parameters are under-documented. I
think I understand what a lot of them do. But it isn't obvious when
I should consider adjusting what. The default compression levels
seem to work pretty well and map to reasonable compression parameters.
But a few times I've noticed that tweaking things slightly can result
in desirable improvements. I wish there were a guide of sorts to
help you tune these parameters.</p>
<p>Fourth, dictionary compression is still a bit too complicated and
hand-wavy for my liking. I can measure obvious benefits when using it
largely out of the box with some corpora. But it isn't always a win
and the cost for training dictionaries is too high to justify using
it outside of scenarios where you are pretty sure it will be beneficial.
When I do use it, I'm not sure which compression levels it works best
with, how many samples need to be fed into the dictionary trainer,
which training algorithm to use, etc. If that isn't enough, there is
also the concept of <em>content-only dictionaries</em> where you use a
fulltext as the dictionary. This can be useful for delta-encoding
schemes (where compression effectively acts like a diff/delta
generator instead of using something like Myers diff). If this topic
interests you, there is a
<a href="https://www.mercurial-scm.org/pipermail/mercurial-devel/2017-January/092186.html">thread on the Mercurial developers list</a>
where Yann Collet and I discuss this.</p>
<p>Fifth and finally, Zstandard is still relatively new. I can totally
relate to holding off until something new and shiny proves itself.
That being said, the Zstandard framing protocol has some escape
hatches for future needs. And, the project proved during its pre-1.0
days that it knows how to handle backwards and future compatibility
issues. And considering Facebook and others are using Zstandard in
production, I wouldn't be too worried. I think the biggest risk is
to people (like me) who are writing code against the <em>experimental</em>
C APIs. But even then, the changes to the experimental APIs in the
past several months have been minor. I'm not losing sleep over it.</p>
<p>That may seem like a long and concerning list. But I think the issues are
relatively minor. From my perspective, the biggest thing Zstandard has
going against it is its youth. But that will only improve with age.
While I'm usually pretty conservative about adopting new technology
(I've gotten burned enough times that I prefer the neophytes do the
field testing for me), the upside to using Zstandard is potentially
drastic performance and efficiency gains. And that can translate to
success versus failure or millions of dollars in saved infrastructure
costs and productivity gains. I'm willing to take my chances.</p>
<h2>Conclusion</h2>
<p>For the corpora I've thrown at it, Zstandard handily outperforms zlib
in almost every dimension. And, it even manages to best other <em>modern</em>
compression algorithms like brotli in many tests.</p>
<p>The underlying algorithm and techniques used by Zstandard are highly
parameterized, lending themselves to a variety of use cases from embedded
hardware to massive data crunching machines with hundreds of gigabytes
of memory and dozens of CPU cores.</p>
<p>The C API is well-designed and facilitates high performance and
adaptability to numerous use cases. It is <em>batteries included</em>,
providing functions to train dictionaries and perform multi-threaded
compression.</p>
<p>Zstandard is backed by Facebook and seems to have a healthy open source
culture <a href="https://github.com/facebook/zstd">on Github</a>. My interactions
with Yann Collet have been positive and he seems to be a great
project maintainer.</p>
<p>Zstandard is an exciting advancement for data compression and therefore
for the entire computing field. As someone who has lived in the world
of zlib for years, was a casual user of compression, and thought zlib
was <em>good enough</em> for most use cases, I can attest that Zstandard is
game changing. After being enlightened to all the advantages of
Zstandard, I'll never casually use zlib again: it's just too slow and
inflexible for the needs of modern computing. If you use compression,
I highly recommend investigating Zstandard.</p>]]></content:encoded>
    </item>
    <item>
      <title>Automatic Python Static Analysis on MozReview</title>
      <link>http://gregoryszorc.com/blog/2015/01/24/automatic-python-static-analysis-on-mozreview</link>
      <pubDate>Sat, 24 Jan 2015 23:30:00 PST</pubDate>
      <category><![CDATA[Python]]></category>
      <category><![CDATA[MozReview]]></category>
      <category><![CDATA[Mozilla]]></category>
      <category><![CDATA[code review]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2015/01/24/automatic-python-static-analysis-on-mozreview</guid>
      <description>Automatic Python Static Analysis on MozReview</description>
      <content:encoded><![CDATA[<p>A bunch of us were in Toronto last week hacking on MozReview.</p>
<p>One of the cool things we did was deploy a bot for performing Python
static analysis. If you submit some .py files to MozReview, the bot
should leave a review. If it finds violations (it uses
<a href="https://flake8.readthedocs.org/">flake8</a> internally), it will open
an issue for each violation. It also leaves a comment that should
hopefully give enough detail on how to fix the problem.</p>
<p>While we haven't done much in the way of performance optimizations,
the bot typically submits results less than 10 seconds after the review
is posted! So, a human should never be reviewing Python that the bot
hasn't seen. This means you can stop thinking about style nits and start
thinking about what the code does.</p>
<p>This bot should be considered an alpha feature. The code for the bot
isn't even checked in yet. We're running the bot against production
to get a feel for how it behaves. If things don't go well, we'll turn
it off until the problems are fixed.</p>
<p>We'd like to eventually deploy C++, JavaScript, etc bots. Python won out
because it was the easiest to integrate (it has sane and efficient
tooling that is compatible with Mozilla's code bases - most existing
JavaScript tools won't work with Gecko-flavored JavaScript, sadly).</p>
<p>I'd also like to eventually make it easier to locally run the same
static analysis we run in MozReview. Addressing problems locally before
pushing is a no-brainer since it avoids needless context switching from
other people and is thus better for productivity. This will come in
time.</p>
<p>Report issues in #mozreview or in the Developer Services :: MozReview
Bugzilla component.</p>]]></content:encoded>
    </item>
    <item>
      <title>Python Packaging Do's and Don'ts</title>
      <link>http://gregoryszorc.com/blog/2014/07/15/python-packaging-do's-and-don'ts</link>
      <pubDate>Tue, 15 Jul 2014 17:20:00 PDT</pubDate>
      <category><![CDATA[Python]]></category>
      <category><![CDATA[Mozilla]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2014/07/15/python-packaging-do's-and-don'ts</guid>
      <description>Python Packaging Do's and Don'ts</description>
      <content:encoded><![CDATA[<p>Are you someone who casually interacts with Python but don't know the
inner workings of Python? Then this post is for you. Read on to learn
why some things are the way they are and how to avoid making some
common mistakes.</p>
<h2>Always use Virtualenvs</h2>
<p>It is an easy trap to view <a href="https://virtualenv.pypa.io/en/latest/virtualenv.html">virtualenvs</a>
as an obstacle, a distraction towards accomplishing something. People
see me adding virtualenvs to build instructions and they say <em>I don't
use virtualenvs, they aren't necessary, why are you doing that?</em></p>
<p>A virtualenv is effectively an overlay on top of your system Python
install. Creating a virtualenv can be thought of as copying your system
Python environment into a local location. When you modify virtualenvs,
you are modifying an isolated container. Modifying virtualenvs has no
impact on your system Python.</p>
<p>A goal of a virtualenv is to isolate your system/global Python install
from unwanted changes. When you accidentally make a change to a
virtualenv, you can just delete the virtualenv and start over from
scratch. When you accidentally make a change to your system Python, it
can be much, much harder to recover from that.</p>
<p>Another goal of virtualenvs is to allow different versions of packages
to exist. Say you are working on two different projects and each
requires a specific version of Django. With virtualenvs, you install one
version in one virtualenv and a different version in another virtualenv.
Things happily coexist because the virtualenvs are independent.
Contrast with trying to manage both versions of Django in your system
Python installation. Trust me, it's not fun.</p>
<p>Casual Python users may not encounter scenarios where virtualenvs make
their lives better... until they do, at which point they realize their
system Python install is beyond saving. People who eat, breath, and die
Python run into these scenarios all the time. We've learned how bad life
without virtualenvs can be and so we use them everywhere.</p>
<p>Use of virtualenvs is a best practice. Not using virtualenvs will result
in something unexpected happening. It's only a matter of time.</p>
<p>Please use virtualenvs.</p>
<h2>Never use sudo</h2>
<p>Do you use sudo to install a Python package? You are doing it wrong.</p>
<p>If you need to use sudo to install a Python package, that almost
certainly means you are installing a Python package to your
system/global Python install. And this means you are modifying your
system Python instead of isolating it and keeping it pristine.</p>
<p>Instead of using sudo to install packages, create a virtualenv and
install things into the virtualenv. There should never be permissions
issues with virtualenvs - the user that creates a virtualenv has full
realm over it.</p>
<h2>Never modify the system Python environment</h2>
<p>On some systems, such as OS X with Homebrew, you don't need sudo to
install Python packages because the user has write access to the Python
directory (<em>/usr/local</em> in Homebrew).</p>
<p>For the reasons given above, don't muck around with the system Python
environment. Instead, use a virtualenv.</p>
<h2>Beware of the package manager</h2>
<p>Your system's package manager (apt, yum, etc) is likely using root and/or
installing Python packages into the system Python.</p>
<p>For the reasons given above, this is bad. Try to use a virtualenv, if
possible. Try to not use the system package manager for installing
Python packages.</p>
<h2>Use pip for installing packages</h2>
<p>Python packaging has historically been a mess. There are a handful of
tools and APIs for installing Python packages. As a casual Python user,
you only need to know of one of them:
<a href="https://pip.pypa.io/en/latest/">pip</a>.</p>
<p>If someone says <em>install a package</em>, you should be thinking <em>create a
virtualenv, activate a virtualenv, <code>pip install &lt;package&gt;</code></em>. <strong>You
should never run <code>pip install</code> outside of a virtualenv.</strong> (The exception
is to install virtualenv and pip itself, which you almost certainly want
in your system/global Python.)</p>
<p>Running <em>pip install <package></em> will install packages from
<a href="https://pypi.python.org/pypi">PyPI</a>, the Python Packaging Index by
default. It's Python's official package repository.</p>
<p>There are a lot of old and outdated tutorials online about Python
packaging. Beware of bad content. For example, if you see documentation
that says <em>use easy_install</em>, you should be thinking, <em>easy_install is
a legacy package installer that has largely been replaced by pip, I
should use pip instead</em>. When in doubt, consult the
<a href="https://python-packaging-user-guide.readthedocs.org/en/latest/index.html">Python packaging user guide</a>
and do what it recommends.</p>
<h2>Don't trust the Python in your package manager</h2>
<p>The more Python programming you do, the more you learn to not trust the
Python package provided by your system / package manager.</p>
<p>Linux distributions such as Ubuntu that sit on the forward edge of
versions are better than others. But I've run into enough problems with
the OS or package manager maintained Python (especially on OS X), that
I've learned to distrust them.</p>
<p>I use <a href="https://github.com/yyuu/pyenv">pyenv</a> for installing and managing
Python distributions from source. pyenv also installs virtualenv and
pip for me, packages that I believe should be in all Python installs
by default. As a more experienced Python programmer, I find pyenv 
<em>just works</em>.</p>
<p>If you are just a beginner with Python, it is probably safe to ignore
this section. Just know that as soon as something weird happens, start
suspecting your default Python install, especially if you are on OS X.
If you suspect trouble, use something like pyenv to enforce a buffer so
the system can have its Python and you can have yours.</p>
<h2>Recovering from the past</h2>
<p>Now that you know the preferred way to interact with Python, you are
probably thinking <em>oh crap, I've been wrong all these years - how do I
fix it?</em></p>
<p>The goal is to get a Python install <em>somewhere</em> that is as pristine as
possible. You have two approaches here: cleaning your existing Python or
creating a new Python install.</p>
<p>To clean your existing Python, you'll want to purge it of pretty much
all packages not installed by the core Python distribution. The
exception is virtualenv, pip, and setuptools - you almost certainly want
those installed globally. On Homebrew, you can uninstall everything related to
Python and blow away your Python directory, typically
/usr/local/lib/python*. Then, <em>brew install python</em>. On Linux distros,
this is a bit harder, especially since most Linux distros rely on Python
for OS features and thus they may have installed extra packages. You
could try a similar approach on Linux, but I don't think it's worth it.</p>
<p>Cleaning your system Python and attempting to keep it pure are ongoing
tasks that are very difficult to keep up with. All it takes is one
dependency to get pulled in that trashes your system Python. Therefore,
I shy away from this approach.</p>
<p>Instead, I install and run Python from my user directory. I use
<a href="https://github.com/yyuu/pyenv">pyenv</a>. I've also heard great things
about <a href="http://conda.pydata.org/miniconda.html">Miniconda</a>. With either
solution, you get a Python in your home directory that starts clean and
pure. Even better, it is completely independent from your system Python.
So if your package manager does something funky, there is a buffer. And,
if things go wrong with your userland Python install, you can always
nuke it without fear of breaking something in system land. This seems to
be the best of both worlds.</p>
<p>Please note that installing packages in the system Python shouldn't be
evil. When you create virtualenvs, you can - and should - tell
virtualenv to not use the system site-packages (i.e. <em>don't use
non-core packages from the system installation</em>). This is the default
behavior in virtualenv. It should provide an adequate buffer. But from
my experience, things still manage to bleed through. My userland Python
install is extra safety. If something wrong happens, I can only blame
myself.</p>
<h2>Conclusion</h2>
<p>Python's long and complicated history of package management makes it
very easy for you to shoot yourself in the foot. The long list of
outdated tutorials on The Internet make this a near certainty for casual
Python users. Using the guidelines in this post, you can adhere to best
practices that will cut down on surprises and rage and keep your Python
running smoothly.</p>]]></content:encoded>
    </item>
    <item>
      <title>Aggregating Version Control Info at Mozilla</title>
      <link>http://gregoryszorc.com/blog/2014/01/21/aggregating-version-control-info-at-mozilla</link>
      <pubDate>Tue, 21 Jan 2014 10:50:00 PST</pubDate>
      <category><![CDATA[Git]]></category>
      <category><![CDATA[Mercurial]]></category>
      <category><![CDATA[Mozilla]]></category>
      <category><![CDATA[Python]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2014/01/21/aggregating-version-control-info-at-mozilla</guid>
      <description>Aggregating Version Control Info at Mozilla</description>
      <content:encoded><![CDATA[<p>Over the winter break, I set out on an ambitious project to create
a service to help developers and others manage the flury
of patches going into Firefox. While the project is far from complete,
I'm ready to unleash the first part of the project upon the world.</p>
<p>If you point your browsers to
<a href="http://moztree.gregoryszorc.com/">moztree.gregoryszorc.com</a>, you'll
hopefully see some documentation about what I've built.
<a href="https://bitbucket.org/indygreg/moz-tree-utopia">Source code</a> is
available and free, of course. Patches very welcome.</p>
<p>Essentially, I built a centralized indexing service for version
control repositories with Mozilla's extra metadata thrown in.
I tell it what repositories to mirror, and it clones everything,
fetches data such as the pushlog and Git SHA-1 mappings, and
stores everything in a central database. It then exposes this
aggregated data through world-readable web services.</p>
<p>Currently, I have the service indexing the popular project branches
for Firefox (central, aurora, beta, release, esr, b2g, inbound, fx-team,
try, etc). You can view the
<a href="http://moztree.gregoryszorc.com/api/repos">full list</a> via the web
service. As a bonus, I'm also serving these repositories via
<a href="http://hg.gregoryszorc.com/">hg.gregoryszorc.com</a>. My server appears
to be significantly faster than
<a href="https://hg.mozilla.org">hg.mozilla.org</a>. If you want to use it for
your daily needs, go for it. I make no SLA guarantees, however.</p>
<p>I'm also using this service as an opportunity to experiment with
alternate forms of Mercurial hosting. I have mirrors of mozilla-central
and the try repository with generaldelta and lz4 compression enabled.
I may blog about what those are eventually. The teaser is that they can
make Mercurial perform a lot faster under some conditions. I'm also
using ZFS under the hood to manage repositories. Each repository is a
ZFS filesystem. This means I can create repository copies on the server
(user repositories anyone?) at a nearly free cost. Contrast this to the
traditional method of full clones, which take lots of time, memory, CPU,
and storage.</p>
<p>Anyway, some things you can do with the existing web service:</p>
<ul>
<li>Obtain metadata about Mercurial changesets.
  <a href="http://moztree.gregoryszorc.com/api/changeset/940b2974f35b">Example</a>.</li>
<li>Look up metadata about Git commits.
  <a href="http://moztree.gregoryszorc.com/api/git-sha1/40438af67c321">Example</a>.</li>
<li>Obtain a <a href="http://moztree.gregoryszorc.com/api/spore">SPORE descriptor</a>
  describing the web service endpoints. This allows you to auto-generate
  clients from descriptors. Yay!</li>
</ul>
<p>Obviously, that's not a lot. But adding new endpoints is relatively
straightforward. See the <a href="https://bitbucket.org/indygreg/moz-tree-utopia/src/tip/repodata/web/app.py">source</a>.
It's literally as easy as defining a URL mapping and writing a
database query.</p>
<p>The performance is also not the best. I just haven't put in the effort
to tune things yet. All of the querying hits the database, not
Mercurial. So, making things faster should merely be a matter of
database and hosting optimization. Patches welcome!</p>
<p>Some ideas that I haven't had time to implement yet:</p>
<ul>
<li>Return changests in a specific repository</li>
<li>Return recently pushed changesets</li>
<li>Return pushes for a given user</li>
<li>Return commits for a given author</li>
<li>Return commits referencing a given bug</li>
<li>Obtain TBPL URLs for pushes with changeset</li>
<li>Integrate bugzilla metadata</li>
</ul>
<p>Once those are in place, I foresee this service powering a number of
dashboards. Patches welcome.</p>
<p>Again, this service is only the tip of what's possible. There's a lot
that could be built on this service. I have ideas. Others have ideas.</p>
<p>The project includes a Vagrant file and Puppet
manifests for provisioning the server. It's a one-liner to get a
development environment up and running. It should be really easy to
contribute to this project. Patches welcome.</p>]]></content:encoded>
    </item>
    <item>
      <title>Why do Projects Support old Python Releases</title>
      <link>http://gregoryszorc.com/blog/2014/01/08/why-do-projects-support-old-python-releases</link>
      <pubDate>Wed, 08 Jan 2014 17:00:00 PST</pubDate>
      <category><![CDATA[Python]]></category>
      <category><![CDATA[Mozilla]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2014/01/08/why-do-projects-support-old-python-releases</guid>
      <description>Why do Projects Support old Python Releases</description>
      <content:encoded><![CDATA[<p>I see a number of open source projects supporting old versions
of Python. Mercurial supports 2.4, for example. I have to ask: why do
projects continue to support old Python releases?</p>
<p>Consider:</p>
<ul>
<li><a href="http://python.org/download/releases/2.4.6/">Python 2.4</a> was last
  released on December 19, 2008 and <strong>there will be no more releases of
  Python 2.4</strong>.</li>
<li><a href="http://python.org/download/releases/2.5.6/">Python 2.5</a> was last
  released on May 26, 2011 and <strong>there will be no more releases of
  Python 2.5</strong>.</li>
<li><a href="http://python.org/download/releases/2.6.9/">Python 2.6</a> was last
  released on October 29, 2013 and <strong>there will be no more releases of
  Python 2.6</strong>.</li>
<li><strong>Everything before Python 2.7 is end-of-lifed</strong></li>
<li>Python 2.7 continues to see periodic releases, but mostly for bug fixes.</li>
<li>Practically all of the work on CPython is happening in the 3.3 and 3.4
  branches. Other implementations continue to support 2.7.</li>
<li>Python 2.7 has been available since July 2010</li>
<li>Python 2.7 provides some very compelling language features over
  earlier releases that developers want to use</li>
<li>It's much easier to write dual compatible 2/3 Python when 2.7 is the
  only 2.x release considered.</li>
<li>Python 2.7 can be installed in userland relatively easily (see
  projects like <a href="https://github.com/yyuu/pyenv">pyenv</a>).</li>
</ul>
<p>Given these facts, I'm not sure why projects insist on supporting old
and end-of-lifed Python releases.</p>
<p><strong>I think maintainers of Python projects should seriously consider
dropping support for Python 2.6 and below.</strong> Are there really that many
people on systems that don't have Python 2.7 easily available? Why are
we Python developers inflicting so much pain on ourselves to support
antiquated Python releases?</p>
<p>As a data point, I successfully transitioned Firefox's build system
from requiring Python 2.5+ to 2.7.3+ and it was relatively
<a href="https://groups.google.com/d/msg/mozilla.dev.platform/djN02O03APc/OS8A9LuHX0sJ">pain</a>
<a href="https://bugzilla.mozilla.org/show_bug.cgi?id=870420">free</a>.
Sure, a few people complained. But as far as I know, not very many new
developers are coming in and complaining about the requirement.
If we can do it with a few thousand developers, I'm guessing your
project can as well.</p>
<p><strong>Update 2014-01-09 16:05:00 PST</strong>: This post is being discussed on
<a href="http://developers.slashdot.org/story/14/01/09/1940232/why-do-projects-continue-to-support-old-python-releases">Slashdot</a>. A lot of the comments
talk about Python 3. Python 3 is its own set of
considerations. The intended focus of this post is strictly about
dropping support for Python 2.6 and below. Python 3 is related
in that porting Python 2.x to Python 3 is much easier the higher
the Python 2.x version. This especially holds true when you want
to write Python that works simultaneously in both 2.x and 3.x.</p>]]></content:encoded>
    </item>
    <item>
      <title>Python Package Providing Clients for Mozilla Services</title>
      <link>http://gregoryszorc.com/blog/2014/01/06/python-package-providing-clients-for-mozilla-services</link>
      <pubDate>Mon, 06 Jan 2014 10:45:00 PST</pubDate>
      <category><![CDATA[Python]]></category>
      <category><![CDATA[Mozilla]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2014/01/06/python-package-providing-clients-for-mozilla-services</guid>
      <description>Python Package Providing Clients for Mozilla Services</description>
      <content:encoded><![CDATA[<p>I have a number of Python projects and tools that interact with
various Mozilla services. I had authored clients for all these services
as standalone Python modules so they could be reused across projects.</p>
<p>I have consolidated all these Python modules into a unified source
control
<a href="https://bitbucket.org/indygreg/python-mozautomation">repository</a>
and have made the project available on
<a href="https://pypi.python.org/pypi/mozautomation">PyPI</a>. You can install it
by running:</p>
<p>$ pip install mozautomation</p>
<p>Currently included in the Python package are:</p>
<ul>
<li>A client for <a href="https://treestatus.mozilla.org/">treestatus.mozilla.org</a></li>
<li>Module for extracting cookies from Firefox profiles (useful for
  programmatically getting Bugzilla auth credentials).</li>
<li>A client for reading and interpretting the
  <a href="http://builddata.pub.build.mozilla.org/buildjson/">JSON dumps of automation jobs</a></li>
<li>An interface to a SQLite database to manage associations between
  Mercurial changesets, bugs, and pushes.</li>
<li>Rudimentary parsing of commit messages to extract bugs and reviewers.</li>
<li>A client to obtain information about Firefox releases via the
  <a href="http://releases-api.mozilla.org/">releases API</a></li>
<li>A module defining common Firefox source repositories, aliases, logical
  groups (e.g. twigs and integration trees), and APIs for fetching
  pushlog data.</li>
<li>A client for the <a href="https://secure.pub.build.mozilla.org/buildapi/self-serve/">self serve API</a></li>
</ul>
<p>Documentation and testing is currently sparse. Things aren't up to my
regular high quality standard. But something is better than nothing.</p>
<p>If you are interested in contributing, drop me a line or send pull
requests my way!</p>]]></content:encoded>
    </item>
    <item>
      <title>Python Bindings Updates in Clang 3.1</title>
      <link>http://gregoryszorc.com/blog/2012/05/14/python-bindings-updates-in-clang-3.1</link>
      <pubDate>Mon, 14 May 2012 00:05:00 PDT</pubDate>
      <category><![CDATA[Python]]></category>
      <category><![CDATA[Clang]]></category>
      <category><![CDATA[compilers]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2012/05/14/python-bindings-updates-in-clang-3.1</guid>
      <description>Python Bindings Updates in Clang 3.1</description>
      <content:encoded><![CDATA[<p><a href="http://clang.llvm.org/">Clang</a> 3.1 is scheduled to be released any hour
now. And, I'm proud to say that I've contributed to it! Specifically,
I've contributed improvements to the Python bindings, which are an
interface to
<a href="http://clang.llvm.org/doxygen/group__CINDEX.html">libclang</a>, the C
interface to Clang.</p>
<p>Since 3.1 is being released today, I wanted to share some of the new
features in this release. An exhaustive list of newly supported APIs is
available in the
<a href="http://clang.llvm.org/docs/ReleaseNotes.html#pythonchanges">release notes</a>.</p>
<h2>Diagnostic Metadata</h2>
<p><em>Diagnostics</em> are how Clang represents warnings and errors during
compilation. The Python bindings
now allow you to get at more metadata. Of particilar interest is
<strong>Diagnostic.option</strong>. This property allows you to see the compiler flag
that triggered the diagnostic. Or, you could query
<strong>Diagnostic.disable_option</strong> for the compiler flag that would silence
this diagnostic.</p>
<p>These might be useful if you are analyzing diagnostics produced by the
compiler. For example, you could parse source code using the Python
bindings and collect aggregate information on all the diagnostics
encountered.</p>
<p>Here is an example:</p>
<pre><code>from clang.cindex import Index

index = Index.create()
tu = index.parse('hello.c')

for diag in tu.diagnostics:
    print diag.severity
    print diag.location
    print diag.spelling
    print diag.option
</code></pre>
<p>Or, if you are using the Python bindings from trunk:</p>
<pre><code>from clang.cindex import TranslationUnit

tu = TranslationUnit.from_source('hello.c')
...
</code></pre>
<p>Sadly, the patch that enabled this simpler usage did not make the 3.1
branch.</p>
<h2>Finding Entities from Source Location</h2>
<p>Two new APIs, <strong>SourceLocation.from_position</strong> and
<strong>Cursor.from_location</strong>, allow you to easily extract a cursor in the AST
from any arbitrary point in a file.</p>
<p>Say you want to find the element in the AST that occupies column 6 of
line #10 in the file <em>foo.c</em>:</p>
<pre><code>from clang.cindex import Cursor
from clang.cindex import Index
from clang.cindex import SourceLocation

index = Index.create()
tu = index.parse('foo.c')

f = File.from_name(tu, 'foo.c')

location = SourceLocation.from_position(tu, f, 10, 6)
cursor = Cursor.from_location(tu, location)
</code></pre>
<p>Of course, you could do this by iterating over cursors in the AST until
one with the desired source range is found. But, that would involve more
API calls.</p>
<p>I would like to say that these APIs feel klunky to me. There is lots of
redundancy in there. In my opinion, there should just be a
<strong>TranslationUnit.get_cursor(file='foo.c', line=10, column=6)</strong> that
does the right thing. Maybe that will make it into a future release.
Maybe it won't. After all, the Python bindings are really a thin wrapper
around the C API and an argument can be made that there should be
minimal extra logic and complexity in the Python bindings. Time will
tell.</p>
<h2>Type Metadata</h2>
<p>It is now possible to access more metadata on <strong>Type</strong> instances. For
example, you can:</p>
<ul>
<li>See what the elements of an array are using <strong>Type.get_array_element_type</strong></li>
<li>See how many elements are in a static array using
  <strong>Type.get_array_element_count</strong></li>
<li>Determine if a function is variadic using <strong>Type.is_function_variadic</strong></li>
<li>Inspect the Types of function arguments using <strong>Type.argument_types</strong></li>
</ul>
<p>In this example, I will show how to iterate over all the functions
declared in a file and to inspect their arguments.</p>
<pre><code>from clang.cindex import CursorKind
from clang.cindex import Index
from clang.cindex import TypeKind

index = Index.create()
tu = index.parse('hello.c')

for cursor in tu.cursor.get_children():
    # Ignore AST elements not from the main source file (e.g.
    # from included files).
    if not cursor.location.file or cursor.location.file.name != 'hello.c':
        continue

    # Ignore AST elements not a function declaration.
    if cursor.kind != CursorKind.FUNCTION_DECL:
        continue

    # Obtain the return Type for this function.
    result_type = cursor.type.get_result()

    print 'Function: %s' % cursor.spelling
    print '  Return type: %s' % result_type.kind.spelling
    print '  Arguments:'

    # Function has no arguments.
    if cursor.type.kind == TypeKind.FUNCTIONNOPROTO:
        print '    None'
        continue

    for arg_type in cursor.argument_types():
        print '    %s' % arg_type.kind.spelling
</code></pre>
<p>This example is overly simplified. A more robust solution would also
inspect the Type instances to see if they are constants, check for
pointers, check for variadic functions, etc.</p>
<p>An example application of these APIs is to build a tool which
automatically generated
<a href="http://docs.python.org/library/ctypes.html">ctypes</a> or similar
<a href="https://en.wikipedia.org/wiki/Foreign_function_interface">FFI</a>
bindings. Many of these tools today use custom parsers. Why invent a
custom (and likely complex) parser when you can call out into Clang and
have it to all the heavy lifting for you?</p>
<h2>Future Features</h2>
<p>As I write this, there are already a handful of Python binding features
checked into Clang's SVN trunk that were made after the 3.1 branch was cut.
And, I'm actively working at integrating many more.</p>
<p>Still to come to the Python bindings are:</p>
<ul>
<li>Better memory management support (currently, not all references are
  kept everywhere, so it is possible for a GC to collect and dispose of
  objects that should be alive, even though they are not in scope).</li>
<li>Support for token API (lexer output)</li>
<li>More complete coverage of Cursor and Type APIs</li>
<li>More friendly APIs</li>
</ul>
<p>I have a personal goal for the Python bindings to cover 100% of the
functionality in libclang. My work towards that goal is captured in my
<a href="https://github.com/indygreg/clang/tree/python_features/">python features</a>
branch on GitHub. I periodically clean up a patch, submit it for review,
apply feedback, and commit. That branch is highly volatile and I do
rebase. You have been warned.</p>
<p>Furthermore, I would like to add additional functionality to libclang
[and expose it to Python]. For example, I would love for libclang to
support code generation (i.e. compiling), not just parsing. This would
enable all kinds of nifty scenarios (like channeling your build system's
compiler calls through a proxy which siphons off metadata such as
diagnostics).</p>
<h2>Credits and Getting Involved</h2>
<p>I'm not alone in my effort to improve Clang's Python bindings.
Anders Waldenborg has landed a number of patches to add functionality
and tests. He has also been actively reviewing patches and implementing
official LLVM Python bindings! On the reviewing front, Manuel Klimek has been
invaluable. I've lost track of how many bugs he's caught and good
suggestions he's made. Tobias Grosser and Chandler Carruth have also
reviewed their fair share of patches and handled community contributions.</p>
<p>If you are interested in contributing to the Python bindings, we could
use your help! You can find me in <a href="http://llvm.org/docs/#irc">#llvm</a> as
IndyGreg. If I'm not around, the LLVM community is generally pretty helpful,
so I'm sure you'll get an answer. If you prefer email, send it to the
<a href="http://lists.cs.uiuc.edu/mailman/listinfo/cfe-dev">cfe-dev</a> list.</p>
<p>If you have any questions, leave them in the comments or ask using one
of the methods above.</p>]]></content:encoded>
    </item>
  </channel>
</rss>

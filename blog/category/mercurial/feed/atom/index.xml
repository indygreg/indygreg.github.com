<?xml version="1.0" encoding="UTF-8"?>
<feed
  xmlns="http://www.w3.org/2005/Atom"
  xmlns:thr="http://purl.org/syndication/thread/1.0"
  xml:lang="en"
   >
  <title type="text">Gregory Szorc's Digital Home</title>
  <subtitle type="text">Rambling on</subtitle>

  <updated>2015-05-04T20:42:13Z</updated>
  <generator uri="http://blogofile.com/">Blogofile</generator>

  <link rel="alternate" type="text/html" href="http://gregoryszorc.com/blog" />
  <id>http://gregoryszorc.com/blog/feed/atom/</id>
  <link rel="self" type="application/atom+xml" href="http://gregoryszorc.com/blog/feed/atom/" />
  <entry>
    <author>
      <name>Gregory Szorc</name>
      <uri>http://gregoryszorc.com/blog</uri>
    </author>
    <title type="html"><![CDATA[Reporting Mercurial Issues]]></title>
    <link rel="alternate" type="text/html" href="http://gregoryszorc.com/blog/2015/05/04/reporting-mercurial-issues" />
    <id>http://gregoryszorc.com/blog/2015/05/04/reporting-mercurial-issues</id>
    <updated>2015-05-04T13:45:00Z</updated>
    <published>2015-05-04T13:45:00Z</published>
    <category scheme="http://gregoryszorc.com/blog" term="Mercurial" />
    <category scheme="http://gregoryszorc.com/blog" term="Mozilla" />
    <summary type="html"><![CDATA[Reporting Mercurial Issues]]></summary>
    <content type="html" xml:base="http://gregoryszorc.com/blog/2015/05/04/reporting-mercurial-issues"><![CDATA[<p>I semi-frequently stumble upon conversations in hallways and
on irc.mozilla.org about issues people are having with Mercurial.
These conversations periodically involve a legitimate bug with
Mercurial. Unfortunately, these conversations frequently end
without an actionable result. Unless someone files a bug, pings me,
etc, the complaints disappear into ether. That's not good for
anyone and only results in bugs living longer than they should.</p>
<p>There are posters around Mozilla offices that say
<em>if you see something, file something.</em> This advice does not just
apply to Mozilla projects!</p>
<p><strong>If you encounter an issue in Mercurial, please take the time
to report it somewhere meaningful.</strong> The
<a href="https://mozilla-version-control-tools.readthedocs.org/en/latest/hgmozilla/issues.html">Reporting Issues with Mercurial</a>
page from the
<a href="https://mozilla-version-control-tools.readthedocs.org/en/latest/hgmozilla/index.html">Mercurial for Mozillians</a>
guide tells you how to do this.</p>
<p>It is OK to complain about something. But if you don't inform
someone empowered to do something about it, you are part of the
problem without being part of the solution. Please make the
incremental effort to be part of the solution.</p>]]></content>
  </entry>
  <entry>
    <author>
      <name>Gregory Szorc</name>
      <uri>http://gregoryszorc.com/blog</uri>
    </author>
    <title type="html"><![CDATA[Mercurial 3.4 Released]]></title>
    <link rel="alternate" type="text/html" href="http://gregoryszorc.com/blog/2015/05/04/mercurial-3.4-released" />
    <id>http://gregoryszorc.com/blog/2015/05/04/mercurial-3.4-released</id>
    <updated>2015-05-04T12:40:00Z</updated>
    <published>2015-05-04T12:40:00Z</published>
    <category scheme="http://gregoryszorc.com/blog" term="Mercurial" />
    <category scheme="http://gregoryszorc.com/blog" term="Mozilla" />
    <summary type="html"><![CDATA[Mercurial 3.4 Released]]></summary>
    <content type="html" xml:base="http://gregoryszorc.com/blog/2015/05/04/mercurial-3.4-released"><![CDATA[<p>Mercurial 3.4 was released on May 1 (following Mercurial's time-based
schedule of releasing a new version every 3 months).</p>
<p>3.4 is a significant release for a few reasons.</p>
<p>First, the next version of the wire protocol (<em>bundle2</em>) has been
marked as non-experimental on servers. This version of the protocol
paves over a number of deficiencies in the classic protocol. I won't
go into low-level details. But I will say that the protocol enables
some rich end-user experiences, such as having the server hand out URLs
for pre-generated bundles (e.g. offload clones to S3), atomic push
operations, and advanced workflows, such as having the server rebase
automatically on push. Of course, you'll need a server running 3.4
to realize the benefits of the new protocol. hg.mozilla.org won't be
updated until at least June 1.</p>
<p>Second, Mercurial 3.4 contains improvements to the tags cache to make
performance concerns a thing of the past. Due to the structure of the
Firefox repositories, the previous implementation of the tags cache
could result in pauses of dozens of seconds during certain workflows.
The problem should go away with Mercurial 3.4. <strong>Please note that on
first use of Mercurial 3.4, your repository may perform a one-time
upgrade of the tags cache. This will spin a full CPU core and will
take up to a few minutes to complete on Firefox repos. Let it run to
completion and performance should not be an issue again.</strong> I wrote the
patches to change the tags cache (with lots of help from Pierre-Yves
David, a Mercurial core contributor). So if you find anything wrong,
I'm the one to complain to.</p>
<p>Third, the HTTP interface to Mercurial (hgweb) now has JSON output
for nearly every endpoint. The implementation isn't yet complete,
but it is better than nothing. But, it should be good enough for
services to start consuming it. Again, this won't be available on
hg.mozilla.org until the server is upgraded on June 1 at the earliest.
This is a feature I added to core Mercurial. If you have feature
requests, send them my way.</p>
<p>Fourth, a number of performance regressions introduced in Mercurial
3.3 were addressed. These performance issues frequently manifested
during <em>hg blame</em> operations. Many Mozillians noticed them on
hg.mozilla.org when looking at blame through the web interface.</p>
<p>For a more comprehensive list of changes, see
<a href="https://groups.google.com/d/msg/mozilla.dev.version-control/z4aWvBoAGYw/d0hUGKJU_psJ">my post about the 3.4 RC</a>
and the
<a href="http://mercurial.selenic.com/wiki/WhatsNew#Mercurial_3.4_.282015-05-01.29">official release notes</a>.</p>
<p>3.4 was a significant release. There are compelling reasons to upgrade.
That being said, there were a lot of changes in 3.4. If you want to wait
until 3.4.1 is released (scheduled for June 1) so you don't run into
any regressions, nobody can fault you for that.</p>
<p>If you want to upgrade, I recommend reading the
<a href="https://mozilla-version-control-tools.readthedocs.org/en/latest/hgmozilla/installing.html">Mercurial for Mozillians Installation Page</a>.</p>]]></content>
  </entry>
  <entry>
    <author>
      <name>Gregory Szorc</name>
      <uri>http://gregoryszorc.com/blog</uri>
    </author>
    <title type="html"><![CDATA[Automatically Redirecting Mercurial Pushes]]></title>
    <link rel="alternate" type="text/html" href="http://gregoryszorc.com/blog/2015/04/30/automatically-redirecting-mercurial-pushes" />
    <id>http://gregoryszorc.com/blog/2015/04/30/automatically-redirecting-mercurial-pushes</id>
    <updated>2015-04-30T12:30:00Z</updated>
    <published>2015-04-30T12:30:00Z</published>
    <category scheme="http://gregoryszorc.com/blog" term="Mercurial" />
    <category scheme="http://gregoryszorc.com/blog" term="Mozilla" />
    <summary type="html"><![CDATA[Automatically Redirecting Mercurial Pushes]]></summary>
    <content type="html" xml:base="http://gregoryszorc.com/blog/2015/04/30/automatically-redirecting-mercurial-pushes"><![CDATA[<p>Managing URLs in distributed version control tools can be a pain,
especially if multiple repositories are involved. For example,
with Mozilla's repository-based code review workflow (you push to a
special review repository to initiate code review - this is conceptually
similar to GitHub pull requests), there exist
<a href="https://reviewboard-hg.mozilla.org/">separate code review repositories</a>
for each logical repository. Figuring out how repositories map to each
other and setting up remote paths for each new clone can be a pain and
time sink.</p>
<p>As of today, we can now do something better.</p>
<p>If you push to <em>ssh://reviewboard-hg.mozilla.org/autoreview</em>,
Mercurial will automatically figure out the appropriate review
repository and redirect your push automatically. In other words, if we
have <a href="https://reviewboard.mozilla.org/">MozReview</a> set up to review
whatever repository you are working on,
your push and review request will automatically go through. No need to
figure out what the appropriate review repo is or configure
repository URLs!</p>
<p>Here's what it looks like:</p>
<div class="pygments_murphy"><pre>$ hg push review
pushing to ssh://reviewboard-hg.mozilla.org/autoreview
searching for appropriate review repository
redirecting push to ssh://reviewboard-hg.mozilla.org/version-control-tools/
searching for changes
remote: adding changesets
remote: adding manifests
remote: adding file changes
remote: added 1 changesets with 1 changes to 1 files
remote: Trying to insert into pushlog.
remote: Inserted into the pushlog db successfully.
submitting 1 changesets for review

changeset:  11043:b65b087a81be
summary:    mozreview: create per-commit identifiers (bug 1160266)
review:     https://reviewboard.mozilla.org/r/7953 (draft)

review id:  bz://1160266/gps
review url: https://reviewboard.mozilla.org/r/7951 (draft)
(visit review url to publish this review request so others can see it)
</pre></div>

<p>Read the <a href="https://mozilla-version-control-tools.readthedocs.org/en/latest/mozreview/install.html#review-repositories">full instructions</a>
for more details.</p>
<p>This requires an updated <a href="https://hg.mozilla.org/hgcustom/version-control-tools/">version-control-tools</a>
repository, which you can get by running <em>mach mercurial-setup</em> from a
Firefox repository.</p>
<p>For those that are curious, the <em>autoreview</em> repo/server advertises a list
of repository URLs and their root commit SHA-1. The client automatically
sends the push to a URL sharing the same root commit. The
<a href="https://hg.mozilla.org/hgcustom/version-control-tools/rev/0b02cd388590">code</a>
is quite simple.</p>
<p>While this is only implemented for MozReview, I could envision us doing
something similar for other centralized repository-centric services, such
as <em>Try</em> and <em>Autoland</em>. Stay tuned.</p>]]></content>
  </entry>
  <entry>
    <author>
      <name>Gregory Szorc</name>
      <uri>http://gregoryszorc.com/blog</uri>
    </author>
    <title type="html"><![CDATA[New High Scores for hg.mozilla.org]]></title>
    <link rel="alternate" type="text/html" href="http://gregoryszorc.com/blog/2015/03/19/new-high-scores-for-hg.mozilla.org" />
    <id>http://gregoryszorc.com/blog/2015/03/19/new-high-scores-for-hg.mozilla.org</id>
    <updated>2015-03-20T09:55:00Z</updated>
    <published>2015-03-19T20:20:00Z</published>
    <category scheme="http://gregoryszorc.com/blog" term="Mercurial" />
    <category scheme="http://gregoryszorc.com/blog" term="Mozilla" />
    <summary type="html"><![CDATA[New High Scores for hg.mozilla.org]]></summary>
    <content type="html" xml:base="http://gregoryszorc.com/blog/2015/03/19/new-high-scores-for-hg.mozilla.org"><![CDATA[<p>It's been a <a href="/blog/2015/03/18/network-events/">rough week</a>.</p>
<p>The very short summary of events this week is that both the Firefox
and Firefox OS release automation has been performing a denial of
service attack against
<a href="https://hg.mozilla.org/">hg.mozilla.org</a>.</p>
<p>On the face of it, this is nothing new. The release automation
is by far the top consumer of hg.mozilla.org data, requesting several
terabytes per day via several million HTTP requests from thousands of
machines in multiple data centers. The very nature of their existence
makes them a significant denial of service threat.</p>
<p>Lots of things went wrong this week. While a post mortem will shed
light on them, many fall under the umbrella of <em>release automation was
making more requests than it should have and was doing so in a way
that both increased the chances of an outage occurring and increased
the chances of a prolonged outage.</em> This resulted in the hg.mozilla.org
servers working harder than they ever have. As a result, we have some
new <em>high scores</em> to share.</p>
<ul>
<li>
<p>On UTC day March 19, hg.mozilla.org transferred 7.4 TB of data.
  This is a significant increase from the ~4 TB we expect on a typical
  weekday. (Even more significant when you consider that most load is
  generated during peak hours.)</p>
</li>
<li>
<p>During the 1300 UTC hour of March 17, the cluster received 1,363,628
  HTTP requests. No HTTP 503 Service Not Available errors were
  encountered in that window! 300,000 to 400,000 requests per hour is
  typical.</p>
</li>
<li>
<p>During the 0800 UTC hour of March 19, the cluster transferred 776 GB
  of repository data. That comes out to at least 1.725 Gbps on average
  (I didn't calculate TCP and other overhead). Anything greater than 250
  GB per hour is not very common. No HTTP 503 errors were served from
  the origin servers during this hour!</p>
</li>
</ul>
<p>We encountered many periods where hg.mozilla.org was operating more than
twice its normal and expected operating capacity and it was able to
handle the load just fine. As a server operator, I'm proud of this.
The servers were provisioned beyond what is normally needed of them and
it took a truly exceptional event (or two) to bring the service down.
This is generally a good way to do hosted services (you rarely want to be
barely provisioned because you fall over at the slighest change and you
don't want to be grossly over-provisioned because you are wasting money
on idle resources).</p>
<p>Unfortunately, the hg.mozilla.org service did fall over. Multiple times,
in fact. There is room to improve. As proud as I am that the service
operated well beyond its expected limits, I can't help but feel ashamed
that it did eventual cave in under even extreme load and that people are
probably making under-informed general assumptions like <em>Mercurial can't
scale</em>. The simple fact of the matter is that clients cumulatively
generated an exceptional amount of traffic to hg.mozilla.org this week.
All servers have capacity limits. And this week we encountered the limit
for the current configuration of hg.mozilla.org. Cause and effect.</p>]]></content>
  </entry>
  <entry>
    <author>
      <name>Gregory Szorc</name>
      <uri>http://gregoryszorc.com/blog</uri>
    </author>
    <title type="html"><![CDATA[Branch Cleanup in Firefox Repositories]]></title>
    <link rel="alternate" type="text/html" href="http://gregoryszorc.com/blog/2015/01/28/branch-cleanup-in-firefox-repositories" />
    <id>http://gregoryszorc.com/blog/2015/01/28/branch-cleanup-in-firefox-repositories</id>
    <updated>2015-01-28T20:35:00Z</updated>
    <published>2015-01-28T20:35:00Z</published>
    <category scheme="http://gregoryszorc.com/blog" term="Mercurial" />
    <category scheme="http://gregoryszorc.com/blog" term="Mozilla" />
    <summary type="html"><![CDATA[Branch Cleanup in Firefox Repositories]]></summary>
    <content type="html" xml:base="http://gregoryszorc.com/blog/2015/01/28/branch-cleanup-in-firefox-repositories"><![CDATA[<p>Mozilla has historically done some funky things with the Firefox
Mercurial repositories. One of the things we've done is create
a bunch of named branches to track the Firefox release process.
These are branch names like <em>GECKO20b12_2011022218_RELBRANCH</em>.</p>
<p>Over in
<a href="https://bugzilla.mozilla.org/show_bug.cgi?id=927219">bug 927219</a>,
we started the process of cleaning up some cruft left over from
many of these old branches.</p>
<p>For starters, the old named branches in the Firefox repositories
are being actively closed. When you <em>hg commit --close-branch</em>,
Mercurial creates a special commit that says <em>this branch is closed</em>.
Branches that are closed are automatically hidden from the output
of <em>hg branches</em> and <em>hg heads</em>. As a result, the output of these
commands is now much more usable.</p>
<p>Closed branches still constitute <em>heads</em> on the DAG. And several heads
lead to degraded performance in some situations (notably push and pull
times - the same thing happens in Git). I'd like to eventually merge
these old heads so that repositories only have 1 or a small number of
DAG heads. However, extra care must be taken before that step. Stay
tuned.</p>
<p>Anyway, for the average person reading, you probably won't be impacted
by these changes at all. The greatest impact will be from the person who
lands the first change on top of any repository whose last commit
was a branch close. If you commit on top of the <em>tip</em> commit,
you'll be committing on top of a previously closed branch! You'll
instead want to <em>hg up default</em> after you pull to ensure you are on
the proper DAG head! And even then, if you have local commits, you may
not be based on top of the appropriate commit! A simple run of
<em>hg log --graph</em> should help you decipther the state of the world.
(Please note that the usability problems around discovering the
appropriate head to land on are a result of our poor branching strategy
for the Firefox repositories. We probably should have named branches
tracking the active Gecko releases. But that ship sailed years ago
and fixing that is pretty far down the priority list. Wallpapering
over things with the
<a href="https://mozilla-version-control-tools.readthedocs.org/en/latest/hgmozilla/firefoxtree.html">firefoxtree extensions</a>
is my recommended solution until matters are fixed.)</p>]]></content>
  </entry>
  <entry>
    <author>
      <name>Gregory Szorc</name>
      <uri>http://gregoryszorc.com/blog</uri>
    </author>
    <title type="html"><![CDATA[Modern Mercurial Documentation for Mozillians]]></title>
    <link rel="alternate" type="text/html" href="http://gregoryszorc.com/blog/2015/01/15/modern-mercurial-documentation-for-mozillians" />
    <id>http://gregoryszorc.com/blog/2015/01/15/modern-mercurial-documentation-for-mozillians</id>
    <updated>2015-01-15T14:45:00Z</updated>
    <published>2015-01-15T14:45:00Z</published>
    <category scheme="http://gregoryszorc.com/blog" term="Mercurial" />
    <category scheme="http://gregoryszorc.com/blog" term="Mozilla" />
    <summary type="html"><![CDATA[Modern Mercurial Documentation for Mozillians]]></summary>
    <content type="html" xml:base="http://gregoryszorc.com/blog/2015/01/15/modern-mercurial-documentation-for-mozillians"><![CDATA[<p>Mozilla's Mercurial documentation has historically been pretty bad. The
documentation on MDN (which I refuse to link to) is horribly disjointed
and contains a lot of outdated recommendations. I've made attempts to
burn some of it to the ground, but it is just too overwhelming.</p>
<p>I've been casually creating my own Mercurial documentation tailored for
Mozillians. It's called
<a href="https://mozilla-version-control-tools.readthedocs.org/en/latest/hgmozilla/index.html">Mercurial for Mozillians</a>.</p>
<p>It started as a way to document extensions inside the
<a href="https://hg.mozilla.org/hgcustom/version-control-tools/">version-control-tools</a>
repository. But, it has since evolved to cover other topics, like how to
install Mercurial, how to develop using bookmarks, and how to interact
with a unified Firefox repository. The documentation is nowhere near
complete. But it already has some very useful content beyond what MDN
offers.</p>
<p>I'm not crazy about the idea of having generic Mercurial documentation
on a Mozilla domain (this should be part of the official Mercurial
documentation). Nor am I crazy about moving content off MDN. I'm sure
content will move to its appropriate location later. Until then,
enjoy some curated Mercurial documentation!</p>
<p>If you would like to contribute to Mercurial for Mozillians,
<a href="https://mozilla-version-control-tools.readthedocs.org/en/latest/contributing.html">read the docs</a>.</p>]]></content>
  </entry>
  <entry>
    <author>
      <name>Gregory Szorc</name>
      <uri>http://gregoryszorc.com/blog</uri>
    </author>
    <title type="html"><![CDATA[Major bzexport Updates]]></title>
    <link rel="alternate" type="text/html" href="http://gregoryszorc.com/blog/2015/01/13/major-bzexport-updates" />
    <id>http://gregoryszorc.com/blog/2015/01/13/major-bzexport-updates</id>
    <updated>2015-01-13T15:55:00Z</updated>
    <published>2015-01-13T15:55:00Z</published>
    <category scheme="http://gregoryszorc.com/blog" term="Mercurial" />
    <category scheme="http://gregoryszorc.com/blog" term="Mozilla" />
    <summary type="html"><![CDATA[Major bzexport Updates]]></summary>
    <content type="html" xml:base="http://gregoryszorc.com/blog/2015/01/13/major-bzexport-updates"><![CDATA[<p>The <em>bzexport</em> Mercurial extension - an extension that enables you to
easily create new Bugzilla bugs and upload patches to Bugzilla for
review - just received some
<a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1033394">major updates</a>.</p>
<p>First, we now have automated test coverage of bzexport! This is built on
top of the version control test harness I
<a href="/blog/2014/10/14/robustly-testing-version-control-at-mozilla/">previously blogged about</a>.
As part of the tests, we start Docker containers that run the same code
that's running on <a href="https://bugzilla.mozilla.org/">bugzilla.mozilla.org</a>,
so interactions with Bugzilla are properly tested. This is much, much
better than mocking HTTP requests and responses because if Bugzilla
changes, our tests will detect it. Yay continuous integration.</p>
<p>Second, bzexport now uses Bugzilla' REST API instead of the legacy bzAPI
endpoint for all but 1 HTTP request. This should make BMO maintainers
very happy.</p>
<p>Third and finally, bzexport now uses shared code for obtaining Bugzilla
credentials. The behavior is
<a href="https://mozilla-version-control-tools.readthedocs.org/en/latest/hgmozilla/auth.html">documented</a>,
of course. Behavior is <strong>not backwards compatible</strong>. If you were using some
old configuration values, you will now see warnings when running bzexport.
These warnings are actionable, so I shouldn't need to describe them
here.</p>
<p>Please obtain the new code by pulling the
<a href="https://hg.mozilla.org/hgcustom/version-control-tools">version-control-tools</a>
repository. Or, if you have a Firefox clone, run <em>mach mercurial-setup</em>.</p>
<p>If you find any regressions, file a bug in the
<a href="https://bugzilla.mozilla.org/enter_bug.cgi?product=Developer%20Services&amp;component=Mercurial%3A%20bzexport">Developers Services :: Mercurial: bzexport</a>
component and have it depend on
<a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1033394">bug 1033394</a>.</p>
<p>Thanks go out to Steve Fink, Ed Morley, and Ted Mielczarek for looking
at the code.</p>]]></content>
  </entry>
  <entry>
    <author>
      <name>Gregory Szorc</name>
      <uri>http://gregoryszorc.com/blog</uri>
    </author>
    <title type="html"><![CDATA[Style Changes on hg.mozilla.org]]></title>
    <link rel="alternate" type="text/html" href="http://gregoryszorc.com/blog/2015/01/09/style-changes-on-hg.mozilla.org" />
    <id>http://gregoryszorc.com/blog/2015/01/09/style-changes-on-hg.mozilla.org</id>
    <updated>2015-01-09T15:25:00Z</updated>
    <published>2015-01-09T15:25:00Z</published>
    <category scheme="http://gregoryszorc.com/blog" term="Mercurial" />
    <category scheme="http://gregoryszorc.com/blog" term="Mozilla" />
    <summary type="html"><![CDATA[Style Changes on hg.mozilla.org]]></summary>
    <content type="html" xml:base="http://gregoryszorc.com/blog/2015/01/09/style-changes-on-hg.mozilla.org"><![CDATA[<p>Starting today and continuing through next week, there will be a number
of styling changes made to <a href="https://hg.mozilla.org/">hg.mozilla.org</a>.</p>
<p>The main goal of the work is to bring the style up-to-date with upstream
Mercurial. This will result in more features being available to the web
interface, hopefully making it more useful. This includes display of
bookmarks and the Mercurial help documentation. As part of this work,
we're also removing some files on the server that shouldn't be used. If
you start getting 404s or notice an unexpected theme change, this is
probably the reason why.</p>
<p>If you'd like to look over the changes before they are made or would
like to file a bug against a regression (we suspect there will be
minor regressions due to the large nature of the changes), head on
over to <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1117021">bug 1117021</a>
or ping people in #vcs on IRC.</p>]]></content>
  </entry>
  <entry>
    <author>
      <name>Gregory Szorc</name>
      <uri>http://gregoryszorc.com/blog</uri>
    </author>
    <title type="html"><![CDATA[Why hg.mozilla.org is Slow]]></title>
    <link rel="alternate" type="text/html" href="http://gregoryszorc.com/blog/2014/12/19/why-hg.mozilla.org-is-slow" />
    <id>http://gregoryszorc.com/blog/2014/12/19/why-hg.mozilla.org-is-slow</id>
    <updated>2014-12-19T14:40:00Z</updated>
    <published>2014-12-19T14:40:00Z</published>
    <category scheme="http://gregoryszorc.com/blog" term="Mercurial" />
    <category scheme="http://gregoryszorc.com/blog" term="Mozilla" />
    <summary type="html"><![CDATA[Why hg.mozilla.org is Slow]]></summary>
    <content type="html" xml:base="http://gregoryszorc.com/blog/2014/12/19/why-hg.mozilla.org-is-slow"><![CDATA[<p>At Mozilla, I often hear statements like <em>Mercurial is slow.</em> That's a
very general statement. Depending on the context, it can mean one or
more of several things:</p>
<ul>
<li>My Mercurial workflow is not very efficient</li>
<li><em>hg</em> commands I execute are slow to run</li>
<li><em>hg</em> commands I execute appear to stall</li>
<li>The Mercurial server I'm interfacing with is slow</li>
</ul>
<p>I want to spend time talking about a specific problem: why
hg.mozilla.org (the server) is slow.</p>
<h2>What Isn't Slow</h2>
<p><strong>If you are talking to hg.mozilla.org over HTTP or HTTPS
(https://hg.mozilla.org/), there should not currently be any server
performance issues</strong>. Our Mercurial HTTP servers are pretty beefy and
are able to absorb a lot of load.</p>
<p>If https://hg.mozilla.org/ is slow, chances are:</p>
<ul>
<li>You are doing something like cloning a 1+ GB repository.</li>
<li>You are asking the server to do something really expensive (like
  generate JSON for 100,000 changesets via the pushlog query interface).</li>
<li>You don't have a high bandwidth connection to the server.</li>
<li>There is a network event.</li>
</ul>
<h2>Previous Network Events</h2>
<p>There have historically been network capacity issues in the datacenter
where hg.mozilla.org is hosted (SCL3).</p>
<p>During Mozlandia, <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1107156">excessive traffic to ftp.mozilla.org</a>
essentially saturated the SCL3 network. During this time, requests to
hg.mozilla.org were timing out: Mercurial traffic just couldn't traverse
the network. Fortunately, events like this are quite rare.</p>
<p>Up until recently, Firefox release automation was effectively
<a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1096337">overwhelming the network</a>
by doing some clownshoesy things.</p>
<p>For example, <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1096653">gaia-central was being cloned all the time</a>
We had a ~1.6 GB repository being cloned over a thousand times per day. We
were transferring close to 2 TB of gaia-central data out of Mercurial servers
per day</p>
<p>We also found issues with
<a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1096650">pushlogs sending 100+ MB responses</a>.</p>
<p>And the <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1100574">build/tools repo was getting cloned for every job</a>. Ditto for mozharness.</p>
<p>In all, <strong>we identified a few terabytes of excessive Mercurial traffic
that didn't need to exist</strong>. This excessive traffic was saturating the
SCL3 network and slowing down not only Mercurial traffic, but other
traffic in SCL3 as well.</p>
<p>Fortunately, people from Release Engineering were quick to respond to
and fix the problems once they were identified. The problem is now
firmly in control. Although, given the scale of Firefox's release
automation, any new system that comes online that talks to version
control is susceptible to causing server outages. I've already raised
this concern when reviewing some TaskCluster code. The thundering herd
of automation will be an ongoing concern. But I have plans to further
mitigate risk in 2015. Stay tuned.</p>
<p>Looking back at our historical data, it appears that we hit these
network saturation limits a few times before we reached a tipping point
in early November 2014. Unfortunately, we didn't realize this because up
until recently, we didn't have a good source of data coming from the
servers. We lacked the tooling to analyze what we had. We lacked the
experience to know what to look for. Outages are effective flashlights.
We learned a lot and know what we need to do with the data moving
forward.</p>
<h2>Available Network Bandwidth</h2>
<p>One person pinged me on IRC with the comment <em>Git is cloning much faster
than Mercurial.</em> I asked for timings and the Mercurial clone wall time
for Firefox was much higher than I expected.</p>
<p>The reason was network bandwidth. This person was performing a Git clone
between 2 hosts in EC2 but was performing the Mercurial clone between
hg.mozilla.org and a host in EC2. In other words, they were partially
comparing the performance of a 1 Gbps network against a link over the
public internet! When they did a fair comparison by removing the network
connection as a variable, the clone times rebounded to what I expected.</p>
<p>The single-homed nature of hg.mozilla.org in a single datacenter in
northern California is not only bad for disaster recovery reasons, it
also means that machines far away from SCL3 or connecting to SCL3 over a
slow network aren't getting optimal performance.</p>
<p>In 2015, expect us to build out a geo-distributed hg.mozilla.org so that
connections are hitting a server that is closer and thus faster. This
will probably be targeted at Firefox release automation in AWS first. We
want those machines to have a fast connection to the server <strong>and</strong> we
want their traffic isolated from the servers developers use so that
hiccups in automation don't impact the ability for humans to access
and interface with source code.</p>
<h2>NFS on SSH Master Server</h2>
<p>If you connect to http://hg.mozilla.org/ or https://hg.mozilla.org/, you
are hitting a pool of servers behind a load balancer. These servers have
repository data stored on local disk, where I/O is fast. In reality,
most I/O is serviced by the page cache, so local disks don't come into
play.</p>
<p>If you connect to ssh://hg.mozilla.org/, you are hitting a single,
master server. Its repository data is hosted on an NFS mount. I/O on the
NFS mount is horribly slow. Any I/O intensive operation performed on the
master is much, much slower than it should be. Such is the nature of
NFS.</p>
<p>We'll be exploring ways to mitigate this performance issue in 2015. But
it isn't the biggest source of performance pain, so don't expect anything
immediately.</p>
<h2>Synchronous Replication During Pushes</h2>
<p>When you <em>hg push</em> to hg.mozilla.org, the changes are first made on the
SSH/NFS master server. They are subsequently mirrored out to the HTTP
read-only slaves.</p>
<p>As is currently implemented, the mirroring process is performed
synchronously during the push operation. The server waits for the
mirrors to complete (to a reasonable state) before it tells the client
the push has completed.</p>
<p>Depending on the repository, the size of the push, and server and
network load, <strong>mirroring commonly adds 1 to 7 seconds to push times</strong>.
This is time when a developer is sitting at a terminal, waiting for <em>hg
push</em> to complete. The time for Try pushes can be larger: 10 to 20
seconds is not uncommon (but fortunately not the norm).</p>
<p>The current mirroring mechanism is overly simple and prone to many
failures and sub-optimal behavior. I plan to work on fixing mirroring in
2015. When I'm done, there should be no user-visible mirroring delay.</p>
<h2>Pushlog Replication Inefficiency</h2>
<p>Up until yesterday (when we
<a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1112415">deployed a rewritten pushlog extension</a>,
the replication of pushlog data from master to server was very
inefficient. Instead of tranferring a delta of pushes since last pull,
we were literally copying the underlying SQLite file across the network!</p>
<p>Try's pushlog is ~30 MB. mozilla-central and mozilla-inbound are in the
same ballpark. 30 MB x 10 slaves is a lot of data to transfer. These
operations were capable of periodically saturating the network, slowing
everyone down.</p>
<p>The rewritten pushlog extension performs a delta transfer automatically
as part of <em>hg pull</em>. Pushlog synchronization now completes in
milliseconds while commonly only consuming a few kilobytes of network
traffic.</p>
<p>Early indications reveal that deploying this change yesterday decreased the
push times to repositories with long push history by 1-3s.</p>
<h2>Try</h2>
<p>Pretty much any interaction with the
<a href="https://hg.mozilla.org/try">Try repository</a> is guaranteed to have poor
performance. The Try repository is doing things that distributed
versions control systems weren't designed to do. This includes Git.</p>
<p>If you are using Try, all bets are off. Performance will be problematic
until we roll out the <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1055298">headless try</a>
repository.</p>
<p>That being said, we've made changes recently to make Try perform better.
<a href="https://twitter.com/lxt/status/546042588822642688">The median time for pushing to Try</a>
has decreased significantly in the past few weeks. The first dip in
mid-November was due to upgrading the server from Mercurial 2.5 to
Mercurial 3.1 and from converting Try to use <em>generaldelta</em> encoding.
The dip this week has been from merging all heads and from deploying the
aforementioned pushlog changes. <strong>Pushing to Try is now significantly
faster than 3 months ago.</strong></p>
<h2>Conclusion</h2>
<p>Many of the reasons for hg.mozilla.org slowness are known. More often
than not, they are due to clownshoes or inefficiencies on Mozilla's
part rather than fundamental issues with Mercurial.</p>
<p><strong>We have made significant progress at making hg.mozilla.org faster.</strong>
But we are not done. We are continuing to invest in fixing the
sub-optimal parts and making hg.mozilla.org faster yet. I'm confident
that within a few months, nobody will be able to say that the servers
are a source of pain like they have been for years.</p>
<p>Furthermore, Mercurial is investing in features to make the wire
protocol faster, more efficient, and more powerful. When deployed,
these should make pushes faster on <em>any</em> server. They will also enable
workflow enhancements, such as Facebook's experimental extension to
perform rebases as part of push (eliminating push races and having to
manually rebase when you lose the push race).</p>]]></content>
  </entry>
  <entry>
    <author>
      <name>Gregory Szorc</name>
      <uri>http://gregoryszorc.com/blog</uri>
    </author>
    <title type="html"><![CDATA[Test Drive the New Headless Try Repository]]></title>
    <link rel="alternate" type="text/html" href="http://gregoryszorc.com/blog/2014/11/20/test-drive-the-new-headless-try-repository" />
    <id>http://gregoryszorc.com/blog/2014/11/20/test-drive-the-new-headless-try-repository</id>
    <updated>2014-11-21T18:50:00Z</updated>
    <published>2014-11-20T14:45:00Z</published>
    <category scheme="http://gregoryszorc.com/blog" term="Mercurial" />
    <category scheme="http://gregoryszorc.com/blog" term="Mozilla" />
    <summary type="html"><![CDATA[Test Drive the New Headless Try Repository]]></summary>
    <content type="html" xml:base="http://gregoryszorc.com/blog/2014/11/20/test-drive-the-new-headless-try-repository"><![CDATA[<p>Mercurial and Git both experience scaling pains as the number of heads
in a repository approaches infinity. Operations like push and pull
slow to a crawl and everyone gets frustrated.</p>
<p>This is the problem Mozilla's Try repository has been dealing with
for years. We know the solution doesn't scale. But we've been content
kicking the can by resetting the repository (blowing away data)
to make the symptoms temporarily go away.</p>
<p>One of my official goals is to ship a scalable Try solution by the end
of 2014.</p>
<p>Today, I believe I finally have enough code cobbled together to
produce a working concept. <strong>And I could use your help testing it.</strong></p>
<p>I would like people to push their Try, code review, and other
miscellaneous heads to a special repository. To do this:</p>
<pre><code>$ hg push -r . -f ssh://hg@hg.gregoryszorc.com/gecko-headless
</code></pre>
<p>That is:</p>
<ul>
<li>Consider the changeset belonging to the working copy</li>
<li>Allow the creation of new heads</li>
<li>Send it to the <em>gecko-headless</em> repo on hg.gregoryszorc.com using
  SSH</li>
</ul>
<p>Here's what happening.</p>
<p>I have deployed a special repository to my personal server that I
believe will behave very similarly to the final solution.</p>
<p>When you push to this repository, instead of your changesets being
applied directly to the repository, it siphons them off to a Mercurial
bundle. It then saves this bundle somewhere along with some metadata
describing what is inside.</p>
<p>When you run <em>hg pull -r <node></em> on that repository and ask for a
changeset that exists in the bundle, the server does some magic
and returns data from the bundle file.</p>
<p>Things this repository doesn't do:</p>
<ul>
<li><strong>This repository will not actually send changesets to Try for you.</strong></li>
<li>You cannot <code>hg pull</code> or <code>hg clone</code> the repository and get all of the
  commits from bundles. This isn't a goal. It will likely never be
  supported.</li>
<li>We do not yet record a pushlog entry for pushes to the repository.</li>
<li>The hgweb HTML interface does not <strong>yet</strong> handle commits that only
  exist in bundles. People want this to work. It will eventually work.</li>
<li>Pulling from the repository over HTTP with a vanilla Mercurial install
  may not preserve phase data.</li>
</ul>
<p>The purpose of this experiment is to expose the repository to some
actual traffic patterns so I can see what's going on and get a feel
for real-world performance, variability, bugs, etc. I plan to do all
of this in the testing environment. But I'd like some real-world
use on the actual Firefox repository to give me peace of mind.</p>
<p>Please report any issues directly to me. Leave a comment here. Ping
me on IRC. Send me an email. etc.</p>
<p><strong>Update 2014-11-21: People discovered a bug with pushed changesets
accidentally being advanced to the public phase, despite the repository
being non-publishing. I have fixed the issue. But you must now push to
the repository over SSH, not HTTP.</strong></p>]]></content>
  </entry>
</feed>

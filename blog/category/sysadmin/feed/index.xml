<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0"
     xmlns:content="http://purl.org/rss/1.0/modules/content/"
     xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
     xmlns:atom="http://www.w3.org/2005/Atom"
     xmlns:dc="http://purl.org/dc/elements/1.1/"
     xmlns:wfw="http://wellformedweb.org/CommentAPI/"
     >
  <channel>
    <title>Gregory Szorc's Digital Home</title>
    <link>http://gregoryszorc.com/blog</link>
    <description>Rambling on</description>
    <pubDate>Sat, 22 Nov 2014 02:50:42 GMT</pubDate>
    <generator>Blogofile</generator>
    <sy:updatePeriod>hourly</sy:updatePeriod>
    <sy:updateFrequency>1</sy:updateFrequency>
    <item>
      <title>Deterministic and Minimal Docker Images</title>
      <link>http://gregoryszorc.com/blog/2014/10/13/deterministic-and-minimal-docker-images</link>
      <pubDate>Mon, 13 Oct 2014 16:50:00 PDT</pubDate>
      <category><![CDATA[sysadmin]]></category>
      <category><![CDATA[Docker]]></category>
      <category><![CDATA[Mozilla]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2014/10/13/deterministic-and-minimal-docker-images</guid>
      <description>Deterministic and Minimal Docker Images</description>
      <content:encoded><![CDATA[<p><a href="https://www.docker.com/">Docker</a> is a really nifty tool. It
vastly lowers the barrier to distributing and executing applications. It
forces people to think about building server side code as a collection
of discrete applications and services. When it was released, I instantly
realized its potential, including for uses it wasn't primary intended
for, such as applications in automated build and test environments.</p>
<p>Over the months, Docker's feature set has grown and many of its
shortcomings have been addressed. It's more usable than ever. Most of my
early complaints and concerns have been addressed or are actively being
addressed.</p>
<p>But one supposedly solved part of Docker still bothers me: image
creation.</p>
<p>One of the properties that gets people excited about Docker is the
ability to ship execution environments around as data. Simply produce an
image once, transfer it to a central server, pull it down from anywhere,
and execute. That's pretty damn elegant. I dare say Docker has solved
the image <em>distribution</em> problem. (Ignore for a minute that the
implementation detail of how images map to filesystems still has a few
quirks to work out. But they'll solve that.)</p>
<p>The ease at which Docker manages images is brilliant. I, like many, was
overcome with joy and marvelled at how amazing it was. But as I started
producing more and more images, my initial excitement turned to
frustration.</p>
<p>The thing that bothers me most about images is that the de facto and
recommended method for producing images is neither deterministic nor
results in minimal images. I strongly believe that the current
recommended and applied approach is far from optimal and has too many
drawbacks. Let me explain.</p>
<p>If you look at the Dockerfiles from the official
<a href="https://github.com/docker-library">Docker library</a>
(examples: <a href="https://github.com/docker-library/node/blob/51b1dd1984e287189106884c453ca506737eed78/0.11/Dockerfile">Node</a>,
<a href="https://github.com/docker-library/mysql/blob/master/5.7/Dockerfile">MySQL</a>),
you notice something in common: they tend to use <em>apt-get update</em> as one
of their first steps. For those not familiar with Apt, that command will
synchronize the package repository indexes with a remote server. In
other words, depending on when you run the command, different versions
of packages will be pulled down and the result of image creation will
differ. The same thing happens when you clone a Git repository. Depending
on when you run the command - when you create the image - you may get
different output. If you create an image from scratch today, it could have
a different version of say Python than it did the day before. This can be
a big deal, especially if you are trying to use Docker to accurately
reproduce environments.</p>
<p>This non-determinism of building Docker images really bothers me. It
seems to run counter to Docker's goal of facilitating reliable
environments for running applications. Sure, one person can
produce an image once, upload it to a Docker Registry server, and have
others pull it. But there are applications where independent production
of the same base image is important.</p>
<p>One area is the security arena. There are many people who are
justifiably paranoid about running binaries produced by others and
pre-built Docker images set off all kinds of alarms. So, these people
would rather build an image from source, from a Dockerfile, than pull
binaries. Except then they build the image from a Dockerfile and the
application doesn't run because of an incompatibility with a new
version of some random package whose version wasn't pinned. Of course,
you probably lost numerous hours tracing down this obscure reason. How
frustrating! Determinism and verifiability as part of Docker image
creation help solve this problem.</p>
<p>Deterministic image building is also important for disaster recovery.
What happens if your Docker Registry and all hosts with copies of its
images go down? If you go to build the images from scratch again, what
guarantee do you have that things will behave the same? Without
determinism, you are taking a risk that things will be different and
your images won't work as intended. That's scary. (Yes, Docker is no
different here from existing tools that attempt to solve this problem.)</p>
<p>What if your open source product relies on a proprietary component that
can't be legally distributed? So much for Docker image distribution. The
best you can do is provide a base image and instructions for completing
the process. But if that doesn't work deterministically, your users now
have varying Docker images, again undermining Docker's goal of
increasing consistency.</p>
<p>My other main concern about Docker images is that they tend to be large,
both in size and in scope. Many Docker images use a full Linux install
as their base. A lot of people start with a base e.g. Ubuntu or Debian
install, <em>apt-get install</em> the required packages, do some extra
configuration, and call it a day. Simple and straightforward, yes. But
this practice makes me more than a bit uneasy.</p>
<p>One of the themes surrounding Docker is minimalism. Containers are
lighter than VMs; just ship your containers around; deploy dozens or
hundreds of containers simultaneously; compose your applications of
many, smaller containers instead of larger, monolithic ones. I get it
and am totally on board. So why are Docker images built on top of the
bloaty excess of a full operating system (modulo the kernel)? Do I
really need a package manager in my Docker image? Do I need a compiler
or header files so I can e.g. build binary Python extensions? No, I
don't, thank you.</p>
<p>As a security-minded person, I want my Docker images to consist of only
the files they need, especially binary files. By leaving out
non-critical elements from your image and your run-time environment,
you are reducing the surface area to attack. If your application
doesn't need a shell, don't include a shell and don't leave yourself
potentially vulnerable to
<a href="https://en.wikipedia.org/wiki/Shellshock_%28software_bug%29">shellshock</a>.
I want the attacker who inevitably breaks out of my application into the
outer container to get nothing, not something that looks like an operating
system and has access to tools like curl and wget that could potentially
be used to craft a more advanced attack (which might even be able to
exploit a kernel vulnerability to break out of the container). Of
course, you can and should pursue additional security protections in
addition to attack surface reduction to secure your execution
environment. Defense in depth. But that doesn't give Docker images a
free pass on being bloated.</p>
<p>Another reason I want smaller containers is... because they are smaller.
People tend to have relatively slow upload bandwidth. Pushing Docker
images that can be hundreds of megabytes clogs my tubes. However, I'll
gladly push 10, 20, or even 50 megabytes of only the necessary data.
When you factor in that Docker image creation isn't deterministic, you
also realize that different people are producing different versions of
images from the same Dockerfiles and that you have to spend extra
bandwidth transferring the different versions around. This bites me all
the time when I'm creating new images and am experimenting with the
creation steps. I tend to bypass the fake caching mechanism (fake
because the output isn't deterministic) and this really results in data
explosion.</p>
<p>I understand why Docker images are neither deterministic nor minimal:
making them so is a hard problem. I think Docker was right to prioritize
solving distribution (it opens up many new possibilities). But I really
wish some effort could be put into making images deterministic (and thus
verifiable) and more minimal. I think it would make Docker an even more
appealing platform, especially for the security conscious. (As an aside,
I would absolutely love if we could ship a
<a href="https://brendaneich.com/2014/01/trust-but-verify/">verifiable Firefox</a>
build, for example.)</p>
<p>These are hard problems. But they are solvable. Here's how I would do
it.</p>
<p>First, let's tackle deterministic image creation. Despite computers and
software being ideally deterministic, building software tends not to be,
so deterministic image creation is a hard problem. Even
tools like Puppet and Chef which claim to solve aspects of this problem
don't do a very good job with determinism. Read my post on
<a href="/blog/2013/06/24/the-importance-of-time-on-automated-machine-configuration/">The Importance of Time on Machine Provisioning</a>
for more on the topic. But there are solutions. <a href="http://nixos.org/">NixOS</a>
and the <a href="http://nixos.org/nix/">Nix</a> package manager have the potential
to be used as the basis of a deterministic image building platform.
The <a href="http://nixos.org/nix/about.html">high-level overview of Nix</a> is
that the inputs and contents of a package determine the package ID. If
you know how Git or Mercurial get their commit SHA-1's, it's pretty much
the same concept. In theory, two people on different machines start with
the same environment and bootstrap the exact same packages, all from
source. <a href="https://gitian.org/">Gitian</a> is a similar solution. Although I
prefer Nix's content-based approach and how it goes about managing
packages and environments. Nix feels so right as a base for
deterministically building software. Anyway, yes, fully verifiable
build environments are turtles all the way down (I recommend reading
<a href="https://blog.torproject.org/blog/deterministic-builds-part-two-technical-details">Tor's overview of the problem and their approach</a>.
However, Nix's approach addresses many of the turtles and silences most
of the critics. I would absolutely love if more and more Docker images
were the result of a deterministic build process like Nix. Perhaps you
could define the full set of packages (with versions) that would be
used. Let's call this the package manifest. You would then PGP sign and
distribute your manifest. You could then have Nix step through all the
dependencies, compiling everything from source. If PGP verification
fails, compilation output changes, or extra files are needed, the build
aborts or issues a warning. I have a feeling the security-minded
community would go crazy over this. I know I would.</p>
<p>OK, so now you can use Nix to produce packages (and thus images)
(more) deterministically. How do you make them minimal? Well, instead of
just packaging the entire environment, I'd employ tools like
<a href="http://www.floc.net/makejail/">makejail</a>. The purpose of makejail is to
create minimal chroot <em>jail</em> environments. These are very similar to
Docker/LXC containers. In fact, you can often take a tarball of a chroot
directory tree and convert it into a Docker container! With makejail,
you define a configuration file saying among other things what binaries
to run inside the jail. makejail will trace file I/O of that binary and
copy over accessed files. The result is an execution environment that
(hopefully) contains only what you need. Then, create an archive of that
environment and pipe it into <em>docker build</em> to create a minimal Docker
image.</p>
<p>In summary, Nix provides you with a reliable and verifiable build
environment. Tools like makejail pair down the produced packages into
something minimal, which you then turn into your Docker image. Regular
people can still pull binary images, but they are much smaller and more
in tune with Docker's principles of minimalism. The paranoid among us can
produce the same bits from source (after verifying the inputs look
credible and waiting through a few hours of compiling). Or, perhaps
the individual files in the image could be signed and thus verified via
trust somehow? The company deploying Docker can have peace of mind
that disaster scenarios resulting in Docker image loss should not
result in total loss of the image (just rebuild it exactly as it was
before).</p>
<p>You'll note that my proposed solution does not involve Dockerfiles as
they exist today. I just don't think Dockerfile's design of stackable
layers of commands is the right model, at least for people who care
about determinism and minimalism. You really want a recipe that
knows how to create a set of relevant files and some metadata like what
ports to expose, what command to run on container start, etc and turn
that into your Docker image. I suppose you could accomplish this all
inside Dockerfiles. But that's a pretty radical departure from how
Dockerfiles work today. I'm not sure the two solutions are compatible.
Something to think about.</p>
<p>I'm pretty sure of what it would take to add deterministic and verifiable
building of minimal and more secure Docker images. And, if someone
solved this problem, it could be applicable outside of Docker (again,
Docker images are essentially chroot environments plus metadata).
As I was putting the finishing touches on this article, I discovered
<a href="http://zef.me/6049/nix-docker/">nix-docker</a>. It looks very promising!
I hope the Docker community latches on to these ideas and makes
deterministic, verifiable, and minimal images the default, not the
exception.</p>]]></content:encoded>
    </item>
    <item>
      <title>The Importance of Time on Automated Machine Configuration</title>
      <link>http://gregoryszorc.com/blog/2013/06/24/the-importance-of-time-on-automated-machine-configuration</link>
      <pubDate>Mon, 24 Jun 2013 21:00:00 PDT</pubDate>
      <category><![CDATA[sysadmin]]></category>
      <category><![CDATA[Mozilla]]></category>
      <category><![CDATA[Puppet]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2013/06/24/the-importance-of-time-on-automated-machine-configuration</guid>
      <description>The Importance of Time on Automated Machine Configuration</description>
      <content:encoded><![CDATA[<p>Usage of machine configuration management software like Puppet
and Chef has taken off in recent years. And rightly so - these
pieces of software make the lives of countless system
administrators much better (in theory).</p>
<p>In their default (and common) configuration, these pieces of
software do a terrific job of ensuring a machine is provisioned with
<em>today's</em> configuration. However, for many server provisioning
scenarios, we actually care about <em>yesterday's</em> configuration.</p>
<p>In this post, I will talk about the importance of time when configuring
machines.</p>
<h2>Describing the problem</h2>
<p>If you've worked on any kind of server application, chances are
you've had to deal with a rollback. Some new version of a package
or web application is rolled out to production. However, due to
unforeseen problems, it needed to be rolled back.</p>
<p>Or, perhaps you operate a farm of machines that continuously build
or compile software from version control. It's desirable to be able
to reproduce the output from a previous build (ideally bit
identical).</p>
<p>In these scenarios, the wall time plays a crucial rule when dealing
with a central, master configuration server (such as a Puppet
master).</p>
<p>Since a client will always pull the latest revision of its
configuration from the server, it's very easy to define your
configurations such that the result of machine provisioning
today is different from yesterday (or last week or last month).</p>
<p>For example, let's say you are running Puppet to manage a machine
that sits in a continuous integration farm and recompiles a source
tree over and over. In your Puppet manifest you have:</p>
<pre><code>package {
    "gcc":
        ensure =&gt; latest
}
</code></pre>
<p>If you run Puppet today, you may pull down GCC 4.7 from the remote
package repository because 4.7 is the latest version available. But
if you run Puppet tomorrow, you may pull down GCC 4.8 because the
package repository has been updated! If for some reason you need
to rebuild one of today's builds tomorrow (perhaps you want to
rebuild that revision plus a minor patch), they'll use different
compiler versions (or any package for that matter) and the output
may not be consistent - it may not even work at all! So much for
repeatability.</p>
<p>File templates are another example. In Puppet, file templates are
evaluated on the server and the results are sent to the client. So,
the output of file template execution today might be different from
the output tomorrow. If you needed to roll back your server to an
old version, you may not be able to do that because the template
on the server isn't backwards compatible! This can be worked around,
sure (commonly by copying the template and branching differences),
but over time these hacks accumulate in a giant pile of complexity.</p>
<p>The common issue here is that time has an impact on the outcome of
machine configuration. I refer to this issue as <strong>time-dependent
idempotency</strong>. In other words, does time play a role in the
supposedly idempotent configuration process? If the output is
consistent no matter <em>when</em> you run the configuration, it is
time-independent and truly idempotent. If it varies depending on
when configuration is performed, it is time-dependent and thus
not truly idempotent.</p>
<h2>Solving the problem</h2>
<p>My attitude towards machine configuration and automation is that
it should be as time independent as possible. If I need to revert
to yesterday's state or want to reproduce something that happened
months ago, I want strong guarantees that it will be similar, if not
identical. Now, this is just my opinion. I've worked in environments
where we had these strong guarantees. And having had this luxury,
I abhore the alternative where so many pieces of configuration vary
over time as the central configuration moves forward without the
ability to turn back the clock. As always, your needs may be
different and this post may not apply to you!</p>
<p>I said <em>as possible</em> a few times in the previous paragraph.
While you could likely make all parts of your configuration time
independent, it's not a good idea. In the real world, things
change over time and making all configuration data static regardless
of time will produce a broken or bad configuration.</p>
<p>User access is one such piece of configuration. Employees come and
go. Passwords and SSH keys change. You don't want to revert user
access to the way it was two months ago, restoring access to a
disgruntled former employee or allowing access via a compromised
password. Network configuration is another. Say the network topology
changed and the firewall rules need updating. If you reverted the
networking configuration, the machine likely wouldn't work on the
network!</p>
<p>This highlights an important fact: <strong>if making your machine
configuration time independent is a goal, you will need to
bifurcate configuration by time dependency and solve for both.</strong>
You'll need to identify every piece of configuration and
ask <em>do I put this in the bucket that is constant over time or
the bucket that changes over time?</em></p>
<p>Machine configuration software can do a terrific job of ensuring
an applied configuration is idempotent. The problem is it typically
can't manage both time-dependent and time-independent attributes
at the same time. Solving this requires a little brain power, but
is achievable if there is will. In the next section, I'll describe
how.</p>
<h3>Technical implementation</h3>
<p>Time-dependent machine configuration is a solved problem. Deploy
Puppet master (or similar) and you are good to go.</p>
<p>Time-independent configuration is a bit more complicated.</p>
<p>As I mentioned above, the first step is to isolate all of the
configuration you want to be time independent. Next, you need to
ensure time dependency doesn't creep into that configuration. You
need to identify things that can change over time and take
measures to ensure those changes won't affect the configuration.
I encourage you to employ the <em>external system
test</em>: <em>does this aspect of configuration depend on an external
system or entity? If so how will I prevent changes in it over
time from affecting us?</em></p>
<p>Package repositories are one such external system. New package
versions are released all the time. Old packages are deleted.
If your configuration says to install the <em>latest</em> package, there's
no guarantee the package version won't change unless the package
repository doesn't change. If you simply pin a package to a specific
version, that version may disappear from the server. The solution:
pin packages to specific versions <strong>and</strong> run your own package
mirror that doesn't delete or modify existing packages.</p>
<p>Does your configuration fetch a file from a remote server or use
a file as a template? Cache that file locally (in case it
disappears) and put it under version control. Have the configuration
reference the version control revision of that file. As long as the
repository is accessible, the exact version of the file can be
retrieved at any time without variation.</p>
<p>In my professional career, I've used two separate systems for
managing time-independent configuration data. Both relied heavily
on version control. Essentially, all the time-independent
configuration data is collected into a single repository - an
independent repository from all the time-dependent data
(although that's technically an implementation detail). For Puppet,
this would include all the manifests, modules, and files used
directly by Puppet. When you want to <em>activate</em> a machine with
a configuration, you simply say <em>check out revision X of this
repository and apply its configuration.</em> Since <em>revision X</em> of
the repository is constant over time, the set of configuration
data being used to configure the machine is constant. And, if
you've done things correctly, the output is idempotent over time.</p>
<p>In one of these systems, we actually had two versions of Puppet
running on a machine. First, we had the daemon communicating with a
central Puppet master. It was continually applying time-dependent
configuration (user accounts, passwords, networking, etc). We
supplemented this was a manually executed standalone Puppet
instance. When you ran a script, it asked the Puppet master for its
configuration. Part of that configuration was the revision of the
time-independent Git repository containing the Puppet configuration
files the client should use. It then pulled the Git repo, checked
out the specified revision, merged Puppet master's settings for the
node with that config (not the manifests, just some variables),
then ran Puppet locally to apply the configuration. While a
machine's configuration typically referenced a SHA-1 of a specific
Git commit to use, we could use anything <em>git checkout</em> understood.
We had some machines running <em>master</em> or other branches if we didn't
care about time-independent idempotency for that machine at that
time. What this all meant was that if you wanted to roll back
a machine's configuration, you simply specified an earlier Git
commit SHA-1 and then re-ran local Puppet.</p>
<p>We were largely satisfied with this model. We felt like we got
the best of both worlds. And, since we were using the same
technology (Puppet) for time-dependent and time-independent
configuration, it was a pretty simple-to-understand system.
A downside was there were two Puppet instances instead of one.
With a little effort, someone could probably devise a way for
the Puppet master to <em>merge</em> the two configuration trees. I'll
leave that as an exercise for the reader. Perhaps someone has
done this already! If you know of someone, please leave a comment!</p>
<h3>Challenges</h3>
<p>The solution I describe does not come without its challenges.</p>
<p>First, deciding whether a piece of configuration is time dependent
or time independent can be quiet complicated. For example,
should a package update for a critical security fix be time
dependent or time independent? It depends! What's the risk of
the machine not receiving that update? How often is that machine
rolled back? Is that package important to the operation/role
of that machine (if so, I'd lean more towards time independent).</p>
<p>Second, minimizing exposure to external entities is hard. While
I recommend putting as much as possible under version control in a
single repository and pinning versions everywhere when you interface
with an external system, this isn't always feasible. It's probably
a silly idea to have your 200 GB Apt repository under version
control and distributed locally to every machine in your network.
So, you end up introducing specialized one-off systems as necessary.
For our package repository, we just ran an internal HTTP server that
only allowed inserts (no deletes or mutates). If we were creative, we
could have likely devised a way for the client to pass a <em>revision</em> with
the request and have the server dynamically serve from that revision
of an underlying repository. Although, that may not work for every
server type due to limited control over client behavior.</p>
<p>Third, ensuring compatibility between the time-dependent configuration
and time-independent configuration is hard. This is a consequence of
separating those configurations. Will a time-independent configuration
from a revision two years ago work with the time-dependent
configuration of today? This issue can be mitigated by first
having as much configuration as possible be time independent and
second not relying on wide support windows. If it's good enough
to only support compatibility for time-independent configurations
less than a month old, then it's good enough! With this issue, I feel
you are trading long-term future incompatibility for well-defined
and understood behavior in the short to medium term. That's
a trade-off I'm willing to make.</p>
<h2>Conclusion</h2>
<p>Many machine configuration management systems only care about
idempotency <em>today</em>. However, with a little effort, it's possible
to achieve consistent state over time. This requires a little extra
effort and brain power, but it's certainly doable.</p>
<p>The next time you are programming your system configuration tool, I
hope you take the time to consider the effects time will have and that
you will take the necessary steps to ensure consistency over time
(assuming you want that, of course).</p>]]></content:encoded>
    </item>
  </channel>
</rss>

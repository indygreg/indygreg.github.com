


<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" 
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<!--
Design by Free CSS Templates
http://www.freecsstemplates.org
Released for free under a Creative Commons Attribution 2.5 License

Name       : Pollinating  
Description: A two-column, fixed-width design with dark color scheme.
Version    : 1.0
Released   : 20101114

-->
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
    
  <title>Gregory Szorc'c Digital Home</title>
<link rel="alternate" type="application/rss+xml" title="RSS 2.0" href="/blog/feed" />
<link rel="alternate" type="application/atom+xml" title="Atom 1.0"
href="/blog/feed/atom" />
<link rel="stylesheet" href="/style/style.css" type="text/css" />
<link rel="stylesheet" href="/css/pygments_murphy.css" type="text/css" />


  </head>
  <body>
    <div id="wrapper">
      
  <div id="menu">
  <ul>
    <li><a href="/">Home</a></li>
    <li><a href="/blog/">Blog</a></li>
    <li><a href="/notes">Notes</a></li>
    <li><a href="/work.html">Work</a></li>
    <li><a href="/skills.html">Skills</a></li>
    <li><a href="/thoughts.html">Thoughts</a></li>
    <li><a href="/resume.pdf">Resume</a></li>
  </ul>
</div>


      <div id="page">
        <div id="page-bgtop">
          <div id="page-bgbtm">
              <div id="content">
                
  
<div class="blog_post">
  <a name="mozilla-central-build-times"></a>
  <h2 class="blog_post_title"><a href="/blog/2012/07/29/mozilla-central-build-times" rel="bookmark" title="Permanent Link to mozilla-central Build Times">mozilla-central Build Times</a></h2>
  <small>July 29, 2012 at 01:20 PM | categories: 

<a href='/blog/category/mozilla'>Mozilla</a>, <a href='/blog/category/build-system'>build system</a>
 | <a href="http://gregoryszorc.com/blog/2012/07/29/mozilla-central-build-times#disqus_thread">View Comments</a>
</small><p/>
  <div class="post_prose">
    
  <p>In my previous post, I
<a href="http://gregoryszorc.com/blog/2012/07/29/mozilla-build-system-overview/">explained how Mozilla's build system works</a>.
In this post, I want to give people a feel for where time is spent and to
identify obvious areas that need improvement.</p>
<p>To my knowledge, nobody has provided such a comprehensive collection of
measurements before. Thus, most of our thoughts on where build time goes
have been largely lacking scientific rigor. I hope this post changes
matters and injects some much-needed quantitative figures into the
discussion.</p>
<h2>Methodology</h2>
<p>All the times reported in this post were obtained from my 2011
generation MacBook Pro. It has 8 GB of RAM and a magnetic hard drive. It
is not an ultimate build rig by any stretch of the imagination. However,
it's no slouch either. The machine has 1 physical Core i7 processor with
4 cores, each clocked at 2.3GHz. Each core has hyperthreading, so to the
OS there appears to be 8 cores. For the remainder of this post, I'll
simply state that my machine has 8 cores.</p>
<p>When I obtained measurements, I tried to limit the number of processes
running elsewhere on the machine. I also performed multiple runs and
reported the best timings. This means that the results are likely
synthetic and highly influenced by the page cache. More on that later.</p>
<p>I configured make to use up to 8 parallel processes (adding -j8 to the
make flags). I also used silent builds (the -s flag to make). Silent
builds are important because terminal rendering can add many seconds of
wall time to builds, especially on slow terminals (like Windows). I
measured results with make output being written to the terminal. In
hindsight, I wish I hadn't done this. Next time.</p>
<p>To obtain the times, I used the ubiquitous <em>time</em> utility. Wall times
are the <em>real</em> times from <em>time</em>. CPU time is the sum of the <em>user</em> and
<em>sys</em> times.</p>
<p>CPU utilization is the percentage of CPU cores busy during the wall time
of execution. In other words, I divided the CPU time by 8 times the wall
time (8 for the number of cores in my machine). 100% is impossible to
obtain, as obviously the CPU on my machine is doing other things during
measurement. But, I tried to limit the number of background processes
running, so there shouldn't have been that much noise.</p>
<p>I built a debug version of Firefox (the <em>browser</em> app in
mozilla-central) using r160922 of the Clang compiler (pulled and built
the day I did this measurement). The revision of mozilla-central being
built was <a href="https://hg.mozilla.org/mozilla-central/rev/08428deb1e89">08428edb1e89</a>.
I also had <em>--enable-tests</em>, which adds a significant amount of extra
work to builds.</p>
<h2>Configure</h2>
<p><em>time</em> reported the following for running <em>configure</em>:</p>
<pre><code>real 0m25.927s
user 0m9.595s
sys  0m8.729s
</code></pre>
<p>This is a one-time cost. Unless you are hacking on the build system or
pull down a tree change that modified the build system, you typically
don't need to worry about this.</p>
<h2>Clobber Builds with Empty ccache</h2>
<p>I built each tier separately with an empty ccache on a
recently-configured object directory. This measures the optimal worst
case time for building mozilla-central. In other words, we have nothing
cached in the object directory, so the maximum amount of work needs to
be performed. Since I measured multiple times and used the best results,
this is what I mean by <em>optimal</em>.</p>
<p>The table below contains the measurements. I omitted CPU utilization
calculation for small time values because I don't feel it is relevant.</p>
<table border="1">
  <tr>
    <th>Tier - Sub-tier</th>
    <th>Wall Time (s)</th>
    <th>CPU Time (s)</th>
    <th>CPU Utilization</th>
  </tr>
  <tr>
    <td>base export</td>
    <td>0.714</td>
    <td>0.774</td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>base libs</td>
    <td>5.081</td>
    <td>8.620</td>
    <td>21%</td>
  </tr>
  <tr>
    <td>base tools</td>
    <td>0.453</td>
    <td>0.444</td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>base (total)</td>
    <td>6.248</td>
    <td>9.838</td>
    <td>19.7%</td>
  </tr>
  <tr>
    <td>nspr</td>
    <td>9.309</td>
    <td>8.404</td>
    <td>11.3%</td>
  </tr>
  <tr>
    <td>js export</td>
    <td>1.030</td>
    <td>1.877</td>
    <td>22.8%</td>
  </tr>
  <tr>
    <td>js libs</td>
    <td>71.450</td>
    <td>416.718</td>
    <td>52%</td>
  </tr>
  <tr>
    <td>js tools</td>
    <td>0.324</td>
    <td>0.246</td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>js (total)</td>
    <td>72.804</td>
    <td>418.841</td>
    <td>71.9%</td>
  </tr>
  <tr>
    <td>platform export</td>
    <td>40.487</td>
    <td>141.704</td>
    <td>43.7%</td>
  </tr>
  <tr>
    <td>platform libs</td>
    <td>1211</td>
    <td>4896</td>
    <td>50.5%</td>
  </tr>
  <tr>
    <td>platform tools</td>
    <td>70.416</td>
    <td>90.917</td>
    <td>16.1%</td>
  </tr>
  <tr>
    <td>platform (total)</td>
    <td>1312</td>
    <td>5129</td>
    <td>48.9%</td>
  </tr>
  <tr>
    <td>app export</td>
    <td>4.383</td>
    <td>3.059</td>
    <td>8.7%</td>
  </tr>
  <tr>
    <td>app libs</td>
    <td>18.727</td>
    <td>18.976</td>
    <td>12.7%</td>
  </tr>
  <tr>
    <td>app tools (no-op)</td>
    <td>0.519s</td>
    <td>0.968</td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>app (total)</td>
    <td>23.629</td>
    <td>23.003</td>
    <td>12.2%</td>
  </tr>
  <tr>
    <td>Total</td>
    <td>1424 (23:44)</td>
    <td>5589 (93:15)</td>
    <td>49.1%</td>
  </tr>
</table>

<p>It's worth mentioning that linking libxul is part of the platform libs
tier. libxul linking should be called out because it is unlike other
parts of the build in that it is more I/O bound and can't use multiple
cores. On my machine, libxul linking (not using gold) takes ~61s. During
this time, only 1 CPU core is in use. The ~61s wall time corresponds to
roughly 5% of platform libs' wall time. Yet, even if we subtract this ~61s
from the effective CPU calculation, the percentage doesn't change much.</p>
<h2>Clobber with Populated ccache</h2>
<p>Using the ccache from a recently built tree to make C/C++ compilation
faster, I measured how long it took each tier to build on a clobber
build.</p>
<p>This measurement can be used to estimate the overhead of C/C++ compilation
during builds. In theory, the difference between CPU times between this
and the former measurement will be the amount of CPU time spent in the
C/C++ compiler.</p>
<p>This will also isolate how much time we spend <em>not</em> in the C/C++
compiler. It will arguably be very difficult to
make the C/C++ compiler faster (although things like reducing the abuse
of templates can have a measureable impact). However, we do have control
over many of the other things we do. If we find that CPU time spent
outside the C/C++ compiler is large, we can look for pieces to optimize.</p>
<p>Tiers not containing compiled files are omitted from the data.</p>
<table border="1">
  <tr>
    <th>Tier - Sub-tier</th>
    <th>Wall Time (s)</th>
    <th>CPU Time (s)</th>
    <th>ccache savings (s) (Time in Compiler)</th>
  </tr>
  <tr>
    <td>base libs</td>
    <td>1.075</td>
    <td>1.525</td>
    <td>7.095</td>
  </tr>
  <tr>
    <td>base tools</td>
    <td>1.957</td>
    <td>0.933</td>
    <td>1.522</td>
  </tr>
  <tr>
    <td>nspr</td>
    <td>5.582</td>
    <td>1.688</td>
    <td>6.716</td>
  </tr>
  <tr>
    <td>js libs</td>
    <td>22.829</td>
    <td>9.529</td>
    <td>407.189</td>
  </tr>
  <tr>
    <td>platform libs</td>
    <td>431</td>
    <td>328</td>
    <td>4568</td>
  </tr>
  <tr>
    <td>platform tools</td>
    <td>14.498</td>
    <td>25.744</td>
    <td>65.173</td>
  </tr>
  <tr>
    <td>app libs</td>
    <td>10.193</td>
    <td>15.809</td>
    <td>3.167</td>
  </tr>
  <tr>
    <td>Total</td>
    <td>487.134 (6:07)</td>
    <td>383.229 (6:23)</td>
    <td>5059 (84:19)</td>
  </tr>
</table>

<h2>No-op Build</h2>
<p>A <em>no-op</em> build is a build performed in an object directory that was
just built. Nothing changed in the source repository nor object
directory, so theoretically the build should do nothing. And, it should
be fast.</p>
<p>In reality, our build system isn't smart and performs some redundant
work. One part of redundant work is because one of the first things the
main Makefile does before invoking the tiers is delete a large chunk of
the <em>dist/</em> directory and the entirety of the <em>_tests/</em> directory from
the object directory.</p>
<p>In these measurements, I bypassed the deletion of these directories. In
other words, I measure what <em>no-op</em> builds are if we eliminate the
clown shoes around blowing away large parts of the object directory.</p>
<table border="1">
  <tr>
    <th>Tier - Sub-tier</th>
    <th>Wall Time (s)</th>
    <th>CPU Time (s)</th>
  </tr>
  <tr>
    <td>base export</td>
    <td>0.524</td>
    <td>0.537</td>
  </tr>
  <tr>
    <td>base libs</td>
    <td>0.625</td>
    <td>0.599</td>
  </tr>
  <tr>
    <td>base tools</td>
    <td>0.447</td>
    <td>0.437</td>
  </tr>
  <tr>
    <td>nspr</td>
    <td>0.809</td>
    <td>0.752</td>
  </tr>
  <tr>
    <td>js export</td>
    <td>0.334</td>
    <td>0.324</td>
  </tr>
  <tr>
    <td>js libs</td>
    <td>0.375</td>
    <td>0.361</td>
  </tr>
  <tr>
    <td>platform export</td>
    <td>10.904</td>
    <td>13.136</td>
  </tr>
  <tr>
    <td>platform libs</td>
    <td>30.969</td>
    <td>44.25</td>
  </tr>
  <tr>
    <td>platform tools</td>
    <td>8.213</td>
    <td>10.737</td>
  </tr>
  <tr>
    <td>app export</td>
    <td>0.524</td>
    <td>1.006</td>
  </tr>
  <tr>
    <td>app libs</td>
    <td>6.090</td>
    <td>13.753</td>
  </tr>
  <tr>
    <td>Total</td>
    <td>59.814</td>
    <td>85.892</td>
  </tr>
</table>

<p>So, no-op builds use ~60s of wall time and only make use of 17.9% of
available CPU resources.</p>
<h2>No-op Build With Directory Removal Silliness</h2>
<p>As mentioned above, before the tiers are iterated, the top-level
Makefile blows away large parts of <em>dist/</em> and the entirety of
<em>_tests/</em>. What impact does this have?</p>
<p>In this section, I try to isolate how much time was thrown away by doing
this.</p>
<p>First, we have to account for the deletion of these directories. On my
test build, deleting 15,005 files in these directories took ~3 seconds.</p>
<p>The table below contains my results. This is a more accurate reading
than the above on how long no-op builds takes because this is actually
what we do during normal builds. The time delta column contains the
difference between this build and a build without the removal silliness.
Positive times can be attributes to overhead associated with
repopulating <em>dist/</em> and <em>_tests/</em>.</p>
<table border="1">
  <tr>
    <th>Tier - Sub-tier</th>
    <th>Wall Time (s)</th>
    <th>Wall Time Delta (s)</th>
    <th>CPU Time (s)</th>
    <th>CPU Time Delta (s)</th>
  </tr>
  <tr>
    <td>base export</td>
    <td>0.544</td>
    <td>Negligible</td>
    <td>0.559</td>
    <td>Negligible</td>
  </tr>
  <tr>
    <td>base libs</td>
    <td>0.616</td>
    <td>Negligible</td>
    <td>0.594</td>
    <td>Negligible</td>
  </tr>
  <tr>
    <td>base tools</td>
    <td>0.447</td>
    <td>Negligible</td>
    <td>0.436</td>
    <td>Negligible</td>
  </tr>
  <tr>
    <td>nspr</td>
    <td>0.803</td>
    <td>Negligible</td>
    <td>0.743</td>
    <td>Negligible</td>
  </tr>
  <tr>
    <td>js export</td>
    <td>0.338</td>
    <td>Negligible</td>
    <td>0.329</td>
    <td>Negligible</td>
  </tr>
  <tr>
    <td>js libs</td>
    <td>0.378</td>
    <td>Negligible</td>
    <td>0.363</td>
    <td>Negligible</td>
  </tr>
  <tr>
    <td>platform export</td>
    <td>13.140</td>
    <td>2.236</td>
    <td>13.314</td>
    <td>Negligible</td>
  </tr>
  <tr>
    <td>platform libs</td>
    <td>35.290</td>
    <td>4.329</td>
    <td>43.059</td>
    <td>-1.191 (normal variance?)</td>
  </tr>
  <tr>
    <td>platform tools</td>
    <td>8.819</td>
    <td>0.606</td>
    <td>10.983</td>
    <td>0.246</td>
  </tr>
  <tr>
    <td>app export</td>
    <td>0.525</td>
    <td>Negligible</td>
    <td>1.012</td>
    <td>Negligible</td>
  </tr>
  <tr>
    <td>app libs</td>
    <td>8.876</td>
    <td>2.786</td>
    <td>13.527</td>
    <td>-0.226</td>
  </tr>
  <tr>
    <td>Total</td>
    <td>69.776</td>
    <td>9.962</td>
    <td>84.919</td>
    <td>-0.973 (normal variance)</td>
  </tr>
</table>

<p>If a delta is listed as negligible, it was within 100ms of the original
value and I figured this was either due to expected variance between
runs or below our threshold for caring. In the case of base, nspr, and
js tiers, the delta was actually much smaller than 100ms, often less
then 10ms.</p>
<p>It certainly appears that the penalty for deleting large parts of
<em>dist/</em> and the entirety of <em>_tests/</em> is about 10 seconds.</p>
<h2>The Overhead of Make</h2>
<p>We've measured supposed no-op build times. As I stated above, our no-op
builds actually aren't no-op builds. Even if we bypass the deletion of
<em>dist/</em> and <em>_tests/</em> we always evaluate some make rules. Can we measure how
much work it takes to just load the make files without actually doing
anything? This would allow us to get a rough estimate of how much we are
wasting by doing redundant work. It will also help us establish a
baseline for make overhead.</p>
<p>Turns out we can! Make has a <em>--dry-run</em> argument which evaluates the make
file but doesn't actually do anything. It simply prints what would have
been done.</p>
<p>Using <em>--dry-run</em>, I timed the different tiers. The difference from a no-op
build should roughly be the overhead associated with make itself. It is
possible that <em>--dry-run</em> adds a little overhead because it prints the
commands that would have been executed. (Previous timings were using
<em>-s</em>, which suppresses this.)</p>
<p>The delta times in the following table are the difference in times
between the true no-op build from above (the one where we don't delete
<em>dist/</em> and <em>_tests/</em>) and the times measured here. It roughly isolates
the amount of time spent outside of make, doing redundant work.</p>
<table border="1">
  <tr>
    <th>Tier - Sub-tier</th>
    <th>Wall Time (s)</th>
    <th>Wall Time Delta (s)</th>
    <th>CPU Time (s)</th>
    <th>CPU Time Delta (s)</th>
  </tr>
  <tr>
    <td>base export</td>
    <td>0.369</td>
    <td>0.155</td>
    <td>0.365</td>
    <td>0.172</td>
  </tr>
  <tr>
    <td>base libs</td>
    <td>0.441</td>
    <td>0.184</td>
    <td>0.431</td>
    <td>0.168</td>
  </tr>
  <tr>
    <td>base tools</td>
    <td>0.368</td>
    <td>0.079</td>
    <td>0.364</td>
    <td>0.073</td>
  </tr>
  <tr>
    <td>nspr</td>
    <td>0.636</td>
    <td>0.173</td>
    <td>0.591</td>
    <td>0.161</td>
  </tr>
  <tr>
    <td>js export</td>
    <td>0.225</td>
    <td>0.109</td>
    <td>0.225</td>
    <td>0.099</td>
  </tr>
  <tr>
    <td>js libs</td>
    <td>0.278</td>
    <td>0.097</td>
    <td>0.273</td>
    <td>0.088</td>
  </tr>
  <tr>
    <td>platform export</td>
    <td>3.841</td>
    <td>7.063</td>
    <td>6.108</td>
    <td>7.028</td>
  </tr>
  <tr>
    <td>platform libs</td>
    <td>8.938</td>
    <td>22.031</td>
    <td>14.723</td>
    <td>29.527</td>
  </tr>
  <tr>
    <td>platform tools</td>
    <td>3.962</td>
    <td>4.251</td>
    <td>6.185</td>
    <td>4.552</td>
  </tr>
  <tr>
    <td>app export</td>
    <td>0.422</td>
    <td>0.102</td>
    <td>0.865</td>
    <td>0.141</td>
  </tr>
  <tr>
    <td>app libs</td>
    <td>0.536</td>
    <td>5.554</td>
    <td>1.148</td>
    <td>12.605</td>
  </tr>
  <tr>
    <td>Total</td>
    <td>20.016</td>
    <td>39.798</td>
    <td>31.278</td>
    <td>54.614</td>
  </tr>
</table>

<h2>Observations</h2>
<p>The numbers say a lot. I'll get to key takeaways in a bit.</p>
<p>First, what the numbers don't show is the variance between runs.
Subsequent runs are almost always <em>significantly</em> faster than the
initial, even on no-op builds. I suspect this is due mostly to
I/O wait. In the initial tier run, files are loaded into the page cache.
Then, in subsequent runs, all I/O comes from physical memory rather than
waiting on a magnetic hard drive.</p>
<p>Because of the suspected I/O related variance, I fear that the numbers I
obtained are highly synthetic, at least for my machine. It is unlikely
I'll ever see all these numbers in one mozilla-central build. Instead,
it requires a specific sequence of events to obtain the best times
possible. And, this sequence of events is not likely to correspond with
real-world usage.</p>
<p>That being said, I think these numbers are important. If you remove I/O
from the equation - say you have an SSD with near 0 service times or
have enough memory so you don't need a hard drive - these numbers will
tell what limits you are brushing up against. And, as computers get more
powerful, I think more and more people will cross this threshold and
will be more limited by the build system than the capabilities of their
hardware. (A few months ago, I
<a href="https://groups.google.com/forum/#!topic/mozilla.dev.builds/FJclsTA_OBQ/discussion">measured resource usage</a>
when compiling mozilla-central on Linux and concluded you need roughly
9GB of dedicated memory to compile and link mozilla-central without page
cache eviction. In other words, if building on a machine with only 8GB
of RAM, your hard drive will play a role.)</p>
<p>Anyway, to the numbers.</p>
<p>I think the most important number in the above tables is <strong>49.1%</strong>. That
is the effective CPU utilization during a clobber build. This means that
<strong>during a build, on average half of the available CPU cores are
unused.</strong> Now, I could be generous and bump this number to 50.7%. That's
what the effective CPU utilization is if you remove the ~60s of libxul
linking from the calculation.</p>
<p>The 49.1% has me reaching the following conclusions:</p>
<ol>
<li>I/O wait really matters.</li>
<li>Our recursive use of make is incapable of executing more than 4 items
   at a time on average (assuming 8 cores).</li>
<li>My test machine had more CPU wait than I think.</li>
</ol>
<p>I/O wait is easy to prove: compare times on an SSD or with a similar I/O
bus with near zero service times (e.g. a filled page cache with no
eviction - like a machine with 16+ GB of memory that has built
mozilla-central recently).</p>
<p>A derived time not captured in any table is 11:39. This is the total
CPU time of a clobber build (93:15) divided by the number of cores (8).
<strong>If we had 100% CPU utilization across all cores during builds, we should
be able to build mozilla-central in 11:39.</strong> This is an ideal figure and
won't be reached. As mentioned above, libxul linking takes ~60s itself! I
think 13:00 is a more realistic optimal compilation time for a modern 8
core machine. This points out a startling number: <strong>we are wasting
~12 minutes of wall time due to not fully utilizing CPU cores during
clobber builds.</strong></p>
<p>Another important number is 5059 out of 5589, or 90.5%. That is the CPU
time in a clobber build spent in the C/C++ compiler, as measured by the
speedup of using ccache. It's unlikely we are going to make the C/C++
compiler go much faster (short of not compiling things). So, this is a
big fat block of time we will never be able to optimize. On my machine
<strong>compiling mozilla-central will always take at least ~10:30 wall time,
just in compiling C/C++.</strong></p>
<p>A clobber build with a saturated ccache took 487s wall time but only 383s
CPU time. That's only about 10% total CPU utilization. And, this
represents only 6.8% of total CPU time from the original clobber build.
Although, it is 34.2% of total wall time.</p>
<p>The above means that everything not the C/C++ compiler is horribly
inefficient. These are clown shoes of epic proportions. We're not even
using 1 full core doing build actions outside of the C/C++ compiler!</p>
<p>Because we are inefficient when it comes to core usage, I
think a key takeaway is that throwing more cores at the existing build
system will have diminishing returns. Sure, some parts of the build system
today could benefit from it (mainly js, layout, and dom, as they have
Makefile's with large numbers of source files). But, most of the build
system won't take advantage of many cores. <strong>If you want to throw money
at a build machine, I think your first choice should be an SSD. If you
can't do that, have as much memory as you can so most of your filesystem
I/O is serviced by the page cache, not your disk drive.</strong></p>
<p>In the final table, we isolated how much time <em>make</em> is spending to
just to figure out what to do. That amounts to ~20 seconds wall
time and ~31s CPU time. That leaves ~40s wall and ~55s CPU for non-make
work during no-op builds. Translation: we are doing 40s of wall time work
during no-op builds. Nothing changed. <strong>We are throwing 40s of wall time
away because the build system isn't using proper dependencies and is
doing redundant work.</strong></p>
<p>I've long been a critic of us blowing away parts of <em>dist/</em> and
<em>_tests/</em> at the top of builds. Well, after measuring it, I have mixed
reactions. It only amounts to about ~10s of added time to builds. This
doesn't seem like a lot in the grand scheme of things. However, this is
~10s on top of the ~60s it actually takes to iterate through the tiers.
So, in terms of percentages for no-op builds, it is actually quite
significant.</p>
<p>No-op builds with the existing build system take ~70s under ideal
conditions. In order of time, the breakdown is roughly:</p>
<ul>
<li>~40s for doing redundant work in Makefiles</li>
<li>~20s for make traversal and loading overhead</li>
<li>~10s for repopulating deleted content from <em>dist/</em> and <em>_tests/</em></li>
</ul>
<p>In other words, <strong>~50s of ~70s no-op build times are spent doing work
we have already done.</strong> This is almost purely clown shoes. Assuming we
can't make make traversal and loading faster, the shortest possible
no-op build time will be ~20s.</p>
<p>Splitting things up a bit more:</p>
<ul>
<li>~22s - platform libs make evaluation</li>
<li>~20s - make file traversal and loading (readying for evaluation)</li>
<li>~10s - repopulating deleted content from <em>dist/</em> and <em>_tests/</em></li>
<li>~7s - platform export make evaluation</li>
<li>~5.5 - app libs make evaluation</li>
<li>~4s - platform tools</li>
</ul>
<p>The ~20s for make file traversal and loading is interesting. I suspect
(although I haven't yet measured) that a lot of this is due to the sheer
size of rules.mk. As I
<a href="http://gregoryszorc.com/blog/2012/07/28/makefile-execution-times/">measured</a>
on Friday, the overhead of rules.mk with pymake is significant. I
hypothesized that it would have a similar impact on GNU make. I think a
good amount of this ~20s is similar overhead. I need to isolate,
however. I am tempted to say that if we truly did no-op builds and make
Makefile's load into make faster, we could attain no-op build times in
the ~10s range. I think this is pretty damn good! Even ~20s isn't too
bad. As surprising as it is for me to say it, <strong>recursive make is not
(a significant) part of our no-op build problem</strong>.</p>
<h2>Why is the Build System Slow?</h2>
<p>People often ask the question above. As the data has told me, the
answer, like many to complicated problems, is nuanced.</p>
<p>If you are doing a clobber build on a fresh machine, the build system is
slow because 1) compiling all the C/C++ takes a lot of time (84:19 CPU
time actually) 2) we don't make efficient use of all available cores
when building. Half of the CPU horsepower during a fresh build is
unharnessed.</p>
<p>If you are doing a no-op build, the build system is slow mainly because
it is performing a lot of needless and redundant work. A significant
contributor is the overhead of make, probably due to rules.mk being
large.</p>
<p>If you are doing an incremental build, you will fall somewhere between
either extreme. You will likely get nipped by both inefficient core
usage as well as redundant work. Which one hurts the most depends on the
scope of the incremental change.</p>
<p>If you are building on a machine with a magnetic hard drive (not an
SSD), your builds are slow because you are waiting on I/O. You can
combat this by putting 8+GB of memory in your system and doing your best
to ensure that building mozilla-central can use as much of it as
possible. I highly recommend 12GB, if not 16GB.</p>
<h2>Follow-ups</h2>
<p>The measurements reported in this post are only the tip of the iceberg.
If I had infinite time, I would:</p>
<ul>
<li>Measure other applications, not just browser/Firefox. I've heard that
  mobile/Fennec's build config is far from optimal, for example. I would
  love to quantify that.</li>
<li>Set up buildbot to record and post measurements so we have a dashboard
  of build times. We have some of this today, but the granularity isn't
  as fine as what I captured.</li>
<li>Record per-directory times.</li>
<li>Isolate time spent in different processes (DTrace could be used here).</li>
<li>Capture I/O numbers.</li>
<li>Correlate the impact of I/O service times on build times.</li>
<li>Isolate the overhead of ccache (mainly in terms of I/O).</li>
<li>Obtain numbers on other platforms and systems. Ensure results can be
  reproduced.</li>
</ul>
<h2>Next Steps</h2>
<p>If we want to make our existing recursive make build backend faster, I
recommend the following actions (in no particular order):</p>
<ol>
<li>Factor pieces out of rules.mk into separate .mk files and
   conditionally load based on presence of specific variables. In other
   words, finish what we have started. This definitely cuts down on the
   overhead with pymake (as measured on Friday) and <em>likely</em> makes GNU
   make faster as well.</li>
<li>Don't blow away parts of <em>dist/</em> and <em>_tests/</em> at the top of builds.
   I know this introduces a problem where we could leave orphaned files
   in the object directory. We should solve this problem by having
   proper manifests for everything so we can detect and delete orphans.
   The cheap man's solution is to periodically clobber these
   directories.</li>
<li>Don't perform unnecessary work during no-op builds. I suspect a lot
   of redundant work is due to rules in Makefile's not the rules in
   rules.mk. As we eliminate rules from Makefile's, this problem should
   gradually go away since rules.mk is generally intelligent about these
   things.</li>
<li>More parallelism. I'm not sure how we're going to solve this with
   recursive make short of using PARALLEL_DIRS more and/or consolidating
   Makefile's together.</li>
</ol>
<p>Again, these steps apply to our current recursive make build backend.</p>
<p>Because the most significant losses are due to ungained parallelism, <strong>our
focus should be on increasing parallelism.</strong> We can only do this so much
with recursive make. It is clear now more than ever that recursive make
needs to be replaced with something that can fully realize the potential
of multiple CPU cores. That could be non-recursive make or a separate
build backend altogether.</p>
<p>We will likely not have an official alternate build backend soon. Until
then, there are no shortage of clown shoes that can be looked at.</p>
<p>The redundant work during no-op builds is definitely tempting to
address, as I think that has significant impact to most developers.
Eliminating the absurdly long no-op build times removes the needs for
hacks like <em>smart-make</em> and instills a culture of <em>trust the build
system.</em></p>
<p>I suspect a lot of the redundant work during no-op builds is due to
poorly implemented rules in individual Makefiles rather than on
silliness in rules.mk. Therefore,
<a href="https://bugzilla.mozilla.org/show_bug.cgi?id=769378">removing rules from Makefile's</a>
again seems to be one of the most important things we can do to make the
build system faster. It also prepares us for implementing newer build
backends, so it is a win-win!</p>

  </div>
</div>



  <div class="after_post"><a href="http://gregoryszorc.com/blog/2012/07/29/mozilla-central-build-times#disqus_thread">Read and Post Comments</a></div>
  <hr class="interblog" />
  
<div class="blog_post">
  <a name="mozilla-build-system-overview"></a>
  <h2 class="blog_post_title"><a href="/blog/2012/07/29/mozilla-build-system-overview" rel="bookmark" title="Permanent Link to Mozilla Build System Overview">Mozilla Build System Overview</a></h2>
  <small>July 29, 2012 at 01:15 PM | categories: 

<a href='/blog/category/mozilla'>Mozilla</a>, <a href='/blog/category/build-system'>build system</a>
 | <a href="http://gregoryszorc.com/blog/2012/07/29/mozilla-build-system-overview#disqus_thread">View Comments</a>
</small><p/>
  <div class="post_prose">
    
  <p>Mozilla's build system is a black box to many. This post attempts to
shed some light onto how it works.</p>
<h2>Configuration File</h2>
<p>The first part of building is creating a configuration file. This
defines what application to build (Firefox, Firefox OS, Fennec, etc) as
well as build options, like to create a release or debug build. This
step isn't technically required, but most people do it.</p>
<p>Configuration files currently exist as <em>mozconfig</em> files. By default,
most people create a <em>.mozconfig</em> file in the root directory of
mozilla-central.</p>
<h2>Interaction</h2>
<p>All interaction with the build system is currently gated through the
<em>client.mk</em> file in the root directory of mozilla-central. Although, I'm
trying to land an alternate (and eventual replacement) to <em>client.mk</em>
called <em>mach</em>. You can read about it in previous posts on this blog.</p>
<p>When you run <em>make -f client.mk</em>, you are invoking the build system and
telling it to do whatever it needs to do build the tree.</p>
<h2>Running Configure</h2>
<p>The first thing <em>client.mk</em> does to a fresh tree is invoke <em>configure</em>.
<em>configure</em> is a shell script in the root directory of the repository.
It is generated from the checked-in <em>configure.in</em> file using the GNU
<em>autoconf</em> utility. I won't go into detail on how autoconf works because
I don't have a beard.</p>
<p><em>configure</em> accomplishes some important tasks.</p>
<p>First, it validates that the build environment is sane. It performs some
sanity testing on the directory tree then looks at the system and build
configuration to make sure everything should work.</p>
<p>It identifies the active compiler, locations of common tools and
utilities, and ensures everything works as needed. It figures out how to
convert desired traits into system-specific options. e.g. the exact
argument to pass to the compiler to enable warnings.</p>
<p>Once <em>configure</em> determines the environment is sane, it writes out what
it learned.</p>
<p>Currently, <em>configure</em> takes what it has learned and invokes the
<em>allmakefiles.sh</em> script in the root directory. This script prints out
the set of Makefile's that will be used to build the tree for the
current configuration. <em>configure</em> takes the output of filenames and
then procedes to generate those files.</p>
<p>Generation of Makefile's is rather simple. In the source tree are a
bunch of <em>.in</em> files, typically <em>Makefile.in</em>. These contain special
markers. <em>configure</em> takes the set of determined configuration variables
and performs substitution of the variable markers in the <em>.in</em> files with
them. The <em>.in</em> files with variables substitutes are written out in the
object directory. There are also some GYP files in the source tree.
<em>configure</em> invokes a tool to convert these into Mozilla-style
Makefile's.</p>
<p><em>configure</em> also invokes <em>configure</em> for other managed projects
in mozilla-central, such as the SpiderMonkey source in <em>js/src</em>.</p>
<p><em>configure</em> finishes by writing out other miscellaneous files in the
object directory.</p>
<h2>Running Make</h2>
<p>The next step of the build is running make. <em>client.mk</em> simply points
GNU make (or pymake) at the <em>Makefile</em> in the top-level directory of the
object directory and essentially says <em>evaluate</em>.</p>
<h3>Build System Tiers</h3>
<p>The build system is broken up into different tiers. Each tier represents
a major phase or product in the build system. Most builds have the
following tiers:</p>
<ol>
<li>base - Builds global dependencies</li>
<li>nspr - Builds NSPR</li>
<li>js - Builds SpiderMonkey</li>
<li>platform - Builds the Gecko platform</li>
<li>app - Builds the configured application (e.g. Firefox, Fennec,
   Firefox OS)</li>
</ol>
<p>Inside each tier are the distinct sub-tiers:</p>
<ol>
<li>export</li>
<li>libs</li>
<li>tools</li>
</ol>
<p>A Makefile generally belongs to 1 main tier. Inside Makefile's or in
other included .mk files (make files that are not typically called
directly by make) are statements which define which directories
belong to which tiers. See
<a href="https://mxr.mozilla.org/mozilla-central/source/toolkit/toolkit-tiers.mk">toolkit-tiers.mk</a>
for an example.</p>
<p>When the top-level Makefile is invoked, it iterates through every tier
and every sub-tier within it. It starts at the first tier and evaluates
the <em>export</em> target on every Makefile/directory defined in it. It then
moves on to the <em>libs</em> target then finally the <em>tools</em> target. When it's
done with the <em>tools</em> target, it moves on to the next tier and does the
same iteration.</p>
<p>For example, we first start by evaluating the <em>export</em> target of the
<em>base</em> tier. Then we evaluate <em>base</em>'s <em>libs</em> and <em>tools</em> tiers. We then
move on to <em>nspr</em> and do the same. And, we keep going. In other words,
the build system makes 3 passes through each tier.</p>
<p>Tiers are composed of directory members. e.g. <em>dom</em> or <em>layout</em>. When
make descends into a tier member directory, it looks for specially named
variables that tell it what sub-directories are also part of this
directory. The <em>DIRS</em> variable is the most common. But, we also use
<em>TEST_DIRS</em>, <em>PARALLEL_DIRS</em>, <em>TOOL_DIRS</em>, and a few others. make will
invoke make for all defined child directories and for the children of
the children, and so on. This is what we mean by <em>recursive make</em>. make
essentially recurses into directory trees, evaluating all the
directories linearly.</p>
<p>Getting back to the tiers, the sub-tiers <em>export</em>, <em>libs</em>, and <em>tools</em>
can be thought of as <em>pre-build</em>, <em>build</em>, and <em>post-build</em> events.
Although, this analogy is far from perfect.</p>
<p><em>export</em> generally prepares the object directory for more comprehensive
building. It copies C/C++ header files into a unified object directory,
generates header files from IDLs files, etc.</p>
<p><em>libs</em> does most of the work. It compiles C++ code and performs lots of
other main work, such as Jar manifest creation.</p>
<p><em>tools</em> does a lot of miscellaneous work. If you have tests enabled,
this is where tests are typically compiled and/or installed, for
example.</p>
<h3>Processing a Makefile</h3>
<p>For each directory inside a tier, make evaluates the Makefile in that
directory for the target/sub-tier specified.</p>
<p>The basic gist of Makefile execution is actually pretty simple.</p>
<p>Mozilla's Makefiles typically look like:</p>
<pre><code>DEPTH := .
topsrcdir := @top_srcdir@
srcdir := @srcdir@
VPATH := @srcdir@

include $(DEPTH)/config/autoconf.mk

IDLSRCS := foo.idl bar.idl
CPPSRCS := hello.cpp world.cpp

include $(topsrcdir)/config/rules.mk
</code></pre>
<p>All the magic in Makefile processing happens in <em>rules.mk</em>. This make file
simply looks for specially named variables (like <em>IDLSRCS</em> or <em>CPPSRCS</em>)
and magically converts them into targets for make to evaluate.</p>
<p>In the above sample Makefile, the <em>IDLSRCS</em> variable will result in an
implicit <em>export</em> target which copies IDLs into the object directory and
compiles them to .h files. <em>CPPSRCS</em> will result in a <em>libs</em> target that
results in each .cpp file being compiled into a .o file.</p>
<p>Of course, there is nothing stopping you from defining targets/rules in
Makefile's themselves. This practice is actually quite widespread.
Unfortunately, it is a bad practice, so you shouldn't do it. The
preferred behavior is to define variables in a Makefile and have
rules.mk magically provide the make targets/rules to do stuff with them.
<a href="https://bugzilla.mozilla.org/show_bug.cgi?id=769378">Bug 769378</a> tracks
fixing this bad practice.</p>
<h2>Conclusion</h2>
<p>So, there you have it: a very brief overview of how Mozilla's build
system works!</p>
<p>In my next post, I will shed some light onto how much times goes into
different parts of the build system.</p>

  </div>
</div>



  <div class="after_post"><a href="http://gregoryszorc.com/blog/2012/07/29/mozilla-build-system-overview#disqus_thread">Read and Post Comments</a></div>
  <hr class="interblog" />
  
<div class="blog_post">
  <a name="makefile-execution-times"></a>
  <h2 class="blog_post_title"><a href="/blog/2012/07/28/makefile-execution-times" rel="bookmark" title="Permanent Link to Makefile Execution Times">Makefile Execution Times</a></h2>
  <small>July 28, 2012 at 12:45 AM | categories: 

<a href='/blog/category/mozilla'>Mozilla</a>, <a href='/blog/category/pymake'>pymake</a>, <a href='/blog/category/build-system'>build system</a>
 | <a href="http://gregoryszorc.com/blog/2012/07/28/makefile-execution-times#disqus_thread">View Comments</a>
</small><p/>
  <div class="post_prose">
    
  <p>In my course of hacking about with Mozilla's build system, I've been
using pymake (a Python implementation of GNU make) to parse, examine,
and manipulate make files. In doing so, I've learned some interesting
things, dispelling myths in the process.</p>
<p>People often say that parsing make files is slow and that the sheer
number of Makefile.in's in mozilla-central (Firefox's source tree) is
leading to lots of overhead in make execution. This statement is only
partially correct.</p>
<p><em>Parsing</em> make files is actually pretty fast. Using pymake's parser API,
I'm able to parse every Makefile.in in mozilla-central in under 5
seconds on my 2011 generation MacBook Pro using a single core. Not too
shabby, especially considering that there are about 82,500 lines in all
the Makefile.in's.</p>
<p>Evaluation of make files, however, is a completely different story. You
see, parsing a string containing make file directives is only part of
what needs to be done. Once you've parsed a make file into a statement
list (essentially an AST), you need to load that into a data structure
fit for evaluation. Because of the way make files are evaluated,
you need to iterate through every parsed statement and evaluate it
for side-effects. This occurs before you actually evaluate specific
targets in the make file itself. As I found out, this process can be
time-consuming.</p>
<p>For mozilla-central, the cost of loading the statement list into a data
structure ready for target evaluation takes about 1 minute in aggregate.
And, considering we effectively iterate through every Makefile in
mozilla-central 3 times when building (once for every tier state of
export, libs, and tools), you can multiply this figure by 3.</p>
<p>Put another way, <em>parsing</em> Makefile's is fast: loading them for
target evaluation is slow.</p>
<p>Digging deeper, I uncovered the main source of the additional overhead:
<em>rules.mk</em>.</p>
<p>Nearly every Makefile in mozilla-central has a pattern that looks like:</p>
<pre><code>DEPTH = ../..
topsrcdir = @top_srcdir@
srcdir = @srcdir@
VPATH = @srcdir@

include $(DEPTH)/config/autoconf.mk

&lt;LOCAL MAKE FILE DECLARATIONS&gt;

include $(topsrcdir)/config/rules.mk
</code></pre>
<p>We have a header boilerplate, followed by a bunch of Makefile-specific
variables definitions and rules. Finally, we include the <em>rules.mk</em>
file. This is the make file that takes specially-named variables and
converts them to rules (actions) for make to perform.</p>
<p>A typical Makefile.in is a few dozen lines or so. This often reduces to
maybe a dozen parsed statements. By contrast, <em>rules.mk</em> is massive. It
is currently 1770 lines and may include other make files, bringing the
total to ~3000 lines.</p>
<p>Pymake has an LRU cache that caches the results of <em>parsing</em> make files.
This means it only has to parse a single make file into a statement list
once (assuming no cache eviction). <em>rules.mk</em> is frequently used, so it
should have no eviction. Even if it were evicted, I've measured that
<em>parsing</em> is pretty fast.</p>
<p>Unfortunately, the cache doesn't help with evaluation. For every
Makefile in mozilla-central, pymake will need to evaluate rules.mk
within the context of that specific Makefile. It's impossible to cache
the results of a previous evaluation because the side-effects of rules.mk
are determined by what is defined in the Makefile that includes it.</p>
<p>I performed an experiment where I stripped the <em>include rules.mk</em>
statement from all parsed Makefile.in's. This essentially isolates the
overhead of loading <em>rules.mk</em>. It turns out that all but ~2 seconds
of evaluation time is spent in <em>rules.mk</em>. In other words,
without <em>rules.mk</em>, the Makefile.in's are loaded and ready for evaluation
in just a few seconds (over parsing time), not ~1 minute!</p>
<p>What does this all mean?</p>
<p>Is <em>parsing</em> make files slow? Technically no. <em>Parsing</em> itself is not slow.
It is actually quite fast! Pymake even surprised me at how fast it can
parse all the Makefile.in's in mozilla-central.</p>
<p><em>Loading</em> parsed make file statements to be ready for evaluation is
actually the bit that is slow - at least in the case of mozilla-central.
Specifically, the loading of <em>rules.mk</em> is what constitutes the
overwhelming majority of the time spent loading Makefile's.</p>
<p>That being said, <em>parsing</em> and <em>loading</em> go hand in hand. You almost
never parse a make file without loading and evaluating it. So, if you
consider <em>parsing</em> to include parsing <em>and</em> readying the make file for
execution, there is some truth to the statement that parsing make files
is slow. Someone splitting hairs may say differently.</p>
<p>Is there anything we can do? Good question.</p>
<p>I believe that build times of mozilla-central can be reduced by reducing
the size of <em>rules.mk</em>. Obviously, the content of <em>rules.mk</em> is
important, so we can't just delete content. But, we can be more
intelligent about how it is loaded. For example, we can move pieces of
<em>rules.mk</em> into separate <em>.mk</em> files and conditionally include these
files based on the presence of specific variables. We already do this
today, but only partially: there are still a number of bits of rules.mk
that could be factored out into separate files. By conditionally loading
make file content from <em>rules.mk</em>, we would be reducing the number of
statements that need to be loaded before evaluating each Makefile. And,
this should, in turn, make build times faster. Keep in mind that any
savings will be multiplied by roughly 3 since we do 3 passes over
Makefile's during a build.</p>
<p>To my knowledge, there aren't any bugs yet on file to do this. Given the
measurements I've obtained, I encourage somebody to do this work. Even
if it doesn't reduce build times, I think it will be a win since it will
make the make rules easier to understand since they will be contained in
function-specific files rather than one monolithic file. At worse, we
have better readability. At best, we have better readability and faster
build times. Win!</p>
<p>Finally, I don't know what the impact on GNU make is. Presumably, GNU
make evaluates make files faster than pymake (C is generally faster than
python). Therefore, reducing the size of <em>rules.mk</em> should make GNU make
faster. By how much, I have no clue.</p>

  </div>
</div>



  <div class="after_post"><a href="http://gregoryszorc.com/blog/2012/07/28/makefile-execution-times#disqus_thread">Read and Post Comments</a></div>
  <hr class="interblog" />
  
<div class="blog_post">
  <a name="mozilla-build-system-plan-of-attack"></a>
  <h2 class="blog_post_title"><a href="/blog/2012/07/25/mozilla-build-system-plan-of-attack" rel="bookmark" title="Permanent Link to Mozilla Build System Plan of Attack">Mozilla Build System Plan of Attack</a></h2>
  <small>July 25, 2012 at 11:30 PM | categories: 

<a href='/blog/category/mozilla'>Mozilla</a>, <a href='/blog/category/build-system'>build system</a>
 | <a href="http://gregoryszorc.com/blog/2012/07/25/mozilla-build-system-plan-of-attack#disqus_thread">View Comments</a>
</small><p/>
  <div class="post_prose">
    
  <p>Since I published my <a href="http://gregoryszorc.com/blog/2012/06/25/improving-mozilla%27s-build-system/">brain dump
post</a>
on improving Mozilla's build system for Gecko applications (including
Firefox and Firefox OS), there has been some exciting progress.</p>
<p>It wasn't stated in that original post, but the context for that post
was to propose a plan in preparation of a meeting between the core
contributors to the build system at Mozilla. I'm pleased to report that
the plan was generally well-received.</p>
<p>We pretty much all agreed that parts 1, 2, and 3 are all important and
we should actively work towards them. Parts 4, 5, and 6 were a bit more
contentious. There are some good parts and some bad parts. We're not
ready to adopt them just quite yet. (Don't bother reading the original
post to look up what these parts corresponded to - I'll cover that
later.)</p>
<p>Since that post and meeting, there has also been additional discussion
around more specifics. I am going to share with you now where we stand.</p>
<h2>BuildSplendid</h2>
<p><strong>BuildSplendid</strong> is an umbrella term associated with projects/goals to
make the developer experience (including building) better - more
splendid if you will.</p>
<p><strong>BuildSplendid</strong> started as the name of my personal Git branch for
hacking on the build system. I'm encouraging others to adopt the term
because, well, it is easier to refer to (people like project codenames).
If it doesn't stick, that's fine by me - there are other terms that
will.</p>
<h2>BuildFaster</h2>
<p>An important project inside <em>BuildSplendid</em> is <strong>BuildFaster</strong>.</p>
<p><em>BuildFaster</em> focuses on the following goals:</p>
<ol>
<li>Making the <em>existing</em> build system faster, better, stronger (but not
   harder).</li>
<li>Making changes to the build system to facilitate the <em>future</em> use of
   alternate build backends (like Tup or Ninja). Work to enable Visual
   Studio, Xcode, etc project generation also falls here.</li>
</ol>
<p>The distinction between these goals can be murky. But, I'll try.</p>
<p>Falling squarely in #1 are:</p>
<ul>
<li>Switching the buildbot infrastructure to use pymake on Windows</li>
</ul>
<p>Falling in #2 are:</p>
<ul>
<li>Making Makefile.in's data-centric</li>
<li>Supporting multiple build backends</li>
</ul>
<p>Conflated between the two are:</p>
<ul>
<li>Ensuring Makefile's no-op if nothing has changed</li>
<li>Optimizing existing make rules. This involves merging related
  functionality as well as eliminating clown shoes in existing rules.</li>
</ul>
<p>The two goals of <em>BuildFaster</em> roughly map to the short-term and long-term
strategies, respectively. There is consensus that recursive make (our
existing build backend) does not scale and we will plateau in terms of
performance no matter how optimal we make it. That doesn't mean we are
giving up on it: there are things we can and should do so our existing
non-recursive make backend builds faster.</p>
<p>In parallel, we will also work towards the longer-term solution of
supporting alternate build backends. <strong>This includes non-recursive
make</strong> as well as things like Tup, Ninja, and even Visual Studio and
Xcode. (I consider non-recursive make to be a separate build backend
because changing our existing Makefile.in's to support non-recursive
execution effectively means rewriting the input files (Makefile.in's).
At that point, you've invented a new build backend.)</p>
<p>For people who casually interact with the build system, these two goals
will blend together. It is not important for most to know what bucket
something falls under.</p>
<h3>BuildFaster Action Items</h3>
<p><em>BuildFaster</em> action items are being tracked in Bugzilla using the
[BuildFaster:*] whiteboard annotation.</p>
<p>There is no explicit tracking bug for the short-term goal (#1). Instead,
we are relying on the whiteboard annotation.</p>
<p>We are tracking the longer-term goal of supporting alternate build
backends at <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=774049">bug 774049</a>.</p>
<p>The most important task to help us reach the goals is to make our
Makefile.in's data centric. This means:</p>
<ul>
<li>Makefile.in's <strong>must</strong> consist of only simple variable assignment</li>
<li>Makefile.in's <strong>must</strong> not rely on being evaluated to perform variable
  assignment.</li>
</ul>
<p>Basically, our build config should be defined by static key-value pairs.</p>
<p>This translates to:</p>
<ul>
<li>Move all rules out of Makefile.in's <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=769378">bug 769378</a></li>
<li>Remove use of $(shell) from Makefile.in's <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=769390">bug 769390</a></li>
<li>Remove filesystem functions from Makefile.in's <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=769407">bug 769407</a></li>
<li>And more, as we identify the need</li>
</ul>
<p>While we have these tracking bugs on file, we still don't have bugs
filed that track individual Makefile.in's that need updated. <strong>If you
are a contributor, you can help by doing your part to file bugs for your
Makefile.in's.</strong> If your Makefile.in violates the above rules, please
file a bug in the <strong>Build Config</strong> component of the product it is under
(typically <strong>Core</strong>). See the tree of the above bugs for examples.</p>
<p>Also part of <em>BuildFaster</em> (but not relevant to most) is the task of
changing the build system to support multiple backends. Currently,
pieces like <em>configure</em> assume the Makefile.in to Makefile conversion is
always what is wanted. These parts will be worked on by core
contributors to the build system and thus aren't of concern to most.</p>
<p>I will be the first to admit that a lot of the work to <em>purify</em>
Makefile.in's to be data centric will look like a lot of busy work with
little immediate gain. The real benefits to this work will manifest
down the road. That being said, removing rules from Makefile.in's and
implementing things as rules in rules.mk helps ensure that the
implementation is proper (rules.mk is coded by make ninjas and thus
probably does things <em>right</em>). This can lead to faster build times.</p>
<h2>mozbuild</h2>
<p><strong>mozbuild</strong> is a Python package that provides an API to the build system.</p>
<p>What <em>mozbuild</em> will contain is still up in the air because it hasn't
landed in mozilla-central yet. In the code I'm waiting on review to
uplift to mozilla-central, <em>mozbuild</em> contains:</p>
<ul>
<li>An API for invoking the build system backend (e.g. launching make). It
  basically reimplements client.mk because client.mk sucks and needs to
  be replaced.</li>
<li>An API for launching tests easily. This reimplements functionality in
  testsuite-targets.mk, but in a much cleaner way. Running a single test
  can now be done with a single Python function call. This may sound
  boring, but it is very useful. You just import a module and pass a
  filesystem path to a test file to a function and a test runs. Boom!</li>
<li>Module for extracting compiler warnings from build output and storing
  in a persisted database for post-build retrieval. Compiler warning
  tracking \o/</li>
<li>Module for converting build system output into structured logs. It
  records things like time spent in different directories, etc. We could
  use this for tracking build performance regressions. We just need a
  arewe*yet.com domain...</li>
<li>A replacement for .mozconfig's that sucks less (stronger validation,
  settings for not just build config, convenient Python API, etc).</li>
</ul>
<p>And, upcoming features which I haven't yet tried to land in
mozilla-central include:</p>
<ul>
<li>API for extracting metadata from Makefile.in's and other frontend
  files. Want a Python class instance describing the IDLs defined in an
  individual Makefile.in or across the entire tree? <em>mozbuild</em> can provide
  that. This functionality will be used to configure alternate build
  backends.</li>
<li>Build backend API which allows for different build backends to be
  configured (e.g. recursive make, Tup, Ninja, etc). When we support
  multiple build backends, they'll live in <em>mozbuild</em>.</li>
</ul>
<p><em>mozbuild</em> can really be thought of as a clean backend to the build
system and related functionality (like running tests). Everything in
<em>mozbuild</em> could exist in make files or in .py files littered in
<em>build/</em>, <em>config/</em>, etc. But, that would involve maintaining make files
and/or not having a cohesive API. I wanted a clean slate that was free
from the burdens of the existing world. <em>mozbuild</em> was born.</p>
<p>I concede that there will be non-clear lines of division between
<em>mozbuild</em> and other Python packages and/or Mozilla modules. For
example, is <em>mozbuild</em> the appropriate location to define an API for
taking the existing build configuration and launching a Mochitest? I'm
not sure. For now, I'm stuffing functionality inside <em>mozbuild</em> unless
there is a clear reason for it to exist elsewhere. If we want to
separate (because of module ownership issues, for example), we can do
that.</p>
<p>My vision for <em>mozbuild</em> is for it to be the answer to the question
<em>how does the Mozilla build system work?</em> You should be able to say,
<em>look at the code in python/mozbuild and you will have all the answers.</em></p>
<h3>mozbuild Action Items</h3>
<p>The single action item for <em>mozbuild</em> is getting it landed. I have code
written that I think is good enough for an initial landing (with obvious
shortcomings being addressed in follow-up bugs). It just needs some love
from reviewers.</p>
<p>Landing <em>mozbuild</em> is tracked in
<a href="https://bugzilla.mozilla.org/show_bug.cgi?id=751795">bug 751795</a>. I
initially dropped a monolithic patch. I have since started splitting
bits up into bite-sized patches to facilitate faster, smaller reviews.
(See the blocking bugs.)</p>
<h2>mach</h2>
<p><em>mozbuild</em> is just a Python package - an API. It has no frontend.</p>
<p>Enter <strong>mach</strong>.</p>
<p><strong>mach</strong> is a command-line frontend to <em>mozbuild</em> and beyond.</p>
<p>Currently, <em>mach</em> provides some convenient shortcuts for performing
common tasks. e.g. you can run a test by tab-completing to its filename
using your shell. It also provides nifty output in supported terminals,
including colorizing and a basic progress indicator during building.</p>
<p>You can think of <em>mach</em> as a replacement for <em>client.mk</em> and other make
targets. But, <em>mach</em>'s purpose doesn't end there. My vision for <em>mach</em>
is for it to be the one-stop shop for all your mozilla-central
interaction needs.</p>
<p>From a mozilla-central tree, you should be able to type
<strong>./mach <action></strong> and do whatever you need to do. This could be
building Firefox, running tests, uploading a patch to Bugzilla, etc.</p>
<p>I'm capturing ideas for <em>mach</em> features in
<a href="https://bugzilla.mozilla.org/show_bug.cgi?id=774108">bug 774108</a>.</p>
<h3>mach Action Items</h3>
<p><em>mach</em> is in the same boat as <em>mozbuild</em>: it's waiting for reviewer
love. If you are interested in reviewing it, please let me know.</p>
<p>Once <em>mach</em> lands, I will be looking to the larger community to improve
it. I want people to go wild implementing features. I believe that
<em>mach</em> will have a significant positive impact on driving contributions
to Mozilla because it will make the process much more streamlined and
less prone to error. I think it is a no-brainer to check in <em>mach</em> as
soon as possible so these wins can be realized.</p>
<p>There is an open question of who will own <em>mach</em> in terms of module
ownership. <em>mach</em> isn't really part of anything. Sure, it interacts with
the build system, testing code, tools, etc. But, it isn't actually part
of any of that. Maybe a new module will be created for it. I'm not
familiar with how the modules system works and I would love to chat with
someone about options here.</p>
<h2>Project Interaction</h2>
<p>How are all of these projects related?</p>
<p><em>BuildFaster</em> is what everyone is working on today. It currently focuses
on making the existing recursive make based build backend faster using
whatever means necessary. <em>BuildFaster</em> could theoretically evolve to
cover other build backends (like non-recursive make). Time will tell
what falls under the <em>BuildFaster</em> banner.</p>
<p><em>mozbuild</em> is immediately focused on providing the functionality to
enable <em>mach</em> to land.</p>
<p><em>mach</em> is about improving the overall developer experience when
it comes to contributing to Firefox and other in-tree applications (like
Firefox OS). It's related to the build system in that it provides a nice
frontend to it. That's the only relationship. <em>mach</em> isn't part of the
build system (at least not yet - it may eventually be used to perform
actions on buildbot machines).</p>
<p>Down the road, <em>mozbuild</em> will gain lots of new features core to the
build system. It will learn how to extract metadata from Makefile.in's
which can be used by other build backends. It will define a build
backend interface and the various build backends will be implemented
in mozbuild. Aspects of the existing build system currently implemented in
make files or in various Python files scattered across the tree will be
added to <em>mozbuild</em> and exposed with a clean, reusable, and testable
API.</p>
<p>Today, <em>BuildFaster</em> is the important project with all the attention.
When <em>mach</em> lands, it will (hopefully) gather a lot of attention. But,
it will be from a different group (the larger contributor community -
not just build system people).</p>
<p><em>mozbuild</em> today is only needed to support <em>mach</em>. But, once <em>mozbuild</em>
lands and <em>mach</em> is on its own, <em>mozbuild</em>'s purpose  will shift to support
<em>BuildFaster</em> and other activities under the <em>BuildSplendid</em> banner.</p>
<h2>Conclusion</h2>
<p>We have a lot of work ahead of us. Please look at the bugs linked above
and help out in any way you can.</p>

  </div>
</div>



  <div class="after_post"><a href="http://gregoryszorc.com/blog/2012/07/25/mozilla-build-system-plan-of-attack#disqus_thread">Read and Post Comments</a></div>
  <hr class="interblog" />
  
<div class="blog_post">
  <a name="improving-mozilla's-build-system"></a>
  <h2 class="blog_post_title"><a href="/blog/2012/06/25/improving-mozilla's-build-system" rel="bookmark" title="Permanent Link to Improving Mozilla's Build System">Improving Mozilla's Build System</a></h2>
  <small>June 25, 2012 at 10:15 AM | categories: 

<a href='/blog/category/make'>make</a>, <a href='/blog/category/mozilla'>Mozilla</a>, <a href='/blog/category/pymake'>pymake</a>, <a href='/blog/category/build-system'>build system</a>
 | <a href="http://gregoryszorc.com/blog/2012/06/25/improving-mozilla's-build-system#disqus_thread">View Comments</a>
</small><p/>
  <div class="post_prose">
    
  <p>Makefiles. Love them or hate them, they are ubiquitous. For projects
like Firefox (which has on the order of 1500 Makefiles), Makefiles
aren't going away any time soon. This despite that newer - arguably
better - build system (such as CMake and GYP) exist.</p>
<p>Many have decried the numerous ways in which GNU make and make files
suck. I won't pile on here. Instead, please read
<a href="http://www.conifersystems.com/whitepapers/gnu-make/">What's Wrong with GNU make</a>
for a great overview.</p>
<p>Despite the many flaws with make, I would argue that the single most
significant one is the slow speed often associated with it. People
lauding make replacements (such as
<a href="http://martine.github.com/ninja/">Ninja</a>) almost always emphasize this
one aspect above all others. (OK, Ninja isn't exactly a make replacement,
but the execution aspects are comparable.)</p>
<p>Why is make slow? It is almost always due to the use of recursive make.
This is make executing itself recursively, usually as it traverses a
directory tree. This is how Mozilla's build system works. You start in
one directory. Then, for each directory defined in the <em>DIRS</em> variable,
make invokes make in that directory. In Mozilla's case, we amplify this
by performing multiple passes over the directory tree - an <em>export</em> pass
that installs header files, etc into a common directory, a <em>libs</em> pass
that does most of the compilation, and a <em>tools</em> pass which performs
other miscellaneous actions. Each of these is a recursive make
iteration. As we say at Mozilla: clown shoes.</p>
<p>The deficiencies of recursive make and a workaround are called out in
the classic paper
<a href="http://aegis.sourceforge.net/auug97.pdf">Recursive Make Considered Harmful</a>.
Despite its age, it is still relevant - GNU make hasn't changed that
much, after all. The thesis of the paper can be boiled down to the fact
that make assembles a directed acyclic graph (DAG) and then executes it.
Using recursive make, you are assembling N DAGs instead of of 1. Each DAG
must be executed separately. This adds execution overhead. On top of
that, since the DAGs are separate, you are either
a) missing information from each DAG (it doesn't know about other
DAGs) thus creating an incomplete and possibly error-prone dependency
graph or b) duplicating information in multiple DAGs, creating more
work for make in aggregate.</p>
<p>The solution for this is non-recursive make. That's just a fancy way of
saying <em>create 1 monolithic DAG and execute it once</em> (as opposed to N
separate DAGs with at least N invocations of make). Modern build systems
like Ninja do this. While these build systems have other properties that
contribute to faster execution times, in my opinion the single feature
that has the greatest impact is consolidating the build dependencies
into a single graph. This minimizes the aggregate amount of work the
driver needs to perform and all but eliminates redundant operations.</p>
<p>Non-recursive make is typically achieved one of 2 ways:</p>
<ol>
<li>Create a single monolithic make file</li>
<li>Concatenate all of your make files together (using the built-in
<em>include</em> directive)</li>
</ol>
<p>Either way, you produce a single DAG and all is well. More on this
later.</p>
<h2>Transitioning to a Single DAG</h2>
<p>A big problem with non-recursive make is transitioning to it. For
projects like Firefox with its 1500 Makefiles, each existing in its own
little universe, this is a Herculean effort. There is a reason why
many (including Mozilla) haven't done it: it's just too hard.</p>
<p>The problem of slow build times with Firefox is important to me because
I suffer from it almost every day. So, I got to thinking of creative
ways to solve this problem. How can we work towards a monolithic DAG? How
can we do this incrementally so as to not cause too much disruption and
so the effort isn't near impossible?</p>
<p>I've come up with a plan that I believe Mozilla should pursue. The plan
consists of many different parts, each of which is described in a separate
section below. Each part is presented in roughly the order it needs to
be addressed in.</p>
<p>I want to emphasize that <strong>this is my personal opinion and what I'm about
to propose is merely that: a proposal</strong>. For all I know, people will think
I'm smoking crack and none of these ideas will be pursued.</p>
<p>Let's begin.</p>
<h2>Part 1: No Rules</h2>
<p>Make files consist of
<a href="https://www.gnu.org/software/make/manual/make.html#Rule-Introduction">rules</a>.
These are, well, rules that tell make how to evaluate a target. A rule
will say something like <em>to turn hello.cpp into hello.o, call the C++
compiler with these arguments</em>.</p>
<p>People naturally take advantage of the fact that the body of the rule
(the <em>recipe</em> in make parlance) is often similar for many targets, so
you make use of <a href="https://www.gnu.org/software/make/manual/make.html#Wildcards">wildcard rules</a>
or by specifying multiple prerequisites and/or targets for a rule.</p>
<p>Wildcard rules are great. Their problem is that the recipe is often
useful across many make files. The solution here is to put the rule
definition (and thus the recipe) in a common file and use make's
<a href="https://www.gnu.org/software/make/manual/make.html#Include">include directive</a>
to bring those rules into the current make file.</p>
<p>This is the general approach Mozilla takes. All of the generic rules are
defined in the (oh-it-hurts-my-eyes-to-look-at-so-much-make-syntax)
<a href="https://hg.mozilla.org/mozilla-central/file/default/config/rules.mk">rules.mk</a>
file. Individual Makefiles in the tree simply define specifically-named
variables such as <em>CPPSRCS</em> or <em>XPIDLSRCS</em> and then include <em>rules.mk</em>.
When make executes, <em>rules.mk</em> transforms these variables into
rules which are automagically inserted into the DAG.</p>
<p>From a user perspective, this is splendid. You simply define some variables
and magic ensues. Instead of hairy looking make files, you literally
have a file with simple variable assignments. If all goes to plan, you
don't need to look at these rules definitions. Instead, you leave that
up to professional dragon trainers, such as Ted Mielczarek, Joey
Armstrong, and Kyle Huey.</p>
<p>A large problem is that many make files define custom rules - things not
using the magic from rules.mk. What's worse is that these custom rules
are often <em>cargo culted</em> around. They work, yes, but there is redundancy.
The installation of mochitest files is a perfect example of this.
<a href="https://bugzilla.mozilla.org/show_bug.cgi?id=370750">Bug 370750</a>)
tracks establishing a rule for this
<a href="https://mxr.mozilla.org/mozilla-central/search?string=TEST_FILES">absurd repetition</a>
of make logic in Mozilla's tree. Even in the case of mochitest rules
where the recipe is simple (it's just calling <em>$(INSTALL)</em>), the lack of
a unified rule hurts us. If we wanted to change the directory the
mochitest files were installed to, this would require editing scores of
files. Not cool.</p>
<p>Anyway, the first step towards improving the Mozilla build system (and
any make file based build system really) is what I'm calling the <strong>no
rules initiative</strong>.</p>
<p>We should strive for no explicit rules in non-shared make files
(things outside of <em>config/</em> and <em>build/</em> in the Mozilla source tree -
basically all the files named <em>Makefile.in</em> or <em>Makefile</em>).
Instead, individual make files should define variables that cause rules
from a shared make file (like <em>rules.mk</em>) to be applied.</p>
<p>In Mozilla's case, this will be a lot of manual effort and it will seem
like there is no immediate gain. The benefits will be explained in
subsequent sections.</p>
<h2>Part 2: Eliminate Make File Content Not Related to Building</h2>
<p>Mozilla's make files are riddled with content that isn't directly
related to the action of building Mozilla applications. Not only does
this contribute to the overhead of invoking make (these bits need to be
evaluated when parsing, added to the DAG, etc, adding cost), but they
also make the make files harder to read and thus edit.</p>
<p><a href="https://hg.mozilla.org/mozilla-central/file/default/testing/testsuite-targets.mk">testsuite-targets.mk</a>
is a good example of this. While it isn't included in every make file
(just the top-level one), the code isn't related to building at
all! Instead, it is essentially proxy code that maps make targets to
commands. It is supposed to be convenient: type |make xpcshell-tests|
and some tests run. OK, fine. I'll give you that.
The main problem is this make file is just a glorified shell script.
Make is great at constructing a dependency graph. As a DSL for shell
scripts, I'd rather just write a script. Therefore, I would argue code
like this belongs elsewhere - not in make files.</p>
<p>Anyway, a problem for Mozilla is that our make files are riddled with
pieces of content that aren't directly related to <em>building</em>. And, build
system maintainers pay the price. Every time you see some part of a make
file that doesn't appear to be directly related to the act of building,
your mind is like, "wut ist das?" (That's High German for <em>WTF?</em>) You
start asking questions. What's it for. Who uses it? Is it really needed?
What happens if it breaks? How will I know? These questions all need
answered and that adds overhead to editing these files. Oftentimes these
little one-off gems are undocumented and the questions are nearly
impossible to answer easily. The wise course of action is usually
<em>preserve existing behavior</em>. Nobody wants to melt someone else's
precious snowflake, after all. Thus, cargo cult programming prevails.</p>
<p>The solution I propose is along the same vein as the <em>no rules
initiative</em> - you can actually think of it as a specifically-tailored
subset: <em>limit the make files to what's necessary for building.</em> If it
doesn't relate to the act of building the application - if it is an
island in the DAG - remove it.</p>
<p>But where do you move it to? Good question. We don't have an obvious
location today. But, in
<a href="https://bugzilla.mozilla.org/show_bug.cgi?id=751795">bug 751795</a> I am
creating what is essentially a frontend for the build system. That is
where it should go. (I will publish more information about that project
on this blog once it has landed.)</p>
<h2>Part 3: Make File Style Conventions and Enforced Review Requirements</h2>
<p>My first two points revolved around what is effectively a make file
style convention. If we are serious about making that convention stick,
we need to enforce it better.</p>
<p>This can be done in two ways.</p>
<p>First, we establish a review policy that any changes to make files must
be signed off by a build system peer. Maybe we carve out an exception
for simple cases, such as adding a file to a <em>XPIDLSRCS</em> variable. I
don't know. We certainly don't allow people to add new make files or
even targets without explicit sign-off. I <em>think</em> this is how it is
supposed to be today. It isn't enforced very well if it is.</p>
<p>To aid in enforcing the style convention, we arm people with a tool that
checks the style so they can fix things before it lands on a build
peer's plate. This is actually pretty trivial to implement (I even
have code that does this). The hard part is coming to consensus on the
rules. If there is a will, I can make this happen easily.</p>
<p>Once you have the tool to check for convention/style violations, your
checkin policy is amended to include: <em>no make file changes can be checked
in unless the file passes the style checker.</em> This is sure to draw ire
from some, I'm sure. But, the alternative is having a cat and mouse
game between the build system maintainers making the build system suck
less and other people undoing their work with new features, etc.</p>
<p>Anyway, these rules wouldn't be absolute - there could always be
exceptions, of course. If we're serious about making the build system
better, we need it to be more consistent and have less variance. I'm
open to other suggestions to accomplish this.</p>
<h2>Part 4: Extracting Data from Make Files</h2>
<p>Once we have make files which consist of simple variable assignments (not
rules), it is possible to take the values of these variables, and extract
them from the make files to some higher-level system. For
example, we could scour all the make files for the <em>CPPSRCS</em> variable
and assemble a unified list of all C++ source files defined by that
variable across the entire build system. Then, we could do some really
interesting things.</p>
<p>What kind of things you ask? Well, one idea is you could write out a
monolithic make file that explicitly listed every target from the
extracted data. No recursive make necessary here! Another idea would
be to produce Visual Studio project files.</p>
<p>Think I'm crazy and this is impossible?
<a href="https://raw.github.com/gist/1363034/b3d956a66e049e7624fbdb79762551202fe9a465/optimized.mk">Feast your eyes</a>.
That make file was generated by parsing the existing Firefox make files,
extracting statically-defined data, then transforming that data
structure back into a single make file. I also have code somewhere that
produces Visual Studio projects.</p>
<p>Now, all of this is just a proof-of-concept. It works just well enough
for me to prove it is feasible. (The linked example is 7 months old and
won't work on the current tree.) Although, at one time, I did have have a
functioning transformation of essentially the <em>make export</em> stage. All
include files and IDL generation was in a single make file. It ran in a
fraction of the time as recursive make. A no-op execution of the
monolithic make file took about 0.5 seconds and - <em>gasp</em> - actually did
no work (unlike the no-op builds in the Mozilla tree today which
actually run many commands, even though nothing changed).</p>
<p>My point is that by having your make files - your build system - be
statically defined, you can extract and combine pieces of information.
Then, with relative ease, you can transform it to something else. Instead
of make files/build systems being a DSL for shell script execution, they
are data structures. And, you can do a lot more with data structures
(including generating shell scripts). Representing a build system as a
unified, constant data structure is what I consider to be the <em>holy grail</em>
of build systems. If you can do this, all you need to do is transform a
data structure to a format some other build tool can read and you are
set. If you do a good job designing your data structure, what you've
essentially built is an intermediate language (IR) that can be
compiled to different build tools (like make, Ninja, and Visual Studio).</p>
<p>(The previous paragraph is important. You may want to read it again.)</p>
<p>Getting back on point, our goal is to assemble aggregate data for
Mozilla's build system so we can construct a monolithic DAG. We can work
towards this by extracting variable values from our make files.</p>
<p>Unfortunately, there are technical problems with naive data extraction
from make files. Fortunately, I have solutions.</p>
<p>The first problem is getting at individual variable values in make
files. Simple parsing is ruled out, as make files can be more complex
than you realize (all those pesky built-in functions). You don't want
to burden people to add targets to print variable values and then
execute make just to print variables. This would be violating steps
1 and 2 from above! So, you somehow need to inject yourself into a
make execution context.</p>
<p>Fortunately, we have <a href="http://benjamin.smedbergs.us/pymake/">pymake</a>.
Pymake is a mostly feature complete implementation of GNU make written
in Python. You should be able to swap out GNU make for pymake and things
just work. If they don't, there is a bug in pymake.</p>
<p>pymake is useful because it provides an API for manipulating make and
make files. Contrast this was GNU make, which is essentially a black
box: make file goes in, process invocations come out. There's no really
good way to inspect things. The closest is the <em>-p</em> argument, which
dumps some metadata of the parsed file. But, this would require you to
parse that output. As they say, <em>now you have two problems</em>.</p>
<p>Using pymake, it's possible to get at the raw parser output and to poke
around the assembled data structures before the make file is evaluated.
This opens up some interesting possibilities.</p>
<p>With pymake's API, you can query for the value of a variable, such as
<em>XPIDLSRCS</em>. So, you just need to feed every make file into pymake,
query for the value of a variable, then do cool things with the
extracted data.</p>
<p>Not so fast.</p>
<p>The problem with simple extraction of variables from make files is that
there is no guarantee of idempotence. You see, make files aren't static
structures. You don't parse a make file into a read-only structure,
build the DAG, and go. Oh no, because of how make files work, you have
to constantly re-evaluate things, possibly modifying the DAG in the
process.</p>
<p>When you obtain the value of a variable in make, you need to evaluate it.
In make parlance, this is called <em>expansion</em>.</p>
<p>Expansion is easy when variables have simple string content. e.g.</p>
<pre><code>CPPSRCS = foo.cpp bar.cpp
</code></pre>
<p>It gets more complicated when they have references to other variables:</p>
<pre><code>FILES = foo.cpp bar.cpp
CPPSRCS = $(FILES) other.cpp
</code></pre>
<p>Here the expansion of <em>CPPSRCS</em> must descend into <em>FILES</em> and expand
that. If <em>FILES</em> contained a reference, make would descend into that.
And so on.</p>
<p>The problem of non-guaranteed idempotence is introduced when variable
expansion interfaces with something that is non-deterministic, such as
the file system. This almost always involve a call to one of make's
built-in functions. For example:</p>
<pre><code>CPPSRCS = $(wildcard *.cpp)
SOURCES = $(shell find . -type f -name '*.cpp')
</code></pre>
<p>Here, the expanded value of <em>CPPSRCS</em> depends on the state of the
filesystem at the time of expansion. This is obviously not
deterministic. Since you can't guarantee the results of that expansion,
doing something with the evaluated value (such as generating a new
make file) is dangerous.</p>
<p>It gets worse.</p>
<p>The expansion of a variable can occur multiple times during the execution
of a make file due to deferred evaluation. When you use the <em>=</em> operator
in make files, the variable isn't actually expanded until it is accessed
(make just stores a reference to the string literal on the right
side of the assignment operator). Furthermore, the expansion occurs <em>every
time</em> the variable is accessed. Seriously.</p>
<p>The <em>:=</em> operator, however, expands the variable once - at assignment time.
Instead of storing a string reference, make evaluates that string
immediately and assigns the result to the variable.</p>
<p>The distinction is important and can have significant implications. Use
of <em>=</em> can lead to degraded performance via redundant work during multiple
expansions (e.g. file system access or shell invocation). It can also
cause the value of a variable to change during the course of execution
from changes to systems not directly under the make file's control (such
as the file system). For these reasons, <strong>I recommend to use the
immediate assignment operator (:=) instead of the deferred assignment
operator (=) unless you absolutely need deferred assignment</strong>. This is
because immediate assignment approximates what most think assignment
should be. Unfortunately, since <em>=</em> is the only assignment operator in the
overwhelming majority of popular programming languages, most people don't
even know that other assignment operators exist or that the deferred
assignment operator comes with baggage. Now you do. I hope you use it
properly.</p>
<p>Anyway, if a variable's value changes while a make file is executing, what
is the appropriate value to extract for external uses? In my opinion,
the safe answer is there is none: don't extract the value of that
variable. If you do, you are asking for trouble.</p>
<p>It sounds like I just tore a giant hole in my idea to extract simple
data from make files since I just said that the value may not be
deterministic. Correct. But, the key word here is <em>may</em>. Again, pymake
comes to the rescue.</p>
<p>Using pymake, I was able to implement a prover that guarantees whether a
variable is deterministic and thus idempotent. Basically, it examines
the statement list generated by pymake's parser. It traces the
assignment of a variable through the course of a statement list. If the
variable itself, any variable referenced inside of it, or any variable
that impacts the assignment (such as assignment inside a condition block)
is tainted by a non-deterministic evaluation (notably file system querying
and shell calls), that variable is marked as non-deterministic. Using
this prover, we can identify which variables are guaranteed to be
idempotent as long as the make file doesn't change. If you want to learn
more, see the
<a href="https://raw.github.com/indygreg/mozilla-central/build-splendid/build/buildsplendid/makefile.py">code</a>.</p>
<p>Now that we have a way of proving that a variable in a make file is
deterministic as long as the source make file doesn't change, we can
extract data from make files with confidence, without having to worry
about side-effects during execution of the make file. Essentially, the
only thing we need to monitor is the original make file and any make
files included by it. As long as they don't change, our static analysis
holds.</p>
<p>So, I've solved the technical problem of extracting data from make
files. This allows us to emancipate build system data into a constant
data structure. As mentioned above, this opens up a number of
possibilities.</p>
<p>It's worth noting that we can combine the prover with the style
enforcer from part 3 to help ensure Mozilla's make files are statically
defined. Over time, the make files will not only look like simple
variable assignments (from part 1), but will also become static data
structures. <strong>This will enable the Mozilla build system to be
represented as a single, constant data structure.</strong> This opens up the
transformation possibilities described above. It also allows for more
radical changes, such as replacing the make files with something else.
More on that later.</p>
<h2>Part 5: Rewriting Make Files</h2>
<p>Extracting data from make files to produce new build system data
(possibly a unified make file) introduces a new problem: redundancy
and/or fragmentation. You now have the original definition sitting
around in the original make file and an optimized version in a derived
file. What happens when the original file (still containing the original
values) is evaluated? Do you want the targets (now defined elsewhere from
the extracted data) to still work when executed from the original file?
(Probably not.)</p>
<p>There is also a concern for how to handle the partially converted build
system scenario. In theory, porting the build system to the <em>new world
order</em> would go something like this:</p>
<p>1) Identify a variable/feature you want to extract from the make files
2) Write routine to convert extracted data into an optimized builder
(maybe it just prints out a new make file)
3) For make files where that variable is defined but couldn't be safely
extracted (because it wasn't determinant), continue to execute the old,
inherited rule from <em>rules.mk</em> (as we do today).</p>
<p>This acknowledges the reality that any particular transition of a
variable to use an optimized build routine from extracted values will
likely be difficult to do atomically. The flag day will be held up by
stragglers. It would be much more convenient to accomplish the
transition incrementally. Files that are ready see the benefit today,
not only after everyone else is ready.</p>
<p>My solution to this problem is make file rewriting. For variables that
are deterministic and have their functionality replaced by some other
system, we simply strip out references to these variables from the make
files. No variable. No inherited rule. No redundancy. No extra burden.
Of course, we're not altering the <em>original</em> make file. Instead,
we take the input make file (<em>Makefile.in</em> in the Mozilla tree), do
static analysis, remove variables handled elsewhere, then write out a
new make file.</p>
<p>Unfortunately, this invented a new problem: rewriting make files. As far
as I can tell, nobody has done this before. At least not to the degree
or robustness that I have (previous solutions are effectively sed +
grep).</p>
<p>Using pymake, I was able to create an API sitting between the
parser and evaluator which allows high-level manipulation of the make
file data structure. Want to delete a variable? Go for it. Want to delete
a target? Sure! It's really the missing API from pymake (or any make
implementation for that matter). I stopped short of writing a
fully-featured API (things like adding rules, etc). Someday I would love
to fill that in because then you could create new/anonymous make
files/DAGS using just API calls. You would thus have a generic API for
creating and evaluating a DAG using make's semantics. This is
potentially very useful. For example, the optimized make files
generated from static data extracted from other make files I've been
talking about could be produced with this API. In my opinion, this
would be much cleaner than printing make directives by hand (which is
how I'm doing it today).</p>
<p>This new make file manipulation API was relatively easy to write. The hard
part was actually formatting the statement list back into a make file
that was both valid <strong>and</strong> functionally equivalent to the original.
(There's actually a lot of dark corners involving escaping, etc.) But, I
eventually slayed this dragon. And, the best part is I can prove it is
correct by feeding the output into the pymake parser and verifying the
two statement lists are identical! Furthermore, after the first
iteration (which doesn't preserve formatting of the original make file
because a) it doesn't need to and b) it would be a <em>lot</em> of effort) I can
even compare the generated string content of the make files as an added
sanity check.</p>
<p>If you are interested, the code for this lives alongside the
deterministic proving code linked to in the previous section. I would
like to land it as part of pymake someday. We'll see how receptive the
pymake maintainers are to my crazy ideas.</p>
<p>Anyway, we can now strip extracted variables handled by more efficient
build mechanisms out of generated make files. That's cool. It gives us an
incremental transition path for Mozilla's build system that bridges new
and old.</p>
<p>But wait - there's more!</p>
<p>That deterministic prover I wrote for data extraction: I can actually
use it for rewriting make files!</p>
<p>Make files support
<a href="https://www.gnu.org/software/make/manual/make.html#Conditionals">conditional directives</a>
such as <em>ifeq</em>, <em>ifneq</em>, <em>ifdef</em>, <em>else</em>, etc. For each conditional
directive, I can query the deterministic prover and ask whether an
expansion of that directive is deterministic. If it is, I evaluate the
conditional directive and determine which branch to take. And, I may
have invented the first make file optimizer!</p>
<p>I simply replace the condition block (and all the branches therein) with
the statements constituting the branch that is guaranteed to be
executed.</p>
<p>As a contrived example:</p>
<pre><code>DO_FOO := 1

ifeq ($(DO_FOO), 1)
foo.o: foo.cpp
    clang++ -o foo.o foo.cpp
endif
</code></pre>
<p>We can prove the <em>ifeq</em> branch will always be taken. So, using the
deterministic prover, we can rewrite this as:</p>
<pre><code>DO_FOO := 1

foo.o: foo.cpp
    clang++ -o foo.o foo.cpp
</code></pre>
<p>Actually, we can now see that the value of <em>DO_FOO</em> is not referenced
and is free from side-effects (like a shell call), and can eliminate it!</p>
<pre><code>foo.o: foo.cpp
    clang++ -o foo.o foo.cpp
</code></pre>
<p>Cool!</p>
<p>The practical implication of this is that it is shifting the burden of
make file evaluation from once every time you run make to <em>some time
before</em>. Put another way, we are reducing the amount of work make performs
when evaluating a make file. As long as the original file and any files
included by it don't change frequently, the cost of the static analysis
and rewriting is repaid by simpler make files and the lower run-time
cost they incur. This should translate to speedier make invocations.
(Although, I have no data to prove this.)</p>
<p>Another nice side-effect is that the rewritten and reduced make file is
easy to read. You know exactly what is going to happen without having to
perform the evaluation of (constant) conditional statements in your head
(which likely involves consulting other files, like <em>autoconf.mk</em> in the
case of Mozilla's make files). My subjective feeling is that this makes
generated make files much easier to understand.</p>
<p>So, not only does rewriting make files allow us to incrementally
transition to a build system where work is offloaded to a more optimized
execution model, but it also makes the evaluated make files smaller,
easier to understand, and hopefully faster. As Charlie Sheen would say:
winning!</p>
<h2>Review Through Part 5</h2>
<p>At this point, I've laid out effectively phase 1 of making Mozilla's
build system suck much less. We now have make files that are:</p>
<ul>
<li>Reusing rule logic from rules.mk to do everything (specially-named
  variables cause magic to ensure). No cargo culted rules.</li>
<li>Devoid of cruft from targets not related to building the application.</li>
<li>Variables defined such that they are deterministic (essentially no
  shell and filesystem calls in make files)</li>
<li>A review system in place to ensure we don't deviate from the above</li>
<li>A higher-level tool to extract specific variables from make files
  which also produces an optimized, non-recursive make file to evaluate
  those targets. Ideally, this encompasses all rules inherited from
  rules.mk and the need for recursion and to evaluate individual make
  files is eliminated.</li>
</ul>
<p>If we wanted, we could stop here. This is about the limit we can take
the existing build system while maintaining some resemblence to the
current architecture.</p>
<p>But, I'm not done.</p>
<h2>Part 6: Transition Away from Make Files</h2>
<p>If we've implemented the previous parts of this post, it is only
natural for us to shift away from make files.</p>
<p>If all we have in our make files are variable assignments, why are we
using make files to define a static document? There are much better
formats for static data representation. YAML and JSON come to mind.</p>
<p>If we transition the build system definition to something actually
declarative - not just something we shoehorn into being declarative
(because that is how you make it fast and maintainable) - that cuts out
a lot of the hacky-feeling middleware described above (static analysis,
data extraction, and rewriting). It makes parsing simpler (no need for
pymake). It also takes away a foot gun (non-deterministic make
statements). Still not convinced? We could actually properly represent
an array (make doesn't have arrays, just strings split on whitespace)!
OK, just kidding - the array thing isn't that big of a deal.</p>
<p>Anyway, the important characteristic of the build system is achieving
that <em>holy grail</em> where the entire thing can be represented as a single
generic (and preferably constant) data structure. As long as you can
arrive at that (a state that can be transformed into something else),
it doesn't matter the road you take. But, some roads are certainly
cleaner than others.</p>
<p>At the point where you are transitioning away from make files, I would
argue the correct thing to do is convert the build definition to files
understood by another build tool. Or, we should at least use a format
that is easily loaded into another tool. If we don't, we've just invented
a custom one-off build system. A project the size of Mozilla could
probably justify it. But, I think the right thing to do is find an open
source solution that fits the bill and run with that.</p>
<p>While I'm far from an expert on it, I'll throw out
<a href="https://code.google.com/p/gyp/">GYP</a> as a candidate. GYP defines the
build system in JSON files (which can actually be directly evaluated in
a Python interpreter). GYP consumes all of the individual .gyp/JSON
files and assembles an aggregate representation of the <em>entire</em> build
system. You then feed that representation into a generator, which
produces make files, Ninja files, or even Visual Studio or Xcode
project files. Sound familiar? It's essentially my <em>holy grail</em> where
you transform a data structure into something consumed by a specific
tool. I approve.</p>
<p>Now, GYP is not perfect. No build system is perfect. I will say
that GYP is good enough to build Chromium. And, if it can handle
Chromium, surely it can handle Firefox.</p>
<h2>Next Steps for Mozilla</h2>
<p>I'm not sure what the next steps are. I've been trying to convince
people for a while that a data-centric declarative build system is
optimal and we should be working towards it. Specifically, we should be
working towards a monolithic DAG, not separate DAGs in separate make
files. I think people generally agree with me here. We obviously have a
build system today that is very declarative (with variables turning into
rules). And, I know people like Joey Armstrong understand why recursive
make is so slow and are working on ways to switch away from it.</p>
<p>I think the main problem is people are overwhelmed by what a transition
plan would look like. There is a lot of crap in our make files and
fixing it all in one go is next to impossible. Nobody wants to do it
this way. And, I think the sheer scope of the overall problem along with
a constant stream of new features (Fennec make files, ARM support,
Windows 8, etc) has relegated people to merely chipping away at the
fringe.</p>
<p>Supplementing that, I think there has been doubt over some of what I've
proposed, specifically around the scarier-sounding bits like performing
static analysis and rewriting of make files. It scared me when I first
realized it was possible, too. But, it's possible. It's real. I have
code that does it. I'll admit the code is far from perfect. But, I've
done enough that I've convinced my self it is possible. It just needs a
little polish and everything I described above is a reality. We just
need to commit to it. Let's do it.</p>
<h2>Loose Ends</h2>
<h3>What About "Sand-boxed" Make or Concatenating Make Files?</h3>
<p>I anticipate that the main <em>competition</em> for my proposal will involve
the following approaches:</p>
<ol>
<li>"sand-boxed" make files (described in the aforementioned <em>Recursive
   Make Considered Harmful</em> paper)</li>
<li>Concatenate all the make files together</li>
</ol>
<p>The concatenating one is definitely a non-controversial alternative to
my proposal of extracting data then creating derived build files
(possibly a monolithic make file) from it. And, if we go with
concatenating, I'll generally be happy. We'll have a mostly
statically-defined build system and it will exist in one monolithic DAG.
This has desirable attributes.</p>
<p>FWIW, the stalled
<a href="https://bugzilla.mozilla.org/show_bug.cgi?id=623617">bug 623617</a> has an
amazing patch from Joey Armstrong which partially implements concatenated
make files. If this lands, I will be extremely happy because it will make
the build system much faster and thus make developers' lives less
painful.</p>
<p>That being said, I do have a complaint against the concatenating
approach: it is still defined in make files.</p>
<p>The problem isn't make itself: it's that at the point you have pulled off
concatenating everything together, your build system is effectively static
data declarations. Why not use something like GYP instead? That way, you
get Ninja, Visual Studio, Xcode, Eclipse, or even make file generation
<strong>for free</strong>. I think it would be silly to ignore this opportunity.</p>
<p>If we continue to define everything in make files, we'll have to rely on
data extraction from the concatenated make file to create project files for
Visual Studio, Xcode, etc. This is possible (as I've proved above). And,
I will gladly lend my time to implementing solutions (I've already
performed a lot of work to get Visual Studio project generation working
in this manner, but I abandoned work because I reached the long tail that
is one-off targets and redundancy - which would be eliminated if we
accomplish parts 1 and 2). Furthermore, we may have to reinvent the
wheel of generating these project files (believe me, generating Visual
Studio projects is not fun). Or, we could find a way to extract
the make file data into something like GYP and have it do the generation
for us. But, if we are going to end up in GYP (or similar), why don't we
just start there?</p>
<p>I will also throw out that if we are really intent on preserving our
existing make rules, it would be possible to define the build system in
GYP/other files and write a custom GYP/other generator which
produced make files tailored for our existing make rules.</p>
<p>In summary, the crux of my argument against concatenated make files is
that it is a lot of work and that for roughly the same effort you could
produce something else which has almost identical properties but has
more features (which will make the end-developer experience better).</p>
<p>You could say the additional complexity of my proposal is just to buy us
Visual Studio, etc support. That is more or less accurate. I think I've
put in more time than anyone to devise a solution that programmatically
generates Visual Studio projects. (Manually tailored files out out of
the question because the maintenance of keeping them in sync with build
system changes would be too high and error prone.) From my perspective,
if we want to provide Visual Studio, Xcode, etc files for Mozilla
projects - if we want to provide developers access to amazing tools so
they can be more productive - we need the build system defined in
something <em>not make</em> so this is robust.</p>
<h3>Eclipse Project Files</h3>
<p>Jonathan Watt recently
<a href="https://jwatt.org/blog/2012/06/10/the-new-eclipse-cdt-and-mozilla-development">posted instructions</a>
on configuring Eclipse CDT to build Firefox. Awesome!</p>
<p>Some may say that debunks my theory that it is difficult/impossible to
generate build project files from just make files (without the sorcery I
describe above). Perhaps.</p>
<p>As cool as the Eclipse CDT integration is, the way it works is a giant
hack. And, I don't mean to belittle Jonathan or his work here - it is
very much appreciated and a large step in the right direction.</p>
<p>The Eclipse CDT integration works by scanning the output of make,
looking for compiler invocations. It parses these out, converting the
arguments to Eclipse project data. Not only do you have to perform a
standard make to configure Eclipse, but it also has to run without any
parallelization so as to not confuse the parser (interleaved output,
etc). And, Eclipse doesn't do everything for you - you still have to
manage part of your build from the command line.</p>
<p>So, Eclipse is really a hybrid solution - albeit a rather useful one
and, importantly, one that seems to work. Yet, In my opinion, this is
all a little fragile and less than ideal. I want the ability to build
the whole thing from your build tool. This <em>is</em> possible with the
solutions I've proposed above.</p>
<h3>Improving GYP</h3>
<p>Above, I recommended GYP as a build system which I think is a good
example of my <em>holy grail</em> build system - one which represents
everything as data and allows transformations to multiple build tools.</p>
<p>While I like the concept of GYP, there are parts of the implementation I
don't like. Every build system inevitably needs to deal with
conditionals. e.g. <em>if on Windows, use this flag; if on Linux, use this
one.</em> GYP represents conditions as specially crafted property names in
JSON objects. For example:</p>
<pre><code>{
  'conditions': [
    ['OS=="linux"', {
      'targets': [
        {
          'target_name': 'linux_target'
        },
      ],
    }],
    ['OS=="win"', {
      'targets': [
        {
          'target_name': 'windows_target',
        },
      ],
    }]
  ]
}
</code></pre>
<p>That special string as a key name really rubs me the wrong way. You are
using declarative data to represent procedural flow. Yuck. It reminds me
of a horror from a previous life - VXML. Don't ever try to write a
computer program complete with branching and functions in XML: you will
just hate yourself.</p>
<p>Anyway, despite the conflation of logic and data, it isn't too bad. As
long as you limit the expressions that can be performed (or that
actually are performed - I think I read they just feed it into eval),
it's still readable. And, I'll happily sacrifice some purity to achieve
the <em>holy grail</em>.</p>
<p>Anyway, I think my optimal build system would look a lot like GYP except
that the data definition language would be a little more powerful.
Knowing that you will inevitably need to tackle conditionals and other
simple logic, I would give up and give my build system files access to a
real programming language. But, it would need to be a special language.
One that doesn't allow foot guns (so you can preserve determinism) and
one that can't be abused to allow people to stray too far from a
data-first model.</p>
<p>For this task, I would employ the <a href="http://www.lua.org/">Lua</a>
programming language. I would use Lua because it is designed to be an
embeddable and easily sandboxed programming language. These attributes
are perfect for what I'd need it to do.</p>
<p>Basically, files defining the build system would be Lua scripts. Some
driver program would discover all these Lua files. It would create a new
Lua context (an instance of the Lua interpreter) for each file. The
context's global registry (namespace) would be populated
with variables that defined the existing build
configuration.  <em>IS_WINDOWS</em>, <em>HAVE_STRFTIME</em>, etc - the kind of stuff
<em>exported</em> by configure. The Lua script would get executed in that
context. It would do stuff. It would either set specially-named global
variables to certain values or it would call functions installed in the
global registry like <em>add_library</em>, etc. Then, the driving program would
collect everything the Lua script did and merge that into a data
structure representing the build system, achieving the <em>holy grail</em>.</p>
<p>In the simple case, build files/Lua scripts would just look like simple
variable assignments:</p>
<pre><code>EXPORTED_INCLUDES = {"foo.h", "bar.h"]
LIBRARY_NAME = "hello"
</code></pre>
<p>If the build system needed to do something conditional, you would have a
real programming language to fall back on:</p>
<pre><code>if IS_WINDOWS then
    SOURCE_FILES = "windows.cpp"
else
    SOURCE_FILES = "other.cpp"
end
</code></pre>
<p>To my eye, this is cleaner than hacking JSON while still readable.</p>
<p>For people not familiar with Lua, new context instances are cheap, tiny,
and have almost no features. Even the standard library must be
explicitly added to a context! Contrast this with a batteries-included
language like Python or JavaScript. So, while the build definition files
would have a fully-featured programming language backing them, they
would be severely crippled. I'm thinking I would load the <em>string</em> and
<em>table</em> libraries from the <a href="http://www.lua.org/manual/5.2/manual.html#6">standard library</a>.
Maybe I'd throw the <em>math</em> one in too. If I did, I'd disable the random
function, because that's not deterministic!</p>
<p>Anyway, I think that would get me to a happy place. Maybe I'll get bored
on a weekend and implement it! I'm thinking I'll have a Python program
drive Lua through the ctypes module (if someone hasn't written a binding
already - I'm sure they have). Python will collect data from the Lua
scripts and then translate that to GYP data structures. If someone beats
me to it, please name it <em>Lancelot</em> and leave a comment linking to the
source.</p>
<h3>Using Code for Other Projects</h3>
<p>The code I wrote for pymake can be applied to any make file, not just
Mozilla's. I hope to have some of it integrated with the official pymake
repository some day. If this interests you, please drop me a line in the
comments and I'll see what I can do.</p>

  </div>
</div>



  <div class="after_post"><a href="http://gregoryszorc.com/blog/2012/06/25/improving-mozilla's-build-system#disqus_thread">Read and Post Comments</a></div>
  <hr class="interblog" />

              </div>
              
          <div id="sidebar">
          <ul>
            <li>
              <h2>Categories</h2>
              <ul>
                <li><a href="/blog/category/clang">Clang</a></li>
                <li><a href="/blog/category/firefox">Firefox</a></li>
                <li><a href="/blog/category/mozilla">Mozilla</a></li>
                <li><a href="/blog/category/python">Python</a></li>
                <li><a href="/blog/category/sync">Sync</a></li>
                <li><a href="/blog/category/browsers">browsers</a></li>
                <li><a href="/blog/category/build-system">build system</a></li>
                <li><a href="/blog/category/compilers">compilers</a></li>
                <li><a href="/blog/category/internet">internet</a></li>
                <li><a href="/blog/category/make">make</a></li>
                <li><a href="/blog/category/misc">misc</a></li>
                <li><a href="/blog/category/movies">movies</a></li>
                <li><a href="/blog/category/pymake">pymake</a></li>
                <li><a href="/blog/category/security">security</a></li>
                <li><a href="/blog/category/testing">testing</a></li>
              </ul>
            </li>
          </ul>
        </div>



              <div style="clear: both;">&nbsp;</div>
          </div>
        </div>
      </div>
      <div id="footer">
        
  <hr/>
  <p>Copyright (c) 2012 Gregory Szorc. All rights reserved. Design by <a href="http://www.freecsstemplates.org/"> CSS Templates</a>.</p>


      </div>
    </div>
  </body>
</html>






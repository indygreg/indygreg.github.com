<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0"
     xmlns:content="http://purl.org/rss/1.0/modules/content/"
     xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
     xmlns:atom="http://www.w3.org/2005/Atom"
     xmlns:dc="http://purl.org/dc/elements/1.1/"
     xmlns:wfw="http://wellformedweb.org/CommentAPI/"
     >
  <channel>
    <title>Gregory Szorc's Digital Home</title>
    <link>http://gregoryszorc.com/blog</link>
    <description>Rambling on</description>
    <pubDate>Wed, 13 Jan 2016 23:25:40 GMT</pubDate>
    <generator>Blogofile</generator>
    <sy:updatePeriod>hourly</sy:updatePeriod>
    <sy:updateFrequency>1</sy:updateFrequency>
    <item>
      <title>Making MozReview Faster by Disregarding RESTful Design</title>
      <link>http://gregoryszorc.com/blog/2016/01/13/making-mozreview-faster-by-disregarding-restful-design</link>
      <pubDate>Wed, 13 Jan 2016 15:25:00 PST</pubDate>
      <category><![CDATA[MozReview]]></category>
      <category><![CDATA[Mozilla]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2016/01/13/making-mozreview-faster-by-disregarding-restful-design</guid>
      <description>Making MozReview Faster by Disregarding RESTful Design</description>
      <content:encoded><![CDATA[<p>When I first started writing web services, I was a huge RESTful fan
boy. The architectural properties - especially the parts related to
caching and scalability - really jived with me. But as I've grown
older and gained experienced, I've realized that RESTful design,
like many aspects of software engineering, is more of a guideline
or ideal than a panacea. This post is about one of those experiences.</p>
<p><a href="https://www.reviewboard.org/docs/manual/2.5/webapi/">Review Board's Web API</a>
is RESTful. It's actually one of the better examples of a RESTful API
I've seen. There is a very clear separation between <em>resources</em>. And
everything - and I mean everything - is a resource. <em>Hyperlinks</em> are
used for the purposes described in Roy T. Fielding's dissertation.
I can tell the people who authored this web API understood RESTful
design and they succeeded in transferring that knowledge to a web API.</p>
<p>Mozilla's <a href="https://reviewboard.mozilla.org">MozReview code review tool</a>
is built on top of Review Board. We've made a number of customizations.
The most significant is the ability to submit a series of commits
as one logical review series. This occurs as a side-effect of a
<em>hg push</em> to the code review repository. Once your changesets
are pushed to the remote repository, that server issues a number of
Review Board Web API HTTP requests to reviewboard.mozilla.org to
create the review requests, assign reviewers, etc. This is mostly all
built on the built-in web API endpoints offered by Review Board.</p>
<p>Because Review Board's Web API adheres to RESTful design principles
so well, turning a series of commits into a series of review requests
takes a lot of HTTP requests. For each commit, we have to perform
something like 5 HTTP requests to define the server state. For
series of say 10 commits (which aren't uncommon since we try to
encourage the use of microcommits), this can add up to dozens of
HTTP requests! And that's just counting the HTTP requests to
Review Board: because we've integrated Review Board with Bugzilla,
events like publishing result in additional RESTful HTTP requests from
Review Board to bugzilla.mozilla.org.</p>
<p>At the end of the day, submitting and publishing a series of 10
commits consumes somewhere between 75 and 100 HTTP requests! While
the servers are all in close physical proximity (read: low network
latencies), we are reusing TCP connections, and each HTTP request
completes faily quickly, the overhead adds up. <strong>It's not uncommon
for publishing commit series to take over 30s.</strong> This is unacceptable
to developers. We want them to publish commits for review as quickly
as possible so they can get on with their next task. Humans should not
have to wait on machines.</p>
<p>Over in <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1229468">bug 1220468</a>,
I implemented a new batch submit web API for Review Board and converted
the Mercurial server to call it instead of the classic, RESTful Review
Board web APIs. <strong>In other words, I threw away the RESTful properties
of the web API and implemented a monolith API doing exactly what we
need.</strong> The result is a drastic reduction in net HTTP requests. In our
tests, submitting a series of 20 commits for review reduced the HTTP
request count by 104! Furthermore, the new API endpoint performs
all modifications in a single database transaction. Before, each HTTP
request was independent and we had bugs where failures in the middle of
a HTTP request series left the server in inconsistent and unexpected
state. The new API is significantly faster and more atomic as a bonus.
The main reason the new implementation isn't yet nearly instantaneous
is because we're still performing several RESTful HTTP requests to
Bugzilla from Review Board. But there are plans for Bugzilla to
implement the batch APIs we need as well, so stay tuned.</p>
<p>(I don't want to blame the Review Board or Bugzilla maintainers for
their RESTful web APIs that are giving MozReview a bit of scaling pain.
MozReview is definitely abusing them almost certainly in ways that
weren't imagined when they were conceived. To their credit, the
maintainers of both products have recognized the limitations in their
APIs and are working to address them.)</p>
<p>As much as I still love the properties of RESTful design, there are
practical limitations and consequences such as what I described
above. The older and more experienced I get, the less patience I have
for tolerating architecturally pure implementations that sacrifice
important properties, such as ease of use and performance.</p>
<p>It's worth noting that many of the properties of RESTful design are
applicable to <em>microservices</em> as well. When you create a new service
in a microservices architecture, you are creating more overhead for
clients that need to speak to multiple services, making changes less
transactional and atomic, and making it difficult to consolidate
multiple related requests into a higher-level, simpler, and performant
API. I recommend
<a href="http://martinfowler.com/articles/microservice-trade-offs.html">Microservice Trade-Offs</a>
for more on this subject.</p>
<p>There is a place in the world for RESTful and microservice
architectures. And as someone who does a lot of server-side engineering,
I sympathize with wanting scalable, fault-tolerant architectures. But
like most complex problems, you need to be cognizant of trade-offs. It is
also important to not get too caught up with architectural purity if
it is getting in the way of delivering a simple, intuitive, and fast
product for your users. So, please, follow me down from the ivory tower.
The air was cleaner up there - but that was only because it was so
distant from the swamp at the base of the tower that surrounds every
software project.</p>]]></content:encoded>
    </item>
    <item>
      <title>Investing in the Firefox Build System in 2016</title>
      <link>http://gregoryszorc.com/blog/2016/01/11/investing-in-the-firefox-build-system-in-2016</link>
      <pubDate>Mon, 11 Jan 2016 14:20:00 PST</pubDate>
      <category><![CDATA[Mozilla]]></category>
      <category><![CDATA[build system]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2016/01/11/investing-in-the-firefox-build-system-in-2016</guid>
      <description>Investing in the Firefox Build System in 2016</description>
      <content:encoded><![CDATA[<p>Most of Mozilla gathered in Orlando in December for an all hands meeting.
If you attended any of the plenary sessions, you probably heard people
like David Bryant and Lawrence Mandel make references to improving the
Firefox build system and related tools. Well, the cat is out of the bag:
<strong>Mozilla will be investing heavily in the Firefox build system and related
tooling in 2016!</strong></p>
<p>In the past 4+ years, the Firefox build system has mostly been held
together and incrementally improved by a loose coalition of people who
cared. We had a period in 2013 where a number of people were making
significant updates (this is when moz.build files happened). But for the
past 1.5+ years, there hasn't really been a coordinated effort to
improve the build system - just a lot of one-off tasks and
(much-appreciated) individual heroics. This is changing.</p>
<p>Improving the build system is a high priority for Mozilla in 2016.
And investment has already begun. In Orlando, we had a marathon 3 hour
meeting planning work for Q1. At least 8 people have committed to
projects in Q1.</p>
<p>The focus of work is split between immediate short-term wins and
longer-term investments. <strong>We also decided to prioritize the Firefox and
Fennec developer workflows (over Gecko/Platform) as well as the
development experience on Windows.</strong> This is because these areas have
been under-loved and therefore have more potential for impact.</p>
<p>Here are the projects we're focusing on in Q1:</p>
<ul>
<li>Turnkey artifact based builds for Firefox and Fennec (download
  pre-built binaries so you don't have to spend 20 minutes compiling
  C++)</li>
<li>Running tests from the source directory (so you don't have to copy
  tens of thousands of files to the object directory)</li>
<li>Speed up configure / prototype a replacement</li>
<li>Telemetry for mach and the build system</li>
<li>NSPR, NSS, and (maybe) ICU build system rewrites</li>
<li><em>mach build faster</em> improvements</li>
<li>Improvements to build rules used for building binaries (enables
  non-make build backends)</li>
<li>mach command for analyzing C++ dependencies</li>
<li>Deploy automated testing for <em>mach bootstrap</em> on TaskCluster</li>
</ul>
<p>Work has already started on a number of these projects. I'm optimistic
2016 will be a watershed year for the Firefox build system and the
improvements will have a drastic positive impact on developer
productivity.</p>]]></content:encoded>
    </item>
    <item>
      <title>hg.mozilla.org replication updates</title>
      <link>http://gregoryszorc.com/blog/2016/01/05/hg.mozilla.org-replication-updates</link>
      <pubDate>Tue, 05 Jan 2016 15:00:00 PST</pubDate>
      <category><![CDATA[Mercurial]]></category>
      <category><![CDATA[Mozilla]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2016/01/05/hg.mozilla.org-replication-updates</guid>
      <description>hg.mozilla.org replication updates</description>
      <content:encoded><![CDATA[<p>A few minutes ago, I formally enabled a new replication system
for <a href="https://hg.mozilla.org/">hg.mozilla.org</a>. For the curious,
<a href="https://mozilla-version-control-tools.readthedocs.org/en/latest/hgmo/replication.html">technical details</a>
are available.</p>
<p>This impacts you because pushes to hg.mozilla.org should now be
significantly faster. For example, pushes to mozilla-inbound
that used to take 15s now take 2s. Pushes to Try that used to
take 45s now take 10s. (Yes, the old replication system really
added a lot of overhead.) Pushes to hg.mozilla.org are still not
as fast as they could be due to us running the service on 5 year
old hardware (we plan to buy new servers this year) and due to the
use of NFS on the server. However, I believe push latency is now
reasonable for every repo except Try.</p>
<p>The new replication system opens the door to a number of future
improvements. We'd like to stand up mirrors in multiple data
centers - perhaps even offices - so clients have the fastest
connectivity and so we have a better disaster recovery story.
The new replication system facilitates this.</p>
<p>The new replication log is effectively a unified pushlog -
something people have wanted for years. While there is not yet
a public API for it, one could potentially be exposed, perhaps
indirectly via Pulse.</p>
<p>It is now trivial for us to stand up new consumers of the
replication log that react to repository events merely milliseconds
after they occur. This should eventually result in downstream
systems like build automation and conversion to Git repos starting
and thus completing faster.</p>
<p>Finally, the new replication system has been running unofficially for
a few weeks, so you likely won't notice anything different today
(other than removal of some printed messages when you push). What
changed today is the new system is enabled by default and we have
no plans to continue supporting or operating the legacy system.
Good riddance.</p>]]></content:encoded>
    </item>
    <item>
      <title>Mozilla Mercurial Extension Updates</title>
      <link>http://gregoryszorc.com/blog/2015/12/16/mozilla-mercurial-extension-updates</link>
      <pubDate>Wed, 16 Dec 2015 17:40:00 PST</pubDate>
      <category><![CDATA[Mercurial]]></category>
      <category><![CDATA[Mozilla]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2015/12/16/mozilla-mercurial-extension-updates</guid>
      <description>Mozilla Mercurial Extension Updates</description>
      <content:encoded><![CDATA[<p>There have been a handful of updates to Mozilla's client-side
Mercurial tools in the past week and all users are encouraged
to pull down the latest commit of mozilla-central and run
<em>mach mercurial-setup</em> to ensure their configuration is up to
date.</p>
<p>Improvements to <em>mach mercurial-setup</em> include:</p>
<ul>
<li><em>~</em> are now used in paths when appropriate</li>
<li>Mercurial 3.5.2 is now the oldest version you can run without
  the wizard complaining</li>
<li>The clone bundles feature is enabled when running Mercurial 3.6</li>
<li><em>hg wip</em> is available for configuration</li>
<li><em>hgwatchman</em> (make <em>hg status</em> significantly faster via background
  filesystem watching) is now available on OS X</li>
<li>x509 host key fingerprints are no longer pinned if your Python
  and Mercurial version is modern (this pinning exists in Mercurial
  because old versions of Python don't verify x509 certificates
  properly)</li>
<li>3rd party Mercurial extensions are pulled with extensions disabled
  (to prevent issues with old, incompatible extensions crashing the
  <em>hg pull</em> invocation)</li>
</ul>
<p>The firefoxtree extension has also been updated. It now uses the
new <em>namespaces</em> feature of Mercurial so all Firefox labels/names/refs
are exposed in a <em>firefoxtree</em> namespace instead of as tags. As part
of this change, <strong>you will lose old tags created by this extension
and will need to re-pull repos to recreate them as namespaced
labels</strong>. <em>hg log</em> output will now look like the following:</p>
<pre><code>changeset:   332223:0babaa3edcf9
fxtree:      central
parent:      332188:40038a66525f
parent:      332222:c6fc9d77e86f
user:        Carsten "Tomcat" Book &lt;cbook@mozilla.com&gt;
date:        Wed Dec 16 12:01:46 2015 +0100
summary:     merge mozilla-inbound to mozilla-central a=merge
</code></pre>
<p>(Note the <em>fxtree</em> line.)</p>
<p>The firefoxtree extension also now works with <em>hg share</em>. i.e. if
you <em>hg share</em> from a Firefox repository and <em>hg pull</em> from
the source repo or any shared repo, the labels will be updated in
every repo. This only works on newly-created shares. If you want
to enable this on an existing share, see
<a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1115188#c31">this comment</a>.</p>]]></content:encoded>
    </item>
    <item>
      <title>hg.mozilla.org Updates</title>
      <link>http://gregoryszorc.com/blog/2015/11/05/hg.mozilla.org-updates</link>
      <pubDate>Thu, 05 Nov 2015 09:20:00 PST</pubDate>
      <category><![CDATA[Mozilla]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2015/11/05/hg.mozilla.org-updates</guid>
      <description>hg.mozilla.org Updates</description>
      <content:encoded><![CDATA[<p>A number of changes have been made to
<a href="https://hg.mozilla.org">hg.mozilla.org</a> over the past few days:</p>
<ul>
<li>
<p>Bookmarks are now replicated from master to mirrors properly (before,
  you could have seen <em>foo@default</em> bookmarks appearing). This means
  bookmarks now properly work on hg.mozilla.org!
  (<a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1139056">bug 1139056</a>).</p>
</li>
<li>
<p>Universally upgraded to Mercurial 3.5.2. Previously we were running
  3.5.1 on the SSH master server and 3.4.1 on the HTTP endpoints. Some
  HTML templates received minor changes.
  (<a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1200769">bug 1200769</a>).</p>
</li>
<li>
<p>Pushes from clients running Mercurial older than 3.5 will now see
  an advisory message encouraging them to upgrade.
  (<a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1221827">bug 1221827</a>).</p>
</li>
<li>
<p>Author/user fields are now validated to be a RFC 822-like value
  (e.g. "Joe Smith &lt;someone@example.com&gt;").
  (<a href="https://bugzilla.mozilla.org/show_bug.cgi?id=787620">bug 787620</a>).</p>
</li>
<li>
<p>We can now mark individual repositories as read-only.
  (<a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1183282">bug 1183282</a>).</p>
</li>
<li>
<p>We can now mark all repositories read-only (useful during maintenance
  events).
  (<a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1183282">bug 1183282</a>).</p>
</li>
<li>
<p>Pushlog commit list view only shows first line of commit message.
  (<a href="https://bugzilla.mozilla.org/show_bug.cgi?id=666750">bug 666750</a>).</p>
</li>
</ul>
<p>Please report any issues in their respective bugs. Or if it is an
emergency, #vcs on irc.mozilla.org.</p>]]></content:encoded>
    </item>
    <item>
      <title>Cloning Improvements in Mercurial 3.6</title>
      <link>http://gregoryszorc.com/blog/2015/10/22/cloning-improvements-in-mercurial-3.6</link>
      <pubDate>Thu, 22 Oct 2015 05:00:00 PDT</pubDate>
      <category><![CDATA[Mercurial]]></category>
      <category><![CDATA[Mozilla]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2015/10/22/cloning-improvements-in-mercurial-3.6</guid>
      <description>Cloning Improvements in Mercurial 3.6</description>
      <content:encoded><![CDATA[<p>Mercurial 3.6 (scheduled for release on or shortly after November 1)
contains a number of improvements to cloning. In this post, I will
describe a new feature to help server operators reduce load (while
enabling clients to clone faster) and some performance work to make
clone operations faster on the client.</p>
<p>Cloning repositories can incur a lot of load on servers. For
<a href="https://hg.mozilla.org/mozilla-central">mozilla-central</a> (the main
Firefox repository), clones require the server to spend 4+ minutes CPU
time and send ~1,230 MB over the network. Multiply by thousands of
clients from build and test automation and developers, and you could
quickly finding yourself running out of CPU cores or network bandwidth.
Scaling Mercurial servers (like many services) can therefore be
challenging. (It's worth noting that Git is in the same boat for
reasons technically similar to Mercurial's.)</p>
<p>Mozilla previously implemented a Mercurial extension to
<a href="/blog/2015/05/29/faster-cloning-from-hg.mozilla.org-with-server-provided-bundles/">seed clones from pre-generated bundle files</a>
so the Mercurial servers themselves don't have to work very hard for
an individual clone. (That linked post goes into the technical
reasons why cloning is expensive). We now offload cloning of frequently
cloned repositories on <a href="https://hg.mozilla.org/">hg.mozilla.org</a> to
<a href="/blog/2015/07/08/cloning-from-s3/">Amazon S3</a> and
<a href="/blog/2015/09/01/serving-mercurial-clones-from-a-cdn/">a CDN</a> and
are diverting 1+ TB/day and countless hours of CPU work away from the
hg.mozilla.org servers themselves.</p>
<p>The positive impact from seeding clones from pre-generated,
externally-hosted bundles has been immense. Load on hg.mozilla.org
dropped off a cliff. Clone times on clients became a lot faster (mainly
because they aren't waiting for a server to dynamically generate/stream
bits). But there was a problem with this approach: it required the
cooperation of clients to install an extension in order for clone load
to be offloaded. It didn't <em>just work</em>.</p>
<p><strong>I'm pleased to announce that the ability to seed clones from
server-advertised pre-generated bundles is now a core feature in
Mercurial 3.6!</strong> Server operators can install the
<a href="https://selenic.com/repo/hg/file/default/hgext/clonebundles.py">clonebundles extension</a> (it is distributed with Mercurial)
to advertise the location of pre-generated, externally-hosted bundle
files. Compatible clients will automatically clone from the
server-advertised URLs instead of creating potentially excessive load
on the Mercurial server. The implementation is almost identical to what
Mozilla has deployed with great success. <strong>If you operate a Mercurial
server that needs to serve larger repositories (100+ MB) and/or is under
high load, you should be jumping with joy at the existence of this
feature, as it should make scaling problems attached to cloning mostly
go away.</strong></p>
<p>Documentation for server operators is currently in the extension and
can be accessed at the aforementioned URL or with
<em>hg help -e clonebundles</em>. It does require a bit of setup work. But
if you are at the scale where you could benefit from the feature, the
results will almost certainly be worth it.</p>
<p>One caveat is that the feature is currently behind an <em>experimental</em>
flag on the client. This means that it doesn't <em>just work</em> yet. This
is because we want to reserve the right to change some behaviors without
worrying about backwards compatibility. However, I'm pretty confident
the server parts won't change significantly. Or if they do, I'm pretty
committed to providing an easy transition path since I'll need one for
hg.mozilla.org. So, I'm giving server operators a tentative
green light to deploy this extension. I can't guarantee there won't be a
few bumps transitioning to a future release. But it shouldn't be a
break-the-world type of problem. It is my intent to remove the
experimental flag and have the feature enabled by default in
Mercurial 3.7. At that point, server operators just need clients to run
a modern Mercurial release and they can count on drastically reduced
load from cloning.</p>
<p>To help with adoption and testing of the <em>clone bundles</em> feature,
servers advertising bundles will inform compatible clients of the
existence of the feature when they clone:</p>
<div class="pygments_murphy"><pre>$ hg clone https://hg.mozilla.org/mozilla-central
requesting all changes
remote: this server supports the experimental &quot;clone bundles&quot; feature that should enable faster and more reliable cloning
remote: help test it by setting the &quot;experimental.clonebundles&quot; config flag to &quot;true&quot;
adding changesets
adding manifests
adding file changes
...
</pre></div>

<p>And if you have the feature enabled, you'll see something like:</p>
<div class="pygments_murphy"><pre>$ hg clone https://hg.mozilla.org/mozilla-central
applying clone bundle from https://hg.cdn.mozilla.net/mozilla-central/daa7d98525e859d32a3b3e97101e129a897192a1.gzip.hg
adding changesets
adding manifests
adding file changes
added 265986 changesets with 1501210 changes to 223996 files
finished applying clone bundle
searching for changes
adding changesets
adding manifests
adding file changes
added 1 changesets with 1 changes to 1 files
</pre></div>

<p>This new <em>clone bundles</em> feature is deployed on hg.mozilla.org. Users of
Mercurial 3.6 can start using it today by cloning from one of the
<a href="https://hg.cdn.mozilla.net/">repositories with bundles enabled</a>. (If
you have previously installed the <em>bundleclone</em> extension, please be
sure your <em>version-control-tools</em> repository is up to date, as the
extension was recently changed to better interact with the official
feature.)</p>
<p>And that's the <em>clone bundles</em> feature. I hope you are as excited about
it as I am!</p>
<p>Mercurial 3.6 also contains numerous performance improvements that make
cloning faster, regardless of whether you are using <em>clone bundles</em>! For
starters:</p>
<ul>
<li><a href="https://selenic.com/repo/hg/rev/836291420d53">Caching just-added entries</a>
  made changelog writing 25% faster.</li>
<li><a href="https://selenic.com/repo/hg/rev/39d643252b9f">Reusing file handles</a>
  when adding revlog entries drastically reduced the number of file
  opens, closes, and writes.</li>
<li><a href="https://selenic.com/repo/hg/rev/56a640b0f656">Avoiding excessive file flushing</a>
  when adding revlog entries drastically reduced system call count.</li>
</ul>
<p>These performance enhancements will make all operations that write
new repository data faster. But it will be felt most on clone and pull
operations on the client and push operations on the server.</p>
<p>One of the most impressive performance optimizations was to a Python
class that converts a generator of raw data chunks to something that
resembles a file object so it can be read() from.
<a href="https://selenic.com/repo/hg/rev/6ae14d1ca3aa">Refactoring read()</a>
to avoid <a href="https://docs.python.org/2/library/collections.html#collections.deque">collections.deque</a>
operations and an extra string slice and allocation made <em>unbundle</em>
operations 15-20% faster. Since this function can handle hundreds of
megabytes or even gigabytes of data across hundreds of thousands of
calls, small improvements like this can make a huge difference! This
patch was a stark reminder that function calls, collection mutations,
string slicing, and object allocation all can have a significant cost
in a higher-level, garbage collected language like Python.</p>
<p>The end result of all this performance optimization on applying a
mozilla-central gzip bundle on Linux on an i7-6700K:</p>
<ul>
<li>35-40s wall time faster (~245s to ~205s) (~84% of original)</li>
<li>write(2) calls reduced from 1,372,411 to 679,045 (~49% of
  original)</li>
<li>close(2) calls reduced from 405,147 to 235,039 (~58% of
  original)</li>
<li>total system calls reduced from 5,120,893 to 2,938,479 (~57% of
  original)</li>
</ul>
<p>And the same operation on Windows 10 on the same machine:</p>
<ul>
<li>~300s wall time faster (933s to 633s) (~68% of original)</li>
</ul>
<p>You may have noticed the discrepancy between Linux and Windows wall
times, where Windows is 2-4x slower than Linux. What gives? The reason
is closing file handles that have been appended to is slow on Windows.
For more, read my <a href="/blog/2015/10/22/append-i/o-performance-on-windows/">recent blog post</a>.</p>
<p>Mercurial writes ~226,000 files during a clone of mozilla-central
(excluding the working copy). Assuming 2ms per file close operation,
that comes out to <strong>~450s just for file close operations</strong>! (All
operations are on the same thread.) The current wall time difference
between clone times on Windows and Linux is ~428s. So it's fair to say
that waiting on file closes accounts for most of this.</p>
<p>Along the same vein, the aforementioned performance work reduced total
number of file close operations during a mozilla-central clone by
~165,000. Again assuming 2ms per file close, that comes to ~330s, which
is in the same ballpark as the ~300s wall time decrease we see on
Windows in Mercurial 3.6. <strong>Writing - and therefore closing - hundreds
of thousands of files handles is slower on Windows and accounts for most
of the performance difference on that platform.</strong></p>
<p>Empowered by this knowledge, I wrote some patches to move file closing
to a background thread on Windows. The results were
<a href="https://selenic.com/pipermail/mercurial-devel/2015-September/073788.html">promising</a>
(minutes saved when writing 100,000+ files). Unfortunately, I didn't
have time to finish these patches for Mercurial 3.6. Hopefully they'll
make it into 3.7. I also have some mad scientist ideas for alternate
storage mechanisms that don't rely on hundreds of thousands of files.
This should enable clones to run at 100+ MB/s on all platforms -
basically as fast as your network and system I/O can keep up (yes,
Python and Windows are capable of this throughput). Stay tuned.</p>
<p>And that's a summary of the cloning improvements in Mercurial 3.6!</p>
<p>Mercurial 3.6 is currently in release candidate. Please help test
it by downloading the RC at
<a href="https://www.mercurial-scm.org/">https://www.mercurial-scm.org/</a>.
Mercurial 3.6 final is due for release on or shortly after November 1.
There is a large gathering of Mercurial contributors in London this
weekend. So if a bug is reported, I can pretty much guarantee a lot of
eyeballs will see it and there's a good chance it will be acted upon.</p>]]></content:encoded>
    </item>
    <item>
      <title>Append I/O Performance on Windows</title>
      <link>http://gregoryszorc.com/blog/2015/10/22/append-i/o-performance-on-windows</link>
      <pubDate>Thu, 22 Oct 2015 02:15:00 PDT</pubDate>
      <category><![CDATA[Mozilla]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2015/10/22/append-i/o-performance-on-windows</guid>
      <description>Append I/O Performance on Windows</description>
      <content:encoded><![CDATA[<p>A few weeks ago, some coworkers were complaining about the relative
performance of Mercurial cloning on Windows. I investigated on my
brand new i7-6700K Windows 10 desktop machine and sure enough they were
correct: cloning times on Windows were several minutes slower than
Linux on the same hardware. What gives?</p>
<p>I performed a few clones with Python under a profiler. It pointed to a
potential slowdown in file I/O. I wanted more details so I fired up
<a href="https://technet.microsoft.com/en-us/library/bb896645.aspx">Sysinternals Process Monitor</a>
(strace for Windows) and captured data for a clone.</p>
<p>As I was looking at the raw system calls related to I/O, something
immediately popped out: <em>CloseFile()</em> operations were frequently
taking 1-5 <em>milliseconds</em> whereas other operations like opening, reading,
and writing files only took 1-5 <em>microseconds</em>. <strong>That's a 1000x
difference!</strong></p>
<p>I wrote a custom Python script to analyze an export of Process Monitor's
data. Sure enough, it said we were spending hundreds of seconds in
<em>CloseFile()</em> operations (it was being called a few hundred thousand
times). I posted the findings to some mailing lists.
<a href="https://groups.google.com/d/msg/mozilla.dev.platform/yupx2ToQ5T4/WAMC_Q-DCAAJ">Follow-ups</a>
in Mozilla's dev-platform list pointed me to
<a href="http://blogs.msdn.com/b/oldnewthing/archive/2011/09/23/10215586.aspx">an old MSDN blog post</a>
where it documents behavior similar to what I was seeing.</p>
<p>Long story short, <strong>closing file handles that have been appended to is
slow on Windows.</strong> This is apparently due to an implementation detail of
NTFS. Writing to a file in place is fine and only takes microseconds for
the open, write, and close. But if you append a file, closing the
associated file handle is going to take a few milliseconds. Even if you
are using Overlapped I/O (async I/O on Windows), the <em>CloseHandle()</em>
call to close the file handle blocks the calling thread! Seriously.</p>
<p>This behavior is in stark contrast to Linux and OS X, where system I/O
functions take microseconds (assuming your I/O subsystem can keep up).</p>
<p>There are two ways to work around this issue:</p>
<ol>
<li>Reduce the amount of file closing operations on appended files.</li>
<li>Use multiple threads for I/O on Windows.</li>
</ol>
<p>Armed with this knowledge, I dug into the guts of Mercurial and
proceeded to write a
<a href="https://selenic.com/repo/hg/rev/836291420d53">number</a>
<a href="https://selenic.com/repo/hg/rev/39d643252b9f">of</a>
<a href="https://selenic.com/repo/hg/rev/56a640b0f656">patches</a>
that drastically reduced the amount of file I/O system calls during
clone and pull operations. While I intend to write a blog post with the
full details, <strong>cloning the Firefox repository with Mercurial 3.6 on
Windows is now several minutes faster.</strong> Pretty much all of this is due
to reducing the number of file close operations by aggressively reusing
file handles.</p>
<p>I also
<a href="https://selenic.com/pipermail/mercurial-devel/2015-September/073788.html">experimented</a>
with moving file close operations to a separate thread on Windows. While
this change didn't make it into Mercurial 3.6, the results were very
promising. Even on Python (which doesn't have real asynchronous threads
due to the GIL), moving file closing to a background thread freed up the
main thread to do the CPU heavy work of processing data. This made clones
several minutes faster. (Python does release the GIL when performing an
I/O system call.) Furthermore, <strong>simply creating a dedicated thread for
closing file handles made Mercurial faster than 7-zip at writing tens of
thousands of files from an uncompressed tar archive</strong>. (I'm not going to
post the time for <em>tar</em> on Windows because it is embarassing.) That's a
Python process on Windows faster than a native executable that is lauded
for its speed (7-zip). Just by offloading file closing to a single
separate thread. Crazy.</p>
<p>I can optimize file closing in Mercurial all I want. However,
Mercurial's storage model relies on several files. For the Firefox
repository, we have to write ~225,000 files during clone. Assuming
1ms per file close (which is generous), that's 225s (or 3:45) wall
time performing file closes. That's not going to scale. I've already
started experimenting with alternative storage modes that initially use
1-6 files. This should enable Mercurial clones to run at over 100 MB/s
(yes, Python and Windows can do I/O that quickly if you are smart about
things).</p>
<p>My primary takeaway is that creating/appending to thousands of files
is slow on Windows and should be addressed at the architecture level
by not requiring thousands of files and at the implementation level
by minimizing the number of file close operations after write.
If you absolutely must create/append to thousands of files, use multiple
threads for at least closing file handles.</p>
<p>My secondary takeaway is that Sysinternals Process Monitor is amazing.
I used it against Firefox and immediately found
<a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1211090">performance concerns</a>.
It can be extremely eye opening to see how your higher-level code
is translated into function calls into your operating system and where
the performance hot spots are or aren't at the OS level.</p>]]></content:encoded>
    </item>
    <item>
      <title>Lowering the Barrier to Pushing to MozReview</title>
      <link>http://gregoryszorc.com/blog/2015/10/14/lowering-the-barrier-to-pushing-to-mozreview</link>
      <pubDate>Wed, 14 Oct 2015 12:30:00 PDT</pubDate>
      <category><![CDATA[MozReview]]></category>
      <category><![CDATA[Mozilla]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2015/10/14/lowering-the-barrier-to-pushing-to-mozreview</guid>
      <description>Lowering the Barrier to Pushing to MozReview</description>
      <content:encoded><![CDATA[<p>Starting today, a Mozilla LDAP account with Mercurial SSH access is no
longer required to <em>hg push</em> into
<a href="https://reviewboard.mozilla.org">MozReview</a> to initiate code review
with Mozilla projects.</p>
<p>The <a href="https://mozilla-version-control-tools.readthedocs.org/en/latest/mozreview/install.html">instructions for configuring your client to use MozReview</a>
have been updated to reflect how you can now push to MozReview over HTTP
using a Bugzilla API Key for authentication.</p>
<p>This change effectively enables first-time contributors to use MozReview
for code review. Before, you had to obtain an LDAP account and configure
your SSH client, both of which could be time consuming processes and
therefore discourage people from contributing. (Or you could just use
Bugzilla/Splinter and not get the benefits of MozReview, which many
did.)</p>
<p><strong>I encourage others to update contribution docs to start nudging people
towards MozReview over Bugzilla/patch-based workflows</strong> (such as
bzexport).</p>
<p><a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1195856">Bug 1195856</a>
tracked this feature.</p>]]></content:encoded>
    </item>
    <item>
      <title>Serving Mercurial Clones from a CDN</title>
      <link>http://gregoryszorc.com/blog/2015/09/01/serving-mercurial-clones-from-a-cdn</link>
      <pubDate>Tue, 01 Sep 2015 15:00:00 PDT</pubDate>
      <category><![CDATA[Mercurial]]></category>
      <category><![CDATA[Mozilla]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2015/09/01/serving-mercurial-clones-from-a-cdn</guid>
      <description>Serving Mercurial Clones from a CDN</description>
      <content:encoded><![CDATA[<p>For the past few months, Mozilla has been
<a href="http://gregoryszorc.com/blog/2015/07/08/cloning-from-s3/">serving Mercurial clones from Amazon S3</a>.
We upload snapshots (called <em>bundles</em>) of large and/or high-traffic
repositories to S3. We have a custom Mercurial extension on the
client and server that knows how to exchange the URLs for these
snapshots and to transparently use them to bootstrap a clone. The
end result is drastically reduced Mercurial server load and faster
clone times. The benefits are seriously ridiculous when you operate
version control at scale.</p>
<p><a href="https://aws.amazon.com/cloudfront/">Amazon CloudFront</a> is a CDN.
You can easily configure it up to be backed by an S3 bucket. So
we did.</p>
<p><a href="https://hg.cdn.mozilla.net/">https://hg.cdn.mozilla.net/</a> is
Mozilla's CDN for hosting Mercurial data. Currently it's just bundles to
be used for cloning.</p>
<p>As of today, if you
<a href="https://mozilla-version-control-tools.readthedocs.org/en/latest/hgmo/bundleclone.html">install the bundleclone Mercurial extension</a>
and <em>hg clone</em> a repository on
<a href="https://hg.mozilla.org/">hg.mozilla.org</a> such as
<a href="https://hg.mozilla.org/mozilla-central">mozilla-central</a>
(<em>hg clone https://hg.mozilla.org/mozilla-central</em>), the CDN
URLs will be preferred by default. (Previously we preferred S3 URLs
that hit servers in Oregon, USA.)</p>
<p><strong>This should result in clone time reductions for Mozillians not
close to Oregon, USA</strong>, as the CloudFront CDN has servers all across
the globe and your Mercurial clone should be bootstrapped from the
closest and hopefully therefore fastest server to you.</p>
<p>Unfortunately, you do need the the aforementioned <em>bundleclone</em>
extension installed for this to work. But, this should only be
temporary: I've
<a href="https://mercurial.selenic.com/wiki/StaticBundlePlan">proposed</a>
integrating this feature into the core of Mercurial so if a client
talks to a server advertising pre-generated bundles the clone
offload <em>just works</em>. I already have tentative buy-in from one
Mercurial maintainer. So hopefully I can land this feature in
Mercurial 3.6, which will be released November 1. After that,
I imagine some high-traffic Mercurial servers (such as Bitbucket)
will be very keen to deploy this so CPU load on their servers
is drastically reduced.</p>]]></content:encoded>
    </item>
    <item>
      <title>JSON APIs on hg.mozilla.org</title>
      <link>http://gregoryszorc.com/blog/2015/08/18/json-apis-on-hg.mozilla.org</link>
      <pubDate>Tue, 18 Aug 2015 16:00:00 PDT</pubDate>
      <category><![CDATA[Mercurial]]></category>
      <category><![CDATA[Mozilla]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2015/08/18/json-apis-on-hg.mozilla.org</guid>
      <description>JSON APIs on hg.mozilla.org</description>
      <content:encoded><![CDATA[<p>I added a feature to Mercurial 3.4 that exposes JSON from Mercurial's
various <a href="https://hg.mozilla.org/mozilla-central/help/hgweb">web APIs</a>.
Unfortunately, due to the presence of legacy code on hg.mozilla.org
providing similar functionality, we weren't able to deploy this
feature to hg.mozilla.org when we deployed Mercurial 3.4 several weeks
ago.</p>
<p>I'm pleased to announce that as of today, JSON is now exposed from
hg.mozilla.org!</p>
<p>To access JSON output, simply add <strong>json-</strong> to the <em>command</em> name
in URLs. e.g. instead of
<a href="https://hg.mozilla.org/mozilla-central/rev/de7aa6b08234">https://hg.mozilla.org/mozilla-central/rev/de7aa6b08234</a>
use <a href="https://hg.mozilla.org/mozilla-central/json-rev/de7aa6b08234">https://hg.mozilla.org/mozilla-central/json-rev/de7aa6b08234</a>.
The full list of web commands, URL patterns, and their parameters are
<a href="https://hg.mozilla.org/mozilla-central/help/hgweb">documented</a> in
the <em>hgweb</em> help topic.</p>
<p>Not all web commands support JSON output yet. Not all web commands
expose all data available to them. If there is data you need but isn't
exposed, please
<a href="https://bugzilla.mozilla.org/enter_bug.cgi?product=Developer%20Services&amp;component=Mercurial%3A%20hg.mozilla.org">file a bug</a>
and I'll see what I can do.</p>
<p>Thanks go to Steven MacLeod for
<a href="https://reviewboard.mozilla.org/r/15617/">reviewing</a> the rather large
series it took to make this happen.</p>]]></content:encoded>
    </item>
  </channel>
</rss>

<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0"
     xmlns:content="http://purl.org/rss/1.0/modules/content/"
     xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
     xmlns:atom="http://www.w3.org/2005/Atom"
     xmlns:dc="http://purl.org/dc/elements/1.1/"
     xmlns:wfw="http://wellformedweb.org/CommentAPI/"
     >
  <channel>
    <title>Gregory Szorc's Digital Home</title>
    <link>http://gregoryszorc.com/blog</link>
    <description>Rambling on</description>
    <pubDate>Sat, 28 Mar 2015 21:37:50 GMT</pubDate>
    <generator>Blogofile</generator>
    <sy:updatePeriod>hourly</sy:updatePeriod>
    <sy:updateFrequency>1</sy:updateFrequency>
    <item>
      <title>Notes from Facebook's Developer Infrastructure at Scale F8 Talk</title>
      <link>http://gregoryszorc.com/blog/2015/03/28/notes-from-facebook's-developer-infrastructure-at-scale-f8-talk</link>
      <pubDate>Sat, 28 Mar 2015 11:45:00 PDT</pubDate>
      <category><![CDATA[Mozilla]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2015/03/28/notes-from-facebook's-developer-infrastructure-at-scale-f8-talk</guid>
      <description>Notes from Facebook's Developer Infrastructure at Scale F8 Talk</description>
      <content:encoded><![CDATA[<p>Any time Facebook talks about technical matters I tend to listen.
They have a track record of demonstrating engineering leadership
in several spaces. And, unlike many companies that just talk, Facebook
often gives others access to those ideas via source code and healthy
open source projects. It's rare to see a company operating on the
frontier of the computing field provide so much insight into their
inner workings. You can gain so much by riding their cotails and
following their lead instead of clinging to and cargo culting from
the past.</p>
<p>The Facebook F8 developer conference was this past week. All the
talks are <a href="https://developers.facebooklive.com/">now available online</a>.
<strong>I encourage you to glimpse through the list of talks and watch
whatever is relevant to you.</strong> There's really a little bit for
everyone.</p>
<p>Of particular interest to me is the
<a href="https://developers.facebooklive.com/videos/561/big-code-developer-infrastructure-at-facebook-s-scale">Big Code: Developer Infrastructure at Facebook's Scale</a>
talk. This is highly relevant to my job role as Developer Productivity
Engineer at Mozilla.</p>
<p>My notes for this talk follow.</p>
<p><strong>"We don't want humans waiting on computers. We want computers waiting
on humans."</strong> (This is the common theme of the talk.)</p>
<p>In 2005, Facebook was on Subversion. In 2007 moved to Git. Deployed
a bridge so people worked in Git and had distributed workflow but
pushed to Subversion under the hood.</p>
<p>New platforms over time. Server code, iOS, Android. One Git repo
per platform/project -&gt; 3 Git repos. Initially no code sharing, so
no problem. Over time, code sharing between all repos. Lots of code
copying and confusion as to what is where and who owns what.</p>
<p>Facebook is mere weeks away from completing their migration to
consolidate the big three repos to a Mercurial monorepo. (See also
<a href="/blog/2014/09/09/on-monolithic-repositories/">my post about monorepos</a>.)</p>
<p>Reasons:</p>
<ol>
<li>Easier code sharing.</li>
<li>Easier large-scale changes. Rewrite the universe at once.</li>
<li>Unified set of tooling.</li>
</ol>
<p>Facebook employees run &gt;1M source control commands per day. &gt;100k
commits per week. VCS tool needs to be fast to prevent distractions
and context switching, which slow people down.</p>
<p>Facebook implemented sparse checkout and shallow history in Mercurial.
Necessary to scale distributed version control to large repos.</p>
<p><strong>Quote from Google: "We're excited about the work Facebook is doing with
Mercurial and glad to be collaborating with Facebook on Mercurial
development."</strong> (Well, I guess the cat is finally out of the bag:
Google is working on Mercurial. This was kind of an open secret for
months. But I guess now it is official.)</p>
<p>Push-pull-rebase bottleneck: if you rebase and push and someone beats
you to it, you have to pull, rebase, and try again. This gets worse
as commit rate increases and people do needless legwork. <strong>Facebook
has moved to server-side rebasing on push</strong> to mostly eliminate this
pain point. (This is part of a still-experimental feature in Mercurial,
which should hopefully lose its experimental flag soon.)</p>
<p>Starting 13:00 in we have a speaker change and move away from version
control.</p>
<p>IDEs don't scale to Facebook scale. <strong>"Developing in Xcode at Facebook
is an exercise in frustration."</strong> On average 3.5 minutes to open
Facebook for iOS in Xcode. 5 minutes on average to index. Pegs CPU
and makes not very responsive. 50 Xcode crashes per day across all
Facebook iOS developers.</p>
<p><strong>Facebook measures everything about tools. Mercurial operation times.
Xcode times. Build times. Data tells them what tools and workflows
need to be worked on.</strong></p>
<p>Facebook believes IDEs are worth the pain because they make people
more productive.</p>
<p>Facebook wants to support all editors and IDEs since people want to
use whatever is most comfortable.</p>
<p>React Native changed things. Supported developing on multiple
platforms, which no single IDE supports. People launched several
editors and tools to do React Native development. People needed 4
windows to do development. That experience was "not acceptable."
So they built their own IDE. Set of plugins on top of ATOM. Not
a fork. They like hackable and web-y nature of ATOM.</p>
<p>The demo showing iOS development looks very nice! Doing Objective-C,
JavaScript, simulator integration, and version control in one window!</p>
<p>It can connect to remote servers and transparently save and
deploy changes. It can also get real-time compilation errors and hints
from the remote server! (Demo was with Hack. Not sure if others langs
supported. Having beefy central servers for e.g. Gecko development
would be a fun experiment.)</p>
<p>Starting at 32:00 presentation shifts to continuous integration.</p>
<p>Number one goal of CI at Facebook is developer efficiency. <strong>We
don't want developers waiting on computers to build and test diffs.</strong></p>
<p>3 goals for CI:</p>
<ol>
<li>High-signal feedback. Don't want developers chasing failures that
   aren't their fault. Wastes time.</li>
<li>Must provide rapid feedback. Developers don't want to wait.</li>
<li>Provide frequent feedback. Developers should know as soon as
   possible after they did something. (I think this refers to local
   feedback.)</li>
</ol>
<p>Sandcastle is their CI system.</p>
<p>Diff lifecycle discussion.</p>
<p>Basic tests and lint run locally. (My understanding from talking
with Facebookers is "local" often means on a Facebook server, not
local laptop. Machines at developers fingertips are often dumb
terminals.)</p>
<p>They appear to use code coverage to determine what tests to run.
"We're not going to run a test unless your diff might actually have
broken it."</p>
<p>They run flaky tests less often.</p>
<p>They run slow tests less often.</p>
<p><strong>Goal is to get feedback to developers in under 10 minutes.</strong></p>
<p><strong>If they run fewer tests and get back to developers quicker,
things are less likely to break than if they run more tests but
take longer to give feedback.</strong></p>
<p>They also want feedback quickly so reviewers can see results at
review time.</p>
<p>They use Web Driver heavily. Love cross-platform nature of Web Driver.</p>
<p>In addition to test results, performance and size metrics are reported.</p>
<p>They have a "Ship It" button on the diff.</p>
<p>Landcastle handles landing diff.</p>
<p>"It is not OK at Facebook to land a diff without using Landcastle."
(Read: developers don't push directly to the master repo.)</p>
<p>Once Landcastle lands something, it runs tests again. If an issue
is found, a task is filed. Task can be "push blocking."
Code won't ship to users until the "push blocking" issue resolved.
(Tweets confirm they do backouts "fairly aggressively." A valid
resolution to a push blocking task is to backout. But fixing forward
is fine as well.)</p>
<p>After a while, branch cut occurs. Some cherry picks onto release
branches.</p>
<p>In addition to diff-based testing, they do continuous testing runs.
Much more comprehensive. No time restrictions. Continuous runs on
master and release candidate branches. Auto bisect to pin down
regressions.</p>
<p>Sandcastle processes &gt;1000 test results per second. 5 years of machine
work per day. Thousands of machines in 5 data centers.</p>
<p>They started with buildbot. Single master. Hit scaling limits of
single thread single master. Master could not push work to workers
fast enough. Sandcastle has distributed queue. Workers just pull
jobs from distributed queue.</p>
<p>"High-signal feedback is critical." "Flaky failures erode developer
confidence." "We need developers to trust Sandcastle."</p>
<p>Extremely careful separating infra failures from other failures.
Developers don't see infra failures. Infra failures only reported
to Sandcastle team.</p>
<p>Bots look for flaky tests. Stress test individual tests. Run tests
in parallel with themselves. Goal: developers don't see flaky tests.</p>
<p>There is a "not my fault" button that developers can use to report
bad signals.</p>
<p><strong>"Whatever the scale of your engineering organization, developer
efficiency is the key thing that your infrastructure teams should be
striving for. This is why at Facebook we have some of our top
engineers working on developer infrastructure."</strong> (Preach it.)</p>
<p>Excellent talk. <strong>Mozillians doing infra work or who are in charge
of head count for infra work should watch this video.</strong></p>
<p><em>Update 2015-03-28 21:35 UTC - Clarified some bits in response to
new info Tweeted at me. Added link to my monorepos blog post.</em></p>]]></content:encoded>
    </item>
    <item>
      <title>New High Scores for hg.mozilla.org</title>
      <link>http://gregoryszorc.com/blog/2015/03/19/new-high-scores-for-hg.mozilla.org</link>
      <pubDate>Thu, 19 Mar 2015 20:20:00 PDT</pubDate>
      <category><![CDATA[Mercurial]]></category>
      <category><![CDATA[Mozilla]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2015/03/19/new-high-scores-for-hg.mozilla.org</guid>
      <description>New High Scores for hg.mozilla.org</description>
      <content:encoded><![CDATA[<p>It's been a <a href="/blog/2015/03/18/network-events/">rough week</a>.</p>
<p>The very short summary of events this week is that both the Firefox
and Firefox OS release automation has been performing a denial of
service attack against
<a href="https://hg.mozilla.org/">hg.mozilla.org</a>.</p>
<p>On the face of it, this is nothing new. The release automation
is by far the top consumer of hg.mozilla.org data, requesting several
terabytes per day via several million HTTP requests from thousands of
machines in multiple data centers. The very nature of their existence
makes them a significant denial of service threat.</p>
<p>Lots of things went wrong this week. While a post mortem will shed
light on them, many fall under the umbrella of <em>release automation was
making more requests than it should have and was doing so in a way
that both increased the chances of an outage occurring and increased
the chances of a prolonged outage.</em> This resulted in the hg.mozilla.org
servers working harder than they ever have. As a result, we have some
new <em>high scores</em> to share.</p>
<ul>
<li>
<p>On UTC day March 19, hg.mozilla.org transferred 7.4 TB of data.
  This is a significant increase from the ~4 TB we expect on a typical
  weekday. (Even more significant when you consider that most load is
  generated during peak hours.)</p>
</li>
<li>
<p>During the 1300 UTC hour of March 17, the cluster received 1,363,628
  HTTP requests. No HTTP 503 Service Not Available errors were
  encountered in that window! 300,000 to 400,000 requests per hour is
  typical.</p>
</li>
<li>
<p>During the 0800 UTC hour of March 19, the cluster transferred 776 GB
  of repository data. That comes out to at least 1.725 Gbps on average
  (I didn't calculate TCP and other overhead). Anything greater than 250
  GB per hour is not very common. No HTTP 503 errors were served from
  the origin servers during this hour!</p>
</li>
</ul>
<p>We encountered many periods where hg.mozilla.org was operating more than
twice its normal and expected operating capacity and it was able to
handle the load just fine. As a server operator, I'm proud of this.
The servers were provisioned beyond what is normally needed of them and
it took a truly exceptional event (or two) to bring the service down.
This is generally a good way to do hosted services (you rarely want to be
barely provisioned because you fall over at the slighest change and you
don't want to be grossly over-provisioned because you are wasting money
on idle resources).</p>
<p>Unfortunately, the hg.mozilla.org service did fall over. Multiple times,
in fact. There is room to improve. As proud as I am that the service
operated well beyond its expected limits, I can't help but feel ashamed
that it did eventual cave in under even extreme load and that people are
probably making under-informed general assumptions like <em>Mercurial can't
scale</em>. The simple fact of the matter is that clients cumulatively
generated an exceptional amount of traffic to hg.mozilla.org this week.
All servers have capacity limits. And this week we encountered the limit
for the current configuration of hg.mozilla.org. Cause and effect.</p>]]></content:encoded>
    </item>
    <item>
      <title>Network Events</title>
      <link>http://gregoryszorc.com/blog/2015/03/18/network-events</link>
      <pubDate>Wed, 18 Mar 2015 14:25:00 PDT</pubDate>
      <category><![CDATA[Mozilla]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2015/03/18/network-events</guid>
      <description>Network Events</description>
      <content:encoded><![CDATA[<p>The Firefox source repositories and automation have been closed the past
few days due to a couple of outages.</p>
<p>Yesterday, aggregate CPU usage on many of the machines in the hg.mozilla.org
cluster hit 100%. Previously, whenever hg.mozilla.org was under high load,
we'd run out of network bandwidth before we ran out of CPU on the machines.
In other words, Mercurial was generating data faster than the network could
accept it.</p>
<p>When this happened, the service started issuing HTTP 503 Service Not
Available responses. This is the universal server signal for <em>I'm down, go
away</em>. Unfortunately, not all clients did this.</p>
<p>Parts of Firefox's release automation retried failing requests
immediately, or with insufficient jitter in their backoff interval.
Actively retrying requests against a server that's experiencing load
issues only makes the problem worse. This effectively prolonged the
outage.</p>
<p>Today, we had a similar but different network issue. The load balancer
fronting hg.mozilla.org can only handle so much bandwidth. Today, we hit
that limit. The load balancer started throttling connections. Load on
hg.mozilla.org skyrocketed and request latency increased. From the
perspective of clients, the service grinded to a halt.</p>
<p>hg.mozilla.org was partially sharing a load balancer with
ftp.mozilla.org. That meant if one of the services experienced very high
load, the other service could effectively be <em>locked out</em> of bandwidth.
We saw this happening this morning. ftp.mozilla.org load was high (it
looks like downloads of Firefox Developer Edition are a major
contributor - these don't go through the CDN for reasons unknown to me)
and there wasn't enough bandwidth to go around.</p>
<p>Separately today, hg.mozilla.org again hit 100% CPU. At that time, it
also set a new record for network throughput: ~3 Gbps. It normally
consumes between 200 and 500 Mbps, with periodic spikes to 750 Mbps.
(Yesterday's event saw a spike to around ~2 Gbps.)</p>
<p>Going back through the hg.mozilla.org server logs, an offender is
quite obvious. Before March 9, total outbound transfer for the
<a href="https://hg.mozilla.org/build/tools/">build/tools</a> repo was around
1 tebibyte per day. Starting on March 9, it increased to 3 tebibytes per
day! This is quite remarkable, as a clone of this repo is only about
20 MiB. This means the repo was getting cloned about 150,000 times per
day! (Note: I think all these numbers may be low by ~20% - stay tuned
for the final analysis.)</p>
<p>2 TiB/day is statistically significant because we transfer less than 10
TiB/day across all of hg.mozilla.org. And, 1 TiB/day is close to 100
Mbps, assuming requests are evenly spread out (which of course they
aren't).</p>
<p>Multiple things went wrong. If only one or two happened, we'd likely be
fine. Maybe there would have been a short blip. But not the major event
we've been firefighting the last ~24 hours.</p>
<p>This post is only a summary of what went wrong. I'm sure there will be a
post-mortem and that it will contain lots of details for those who want
to know more.</p>]]></content:encoded>
    </item>
    <item>
      <title>Lost Productivity Due to Non-Unified Repositories</title>
      <link>http://gregoryszorc.com/blog/2015/02/17/lost-productivity-due-to-non-unified-repositories</link>
      <pubDate>Tue, 17 Feb 2015 14:50:00 PST</pubDate>
      <category><![CDATA[Mozilla]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2015/02/17/lost-productivity-due-to-non-unified-repositories</guid>
      <description>Lost Productivity Due to Non-Unified Repositories</description>
      <content:encoded><![CDATA[<p>I'm currently working on
<a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1132771">annotating moz.build files with metadata</a>
that defines things like which bug component and code reviewers map to
which files. It's going to enable a lot of awesomeness.</p>
<p>As part of this project, I'm implementing a new moz.build processing
mode. Instead of reading moz.build files by traversing <em>DIRS</em> variables
from previously-executed moz.build files, we're evaluating moz.build
files according to filesystem topology. This has uncovered a few
cases where a moz.build file errors because of assumptions that no
longer hold. For example, for directories that are only active on
Windows, the moz.build file might assume that <em>if Windows</em> is always true.</p>
<p>One such problem was with
<a href="https://dxr.mozilla.org/mozilla-central/source/gfx/angle/src/libGLESv2/moz.build">gfx/angle/srx/libGLESv2/moz.build</a>.
This file contained code similar to the following:</p>
<pre><code>if CONFIG['IS_WINDOWS']:
    SOURCES += ['foo.cpp']
...
SOURCES['foo.cpp'].flags += ['-DBAR']
</code></pre>
<p>This always ran without issue because this moz.build was only included
if building for Windows. This assumption is of course invalid when in
filesystem traversal mode.</p>
<p>Anyway, as part of updating this trouble file, I lost maybe an hour
of productivity. Here's how.</p>
<p>The top of the trouble moz.build file has a comment:</p>
<pre><code># Please note this file is autogenerated from generate_mozbuild.py,
# so do not modify it directly
</code></pre>
<p>OK. So, I need to modify <em>generate_mozbuild.py</em>. First thing's first: I
need to locate it:</p>
<pre><code>$ hg locate generate_mozbuild.py
gfx/skia/generate_mozbuild.py
</code></pre>
<p>So I load up this file. I see a <em>main()</em>. I run the script in my shell
and get an error. Weird. I look around <em>gfx/skia</em> and see a README_MOZILLA
file. I open it. README_MOZILLA contains some instructions. They aren't
very good. I hop in #gfx on IRC and ask around. They tell me to do a
Subversion clone of Skia and to check out the commit referenced in
README_MOZILLA. There is no repo URL in README_MOZILLA. I search Google.
I find a Git URL. I notice that README_MOZILLA contains a SHA-1 commit,
not a Subversion integer revision. I figure the Git repo is what was
meant. I clone the Git repo. I attempt to run the generation script
referenced by README_MOZILLA. It fails. I ask again in #gfx. They are
baffled at first. I dig around the source code. I see a reference in
Skia's upstream code to a path that doesn't exist. I tell the #gfx
people. They tell me sub-repos are likly involved and to use <em>gclient</em>
to clone the repo. I search for the proper Skia source code docs and
type the necessary gclient commands. (Fortunately I've used gclient
before, so this wasn't completely alien to me.)</p>
<p>I get the Skia clone in the proper state. I run the generation script
and all works. But I don't see it writing the trouble moz.build file I
set out to fix. I set some breakpoints. I run the code again. I'm
baffled.</p>
<p>Suddenly it hits me: I've been poking around with <em>gfx/skia</em> which is
separate from <em>gfx/angle</em>! I look around <em>gfx/angle</em> and see
a README.mozilla file. I open it. It reveals the existence of the Git
repo <a href="https://github.com/mozilla/angle">https://github.com/mozilla/angle</a>.
I open GitHub in my browser. I see a <em>generate_mozbuild.py</em> script.</p>
<p>I now realize there are multiple files named <em>generate_mozbuild.py</em>.
Unfortunately, the one I care about - the ANGLE one - is not checked
into mozilla-central. So, my search for it with <em>hg files</em> did not
reveal its existence. Between me trying to get the Skia code cloned and
generating moz.build files, I probably lost an hour of work. All because
a file with a similar name wasn't checked into mozilla-central!</p>
<p>I assumed that the single <em>generate_mozbuild.py</em> I found under source
control was the only file of that name and that it <em>must</em> be the file
I was interested in.</p>
<p>Maybe I should have known to look at <em>gfx/angle/README.mozilla</em> first.
Maybe I should have known that <em>gfx/angle</em> and <em>gfx/skia</em> are completely
independent.</p>
<p>But I didn't. My ignorance cost me.</p>
<p>Had the contents of the separate ANGLE repository been checked into
mozilla-central, I would have seen the multiple <em>generate_mozbuild.py</em>
files and I would likely have found the correct one immediately. But
they weren't and I lost an hour of my time.</p>
<p>And I'm not done. Now I have to figure out how the separate ANGLE repo
integrates with mozilla-central. I'll have to figure out how to submit the
patch I still need to write. The GitHub description of this repo says
<em>Talk to vlad, jgilbert, or kamidphish for more info</em>. So now I have to
bother them before I can submit my patch. Maybe I'll just submit a pull
request and see what happens.</p>
<p>I'm convinced I wouldn't have encountered this problem if a
<a href="/blog/2014/09/09/on-monolithic-repositories/">monolithic repository</a>
were used. I would have found the separate <em>generate_mozbuild.py</em> file
immediately. And, the change process would likely have been known to me
since all the code was in a repository I already knew how to submit
patches from.</p>
<p>Separate repos are just lots of pain. You can bet I'll link to this post
when people propose splitting up mozilla-central into multiple
repositories.</p>]]></content:encoded>
    </item>
    <item>
      <title>Branch Cleanup in Firefox Repositories</title>
      <link>http://gregoryszorc.com/blog/2015/01/28/branch-cleanup-in-firefox-repositories</link>
      <pubDate>Wed, 28 Jan 2015 20:35:00 PST</pubDate>
      <category><![CDATA[Mercurial]]></category>
      <category><![CDATA[Mozilla]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2015/01/28/branch-cleanup-in-firefox-repositories</guid>
      <description>Branch Cleanup in Firefox Repositories</description>
      <content:encoded><![CDATA[<p>Mozilla has historically done some funky things with the Firefox
Mercurial repositories. One of the things we've done is create
a bunch of named branches to track the Firefox release process.
These are branch names like <em>GECKO20b12_2011022218_RELBRANCH</em>.</p>
<p>Over in
<a href="https://bugzilla.mozilla.org/show_bug.cgi?id=927219">bug 927219</a>,
we started the process of cleaning up some cruft left over from
many of these old branches.</p>
<p>For starters, the old named branches in the Firefox repositories
are being actively closed. When you <em>hg commit --close-branch</em>,
Mercurial creates a special commit that says <em>this branch is closed</em>.
Branches that are closed are automatically hidden from the output
of <em>hg branches</em> and <em>hg heads</em>. As a result, the output of these
commands is now much more usable.</p>
<p>Closed branches still constitute <em>heads</em> on the DAG. And several heads
lead to degraded performance in some situations (notably push and pull
times - the same thing happens in Git). I'd like to eventually merge
these old heads so that repositories only have 1 or a small number of
DAG heads. However, extra care must be taken before that step. Stay
tuned.</p>
<p>Anyway, for the average person reading, you probably won't be impacted
by these changes at all. The greatest impact will be from the person who
lands the first change on top of any repository whose last commit
was a branch close. If you commit on top of the <em>tip</em> commit,
you'll be committing on top of a previously closed branch! You'll
instead want to <em>hg up default</em> after you pull to ensure you are on
the proper DAG head! And even then, if you have local commits, you may
not be based on top of the appropriate commit! A simple run of
<em>hg log --graph</em> should help you decipther the state of the world.
(Please note that the usability problems around discovering the
appropriate head to land on are a result of our poor branching strategy
for the Firefox repositories. We probably should have named branches
tracking the active Gecko releases. But that ship sailed years ago
and fixing that is pretty far down the priority list. Wallpapering
over things with the
<a href="https://mozilla-version-control-tools.readthedocs.org/en/latest/hgmozilla/firefoxtree.html">firefoxtree extensions</a>
is my recommended solution until matters are fixed.)</p>]]></content:encoded>
    </item>
    <item>
      <title>Commit Part Numbers and MozReview</title>
      <link>http://gregoryszorc.com/blog/2015/01/27/commit-part-numbers-and-mozreview</link>
      <pubDate>Tue, 27 Jan 2015 20:17:00 PST</pubDate>
      <category><![CDATA[Mozilla]]></category>
      <category><![CDATA[code review]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2015/01/27/commit-part-numbers-and-mozreview</guid>
      <description>Commit Part Numbers and MozReview</description>
      <content:encoded><![CDATA[<p>It is common for commit messages in Firefox to contains strings like
<em>Part 1</em>, <em>Part 2</em>, etc. See
<a href="https://hg.mozilla.org/projects/build-system/pushloghtml?changeset=5cb8bcab09cc">this push</a>
for <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=784841">bug 784841</a>
for an extreme multi-part example.</p>
<p>When code review is conducted in Bugzilla, these identifiers are
necessary because Bugzilla orders attachments/patches in the order they
were updated or their patch title (I'm not actually sure!). If part
numbers were omitted, it could be very confusing trying to figure out
which order patches should be applied in.</p>
<p>However, when code review is conducted in
<a href="https://mozilla-version-control-tools.readthedocs.org/en/latest/mozreview.html">MozReview</a>,
<strong>there is no need for explicit part numbers to convey ordering</strong> because
the ordering of commits is implicitly defined by the repository history
that you pushed to MozReview!</p>
<p>I argue that if you are using MozReview, you should stop writing <em>Part
N</em> in your commit messages, as it provides little to no benefit.</p>
<p>I, for one, welcome this new world order: I've previously wasted a lot
of time rewriting commit messages to reflect new part ordering after
doing history rewriting. With MozReview, that overhead is gone and I
barely pay a penalty for rewriting history, something that often
produces a more reviewable series of commits and makes reviewing
and landing a complex patch series significantly easier.</p>]]></content:encoded>
    </item>
    <item>
      <title>Automatic Python Static Analysis on MozReview</title>
      <link>http://gregoryszorc.com/blog/2015/01/24/automatic-python-static-analysis-on-mozreview</link>
      <pubDate>Sat, 24 Jan 2015 23:30:00 PST</pubDate>
      <category><![CDATA[Python]]></category>
      <category><![CDATA[Mozilla]]></category>
      <category><![CDATA[code review]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2015/01/24/automatic-python-static-analysis-on-mozreview</guid>
      <description>Automatic Python Static Analysis on MozReview</description>
      <content:encoded><![CDATA[<p>A bunch of us were in Toronto last week hacking on MozReview.</p>
<p>One of the cool things we did was deploy a bot for performing Python
static analysis. If you submit some .py files to MozReview, the bot
should leave a review. If it finds violations (it uses
<a href="https://flake8.readthedocs.org/">flake8</a> internally), it will open
an issue for each violation. It also leaves a comment that should
hopefully give enough detail on how to fix the problem.</p>
<p>While we haven't done much in the way of performance optimizations,
the bot typically submits results less than 10 seconds after the review
is posted! So, a human should never be reviewing Python that the bot
hasn't seen. This means you can stop thinking about style nits and start
thinking about what the code does.</p>
<p>This bot should be considered an alpha feature. The code for the bot
isn't even checked in yet. We're running the bot against production
to get a feel for how it behaves. If things don't go well, we'll turn
it off until the problems are fixed.</p>
<p>We'd like to eventually deploy C++, JavaScript, etc bots. Python won out
because it was the easiest to integrate (it has sane and efficient
tooling that is compatible with Mozilla's code bases - most existing
JavaScript tools won't work with Gecko-flavored JavaScript, sadly).</p>
<p>I'd also like to eventually make it easier to locally run the same
static analysis we run in MozReview. Addressing problems locally before
pushing is a no-brainer since it avoids needless context switching from
other people and is thus better for productivity. This will come in
time.</p>
<p>Report issues in #mozreview or in the Developer Services :: MozReview
Bugzilla component.</p>]]></content:encoded>
    </item>
    <item>
      <title>End to End Testing with Docker</title>
      <link>http://gregoryszorc.com/blog/2015/01/24/end-to-end-testing-with-docker</link>
      <pubDate>Sat, 24 Jan 2015 23:10:00 PST</pubDate>
      <category><![CDATA[Docker]]></category>
      <category><![CDATA[Mozilla]]></category>
      <category><![CDATA[testing]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2015/01/24/end-to-end-testing-with-docker</guid>
      <description>End to End Testing with Docker</description>
      <content:encoded><![CDATA[<p>I've written an extensive testing <em>framework</em> for Mozilla's
<a href="https://mozilla-version-control-tools.readthedocs.org/">version control tools</a>.
Despite it being a little rough around the edges, I'm a bit proud of it.</p>
<p>When you run tests for MozReview, Mozilla's heavily modified
<a href="https://www.reviewboard.org/">Review Board</a> code review tool, the
following things happen:</p>
<ul>
<li>A MySQL server is started in a Docker container.</li>
<li>A Bugzilla server (running the same code as
  <a href="https://bugzilla.mozilla.org/">bugzilla.mozilla.org</a>) is started on
  an Apache httpd server with mod_perl inside a Docker container.</li>
<li>A RabbitMQ server mimicking
  <a href="https://pulse.mozilla.org/">pulse.mozilla.org</a> is started in a Docker
  container.</li>
<li>A Review Board Django development server is started.</li>
<li>A Mercurial HTTP server is started</li>
</ul>
<p>In the future, we'll likely also need to add support for various other
services to support MozReview and other components of version control
tools:</p>
<ul>
<li>The Autoland HTTP service will be started in a Docker container, along
  with any other requirements it may have.</li>
<li>An IRC server will be started in a Docker container.</li>
<li>Zookeeper and Kafka will be started on multiple Docker containers</li>
</ul>
<p>The entire setup is pretty cool. You have actual services running on
your local machine. Mike Conley and Steven MacLeod even did some pair
coding of MozReview while on a plane last week. I think it's pretty cool
this is even possible.</p>
<p>There is very little mocking in the tests. If we need an external
service, we try to spin up an instance inside a local container.
This way, we can't have unexpected test successes or failures due to
bugs in mocking. We have very high confidence that if something works
against local containers, it will work in production.</p>
<p>I currently have each test file owning its own set of Docker
containers and processes. This way, we get full test isolation and can
run tests concurrently without race conditions. This drastically reduces
overall test execution time and makes individual tests easier to reason
about.</p>
<p>As cool as the test setup is, there's a bunch I wish were better.</p>
<p>Spinning up and shutting down all those containers and processes takes a
lot of time. We're currently sitting around 8s startup time and 2s
shutdown time. 10s overhead per test is unacceptable. When I make a one
line change, I want the tests to be instantenous. 10s is too long for
me to sit idly by. Unfortunately, I've already gone to great pains to
make test overhead as short as possible.
<a href="http://www.fig.sh/">Fig</a> wasn't good enough for me for various reasons.
I've reimplemented my own orchestration directly on top of the
<a href="https://docker-py.readthedocs.org/en/latest/">docker-py package</a> to
achieve some significant performance wins. Using
<a href="http://pythonhosted.org//futures/">concurrent.futures</a> to perform
operations against multiple containers concurrently was a big win.
<em>Bootstrapping</em> containers (running their first-run entrypoint scripts
and committing the result to be used later by tests) was a bigger win
(first run of Bugzilla is 20-25 seconds).</p>
<p>I'm at the point of optimizing startup where the longest pole is the
initialization of the services inside Docker containers themselves.
MySQL takes a few seconds to start accepting connections. Apache +
Bugzilla has a semi-involved initialization process. RabbitMQ takes
about 4 seconds to initialize. There are some cascading dependencies in
there, so the majority of startup time is waiting for processes to
finish their startup routine.</p>
<p>Another concern with running all these containers is memory usage. When
you start running 6+ instances of MySQL + Apache, RabbitMQ, + ..., it
becomes really easy to exhaust system memory, incur swapping, and have
performance fall off a cliff. I've spent a non-trivial amount of time
figuring out the minimal amount of memory I can make services consume
while still not sacrificing too much performance.</p>
<p>It is quite an experience having the problem of trying to minimize
resource usage and startup time for various applications. Searching the
internet will happily give you recommended settings for applications.
You can find out how to make a service start in 10s instead of 60s or
consume 100 MB of RSS instead of 1 GB. But what the internet won't tell
you is how to make the service start in 2s instead of 3s or consume as
little memory as possible. I reckon I'm past the point of diminishing
returns where most people don't care about any further performance wins.
But, because of how I'm using containers for end-to-end testing and I
have a surplus of short-lived containers, it is clearly I problem I need
to solve.</p>
<p>I might be able to squeeze out a few more seconds of reduction by
further optimizing startup and shutdown. But, I doubt I'll reduce things
below 5s. If you ask me, that's still not good enough. I want no more
than 2s overhead per test. And I don't think I'm going to get that
unless I start utilizing containers across multiple tests. And I really
don't want to do that because it sacrifices test purity. Engineering is
full of trade-offs.</p>
<p>Another takeaway from implementing this test harness is that the
pre-built Docker images available from the Docker Registry almost always
become useless. I eventually make a customization that can't be
shoehorned into the readily-available image and I find myself having
to reinvent the wheel. I'm not a fan of the <em>download and run a binary</em>
model, especially given Docker's less-than-stellar history on the
security and cryptography fronts (I'll trust Linux distributions to get
package distribution right, but I'm not going to be trusting the Docker
Registry quite yet), so it's not a huge loss. I'm at the point where
I've lost faith in Docker Registry images and my default position is to
implement my own builder. Containers are supposed to do one thing, so
it usually isn't that difficult to roll my own images.</p>
<p>There's a lot to love about Docker and containerized test execution. But
I feel like I'm foraging into new territory and solving problems
like startup time minimization that I shouldn't really have to be
solving. I think I can justify it given the increased accuracy from the
tests and the increased confidence that brings. I just wish the cost
weren't so high. Hopefully as others start leaning on containers and
Docker more for test execution, people start figuring out how to make
some of these problems disappear.</p>]]></content:encoded>
    </item>
    <item>
      <title>Bugzilla and the Future of Firefox Development</title>
      <link>http://gregoryszorc.com/blog/2015/01/16/bugzilla-and-the-future-of-firefox-development</link>
      <pubDate>Fri, 16 Jan 2015 10:50:00 PST</pubDate>
      <category><![CDATA[Bugzilla]]></category>
      <category><![CDATA[Mozilla]]></category>
      <category><![CDATA[code review]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2015/01/16/bugzilla-and-the-future-of-firefox-development</guid>
      <description>Bugzilla and the Future of Firefox Development</description>
      <content:encoded><![CDATA[<p><a href="https://bugzilla.mozilla.org">Bugzilla</a> has played a major role in the
Firefox development process for over 15 years. <strong>With upcoming changes
to how code changes to Firefox are submitted and reviewed, I think it is
time to revisit the central role of Bugzilla and bugs in the Firefox
development process.</strong> I know this is a contentious thing to say. Please,
gather your breath, and calmly read on as I explain why I believe this.</p>
<p>The current Firefox change process defaults to requiring a Bugzilla bug
for everything. It is rare (and from my experience frowned upon) when a
commit to Firefox doesn't reference a bug number. We've essentially made
Bugzilla and a bug prerequisites for changing anything in the Firefox
version control repository. For the remainder of this post, I'm going to
say that we <em>require</em> a bug for any change, even though that statement
isn't technically accurate. Also, when I say <em>Bugzilla</em>, I mean
<em>bugzilla.mozilla.org</em>, not the generic project.</p>
<p>Before I go on, let's boil the Firefox change process down to basics.</p>
<p>At the heart of any change to the Firefox source repository is a diff.
The diff (a representation of the differences between a set of files)
is the smallest piece of data necessary to represent a change to the
Firefox code. I argue that anything more than the vanilla diff is
overhead and could contribute to
<a href="/blog/2015/01/09/firefox-contribution-process-debt/">process debt</a>.
Now, there is some essential overhead. Version control tools supplement
diffs with metadata, such as the author, commit message, and date. Mozilla
has also instituted a near-mandatory code review policy, where changes
need to be signed off by a set of trusted individuals. I view both of
these additions to the vanilla diff as essential for Firefox development
and non-negotiable. Therefore, the bare minimum requirements for changing
Firefox code are a diff plus metadata (a commit/patch) and (almost
always) a review/sign-off. That's it. Notably absent from this list is a
Bugzilla bug. <strong>I argue that a bug is not strictly required to
change Firefox.</strong> Instead, we've instituted a near-universal policy
that we should have bugs. We've <strong>chosen</strong> to add overhead and process
debt - interaction with Bugzilla - to our Firefox change process.</p>
<p>Now, this choice to require all changes be associated with bugs has its
merits. Bugs provide excellent anchor points for historical context and
for additional information after the change has been committed and is
forever set in stone in the repository (commits are immutable in
Mercurial and Git and you can't easily attach metadata to the commit
after the fact). Bugs are great to track relationships between different
problems or units of work. Bugs can even be used to track progress
towards a large feature. Bugzilla components also provide a decent
mechanism to follow related activity. There's also a lot of tooling and
familiar process standing on top of the Bugzilla <em>platform</em>. There's a
lot to love here and I don't want diminish the importance of all these
things.</p>
<p><strong>When I look to the future, I see a world where the current, central
role of Bugzilla and bugs as part of the Firefox change process begin to
wane.</strong> I see a world where the benefits to maintaining our current
Bugzilla-centric workflow start to erode and the cost of maintaining
it becomes higher and harder to justify. You actually don't have to look
too far into the future: that world is already here and I've already
started to feel the pains of it.</p>
<p>A few days ago, I blogged about
<a href="/blog/2015/01/10/code-first-and-the-rise-of-the-dvcs-and-github/">GitHub and its code first approach to change</a>.
That post was spun off from an early draft of this post (as were the
posts about <a href="/blog/2015/01/09/firefox-contribution-process-debt/">Firefox contribution debt</a>
and <a href="/blog/2015/01/12/utilizing-github-for-firefox-development/">utilizing GitHub for Firefox development</a>).
I wanted to introduce the concept of <em>code first</em> because it is
central to my justification for changing how we do things. In summary,
<strong>code first capitalizes on the fact that any change to software
involves code and therefore puts code front and center in the change
process.</strong> (In hindsight, I probably should have used the term <em>code
centric</em>, because that's how I want people to think about things.) So
how does <em>code first</em> relate to Bugzilla and Firefox development?</p>
<p>Historically, code review has occurred in Bugzilla: upload a patch to
Bugzilla, ask for review, and someone will look at it. And, since
practically every change to Firefox requires review, you need a bug in
Bugzilla to contain that review. Thus, <strong>one way to view a bug is as a
vehicle for code review</strong>. Not every bug is <em>just</em> a code review, of
course. But a good number of them are.</p>
<p>The only constant is change. And <strong>the way Mozilla conducts code review
for changes to Firefox (and other projects) is changing</strong>. We now have
<a href="https://mozilla-version-control-tools.readthedocs.org/en/latest/mozreview.html">MozReview</a>,
a code review tool that is <em>not Bugzilla</em>. If we start accepting GitHub
pull requests, we <em>may</em> perform reviews exclusively on GitHub, another
tool that is <em>not Bugzilla</em>.</p>
<p>(Before I go on, I want to quickly point out that MozReview is nowhere
close to its final form. Parts of MozReview are pretty bad right now.
The maintainers all know this and we have plans to fix it. We'll be in
Toronto all of next week working on it. If you don't think you'll ever
use it because parts are bad today, I ask you to withhold judgement for
a few more months.)</p>
<p>In case you were wondering, the question on whether Bugzilla should
always be used for code review for Firefox has been answered and that
answer is <em>no</em>. People, including maintainers of Bugzilla, realized
that better-than-Splinter/Bugzilla code review tools exist and that
continuing to invest time to develop Bugzilla/Splinter into a
best-in-class code review tool would be better spent <em>integrating</em>
Bugzilla with an existing tool. This is why we now have a
<a href="https://www.reviewboard.org/">Review Board</a> based code review tool -
MozReview - integrated with Bugzilla. If you care about code quality and
more powerful workflows, you should be rejoicing at this because
<a href="/blog/2014/10/27/implications-of-using-bugzilla-for-firefox-patch-development/">the implementation of code review in Bugzilla does not maximize optimal outcomes</a>.</p>
<p>The world we're moving to is one where code review occurs outside of
Bugzilla. This raises an important question: <strong>if Bugzilla was being used
primarily as a vehicle for code review, what benefit and/or role should
Bugzilla play when code review is conducted outside of Bugzilla?</strong></p>
<p><strong>I posit that there are a class of bugs that won't need to exist
going forward because bugs will provide little to no value.</strong> Put
another way, I believe that a growing number of commits to the Firefox
repository won't reference bugs.</p>
<p>Come with me on a journey to the future.</p>
<p>MozReview is purposefully being designed in a code and repository
centric way. To initiate the formal process for considering a change to
code, you push to a Mercurial (or Git!) repository. This could be
directly to Mozilla's review repository. If I have my way, this could
even be kicked off by submitting a pull request on GitHub or Bitbucket.
No Bugzilla attachment uploading here: our systems talk in terms of
repositories and commits. Again, this is by design: we don't want
submitting code to Mozilla to be any harder than <em>hg push</em> or <em>git
push</em> so as to not introduce <em>process debt</em>. If you have code, you'll be
able to send it to us.</p>
<p>In the near future, MozReview will stop cross-posting detailed review
updates to Bugzilla. Instead, we'll use Review Board's e-mail feature
to send its flavor of emails. These will have rich HTML content (or
plain text if you insist) and will provide a better experience
than Bugzilla ever will. We'll adopt the model of tools like
Phabricator and GitHub and only post summaries or links of activity,
not full content, to bugs. You may be familiar with the concept as
applied to the web: it's called hyperlinking.</p>
<p>Work is being invested into Autoland. Autoland is an automated landing
queue that pushes/lands commits semi-automatically once they are ready
(have review, pass automation, etc). Think of Autoland as a bot that
does all the labor intensive and menial actions around pushing that
you do now. I believe Autoland will eventually handle near 100% of
pushes to the Firefox repository. And, if I have my way, Autoland will
result in the abolishment of <em>integration branches</em> and merge commits in
the Firefox repository. Good riddance.</p>
<p>MozReview and Autoland will be highly integrated. MozReview will be the
primary user interface for interacting with Autoland. (Some of this
should be in place by the end of the quarter.)</p>
<p>In this world, MozReview and its underlying version control repositories
essentially become a <em>database</em> of all submitted, pending, and discarded
commits to Firefox. The metaphorical <em>primary keys</em> of this <em>database</em>
are not bug numbers: they are code/commits. (Code first!) Some of the
flags stored in this <em>database</em> tell Autoland what it should do. And the
MozReview user interface (and API) provide a mechanism into controlling
those flags.</p>
<p>Landing a change in Firefox will be initiated by a simple action such as
clicking a checkbox in MozReview. (That could even be the <em>Grant Review</em>
checkbox.) Commits cleared for landing will be picked up by
Autoland and eventually automatically pushed to the Firefox repository
(assuming the build and test automation is happy, of course). Once
Autoland takes control, humans are just passengers. We won't be bothered
with menial tasks like updating the commit message to reflect a review
was performed: this will happen automatically inside MozReview or
Autoland. (Although, there's a chance we may adopt some PGP-based
signing to more strongly convey review for some code changes in order to
facilitate stronger auditing and trust guarantees. Stay tuned.)
Likewise, if a commit becomes associated with a bug, we can add that
metadata to the commit before it is landed, no human involvement
necessary beyond specifying the link in the MozReview web UI (or API).
Autoland/MozReview will close review requests and/or bugs automatically.
(Are you excited about performing less work yet?)</p>
<p>When commits are added to MozReview, <strong>MozReview will read metadata from
the repository they came from to automatically determine an appropriate
reviewer</strong>. (We <a href="https://groups.google.com/d/msg/mozilla.dev.platform/iXr70VgapWk/GkTCcKRjNi8J">plan</a>
to leverage moz.build files for this in the case of Firefox.) This
should eliminate a lot of <em>process debt</em> around choosing a reviewer.
<strong>Similar metadata will also be used to determine what Bugzilla component
a change is related to, static analysis rules to use to critique the
phsyical structure of the change, and even automation jobs that should
be executed given the set of files that changed.</strong> The use of this
metadata will erode significant <em>process debt</em> around the change
contribution workflow.</p>
<p>As commits are pushed into MozReview/Autoland, the systems will be
intelligent about automatically tracking dependencies and facilitating
complex development workflows that people run into on a daily basis.</p>
<p>If I create a commit on top of someone else's commit that hasn't been
checked in yet, MozReview will detect the dependency between
my changes and the parent ones. This is an advantage of being code
first: by interfacing with repositories rather than patch files, you
have an explicit dependency graph embedded in the repository commit DAG
that can be used to aid machines in their activities.</p>
<p>It will also be possible to partially land a series of commits. If I get
review on the first 5 of 10 commits but things stall on commit 6, I can ask
Autoland to land the already-reviewed commits so they don't get bit
rotted and so you have partial progress (psychological studies show that
a partial reward for work results in greater happiness through a sense
of accomplishment).</p>
<p>Since initiating actions in MozReview is light weight (just <em>hg push</em>),
itch scratching is encouraged. I don't know about you, but in the course
of working on the Firefox code base, I frequently find myself wanting to
make small, 15-30s changes to fix something really minor. In today's world,
the overhead for these small changes is often high. I need to upload a
separate patch to Bugzilla. Sometimes I even need to create a new bug to
hold that patch. If that patch depends on other work I did, I need to
set up bug dependencies then worry about landing everything in the right
order. All of a sudden, the overhead isn't worth it and my positive
intentions go unacted on. Multiplied by hundreds of developers over
many years, and you can imagine the effect on software quality. With
MozReview, the overhead for itch scratching like this is minor. Just
make a small commit, push, and the system will sort everything out.
(These small commits are where I think a <em>bugless</em> process really
shines.)</p>
<p>This future world revolves around code and commits and operations on
them. While <em>MozReview</em> has <em>review</em> in its name, it's more than a
review tool: it's a <em>database</em> and interface to code and its state.</p>
<p><strong>In this code first world, Bugzilla performs an ancillary role.</strong>
Bugzilla is still there. Bugs are still there. MozReview review requests
and commits <em>link</em> to bugs. But it is the code, not bugs, that are king.
If you want to do anything with code, you interact with the code
tools. And Bugzilla is not one of them.</p>
<p>Another way of looking at this is that nearly everything involving code
or commits becomes excised from Bugzilla. This would relegate Bugzilla
to, well, an issue/bug tracker. And - ta da - that's something it excels
at since that's what it was originally designed to do! MozReview will
provide an adequate platform to discuss code (a platform that Bugzilla
provides today since it hosts code review). So if <em>not Bugzilla</em>
tools are handling everything related to code, do you really need a bug
any more?</p>
<p>This is the future we're trying to build with MozReview and Autoland.
And this is why I think bugs and Bugzilla will play a less central role
in the development process of Firefox in the future.</p>
<p>Yes, there are many consequences and concerns about making this shift.
You would be rational to be skeptical and doubt that this is the right
thing to do. I have another post in the works that attempts to outline
some common conerns and propose solutions to many of them. Before writing
a long comment pointing out every way in which this will fail to work,
I encourage you to wait for that post to be published. Stay tuned.</p>]]></content:encoded>
    </item>
    <item>
      <title>Modern Mercurial Documentation for Mozillians</title>
      <link>http://gregoryszorc.com/blog/2015/01/15/modern-mercurial-documentation-for-mozillians</link>
      <pubDate>Thu, 15 Jan 2015 14:45:00 PST</pubDate>
      <category><![CDATA[Mercurial]]></category>
      <category><![CDATA[Mozilla]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2015/01/15/modern-mercurial-documentation-for-mozillians</guid>
      <description>Modern Mercurial Documentation for Mozillians</description>
      <content:encoded><![CDATA[<p>Mozilla's Mercurial documentation has historically been pretty bad. The
documentation on MDN (which I refuse to link to) is horribly disjointed
and contains a lot of outdated recommendations. I've made attempts to
burn some of it to the ground, but it is just too overwhelming.</p>
<p>I've been casually creating my own Mercurial documentation tailored for
Mozillians. It's called
<a href="https://mozilla-version-control-tools.readthedocs.org/en/latest/hgmozilla/index.html">Mercurial for Mozillians</a>.</p>
<p>It started as a way to document extensions inside the
<a href="https://hg.mozilla.org/hgcustom/version-control-tools/">version-control-tools</a>
repository. But, it has since evolved to cover other topics, like how to
install Mercurial, how to develop using bookmarks, and how to interact
with a unified Firefox repository. The documentation is nowhere near
complete. But it already has some very useful content beyond what MDN
offers.</p>
<p>I'm not crazy about the idea of having generic Mercurial documentation
on a Mozilla domain (this should be part of the official Mercurial
documentation). Nor am I crazy about moving content off MDN. I'm sure
content will move to its appropriate location later. Until then,
enjoy some curated Mercurial documentation!</p>
<p>If you would like to contribute to Mercurial for Mozillians,
<a href="https://mozilla-version-control-tools.readthedocs.org/en/latest/contributing.html">read the docs</a>.</p>]]></content:encoded>
    </item>
  </channel>
</rss>
